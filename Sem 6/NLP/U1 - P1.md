# Master Study Notes: Natural Language Processing with Deep Learning

## 1. The Foundations of NLP: Understanding (NLU) and Generation (NLG)

Natural Language Processing is defined as the union of **Natural Language Understanding (NLU)** and **Natural Language Generation (NLG)**. While NLU maps text to an underlying meaning, NLG maps an underlying meaning back into human-readable text.

### The Levels of Linguistic Analysis

NLU operates through a hierarchy of levels, moving from raw sounds to complex contextual meaning:

- **Phonetics & Phonology:** The processing of speech and its associated ambiguities, such as homophones (e.g., "bank" for finance vs. "bank" of a river).
    
- **Morphology:** The study of forming words from root words through rules. It handles pluralization (boy-boys), gender marking, and tense.
    
- **Lexical Analysis:** Disambiguating words by referring to a dictionary (lexicon). This involves determining the Part of Speech (POS) and the specific "sense" of a word based on context.
    
- **Syntactic Analysis:** Detecting the structure of sentences. A major challenge is **Syntactic Ambiguity**, where a single sentence like "Mary saw the man on the mountain with a telescope" can have at least five distinct structural meanings depending on how prepositional phrases are attached.
    
- **Semantics:** Mapping sentences to knowledge representations like Predicate Calculus or Semantic Nets. It involves **Semantic Role Labeling**, identifying the Action, Agent, Object, and Recipient.
    
- **Pragmatics:** Modeling user intention and world knowledge.
    
- **Discourse:** The processing of collocated, related groups of sentences.
    
    - **Coherence:** The logical structure that makes a discourse "make sense".
        
    - **Cohesion:** Linguistic devices (like anaphora) used to tie text units together.
        

> **Key Takeaway:** NLP progresses from surface-level processing (phonetics/morphology) where ambiguity is high, to deeper semantic and pragmatic levels where world knowledge is required to resolve meaning.

---

## 2. Natural Language Generation (NLG) Architecture

NLG is the deliberate construction of natural language text to meet specific communicative goals, starting from non-linguistic representations.

### The Typical Pipelined Architecture

NLG is generally structured into a three-stage pipeline:

1. **Text Planning:**
    
    - **Content Determination:** Deciding what data to express as "messages" based on communicative goals.
        
    - **Discourse Planning:** Ordering the messages to ensure the final text has an underlying logical structure.
        
2. **Sentence Planning (Micro-planning):**
    
    - **Aggregation:** Combining multiple simple messages into complex, fluid sentences to avoid "choppy" or disfluent text.
        
    - **Lexicalization:** Choosing the specific words (e.g., "leaves" vs. "departs") to express concepts.
        
    - **Referring Expression Generation:** Deciding how to describe entities (using pronouns, proper names, or descriptions) while avoiding ambiguity.
        
3. **Linguistic Realization (Surface Realization):**
    
    - **Syntactic & Morphological Realization:** Applying rules of word and sentence formation (e.g., $walk + ed = walked$).
        
    - **Orthographic Realization:** Handling casing, punctuation, and typography.
        

---

## 3. Deriving Facts: Information Extraction (IE)

Information Extraction deals with finding structured factual information (like database records) within unstructured free text.

### Core Processes in Fact Derivation

To turn a corpus into "facts" (entities, relations, and temporal data), several techniques are applied:

|**Task**|**Description**|**Example**|
|---|---|---|
|**NER**|Identifying and categorizing key entities.|"Barack Obama" â†’ Person.|
|**Coreference Resolution**|Clustering nouns/pronouns that refer to the same entity.|"John bought food. **He**..." (**He** = John).|
|**Dependency Parsing**|Analyzing grammatical relationships between words.|"black" modifies "car" (amod).|
|**Relation Extraction**|Detecting predefined relationships between entities.|EmployeeOf(Steve Jobs, Apple).|
|**Event Extraction**|Identifying "who did what to whom, when, and where".|Bombing in Maiduguri on Sunday.|

> **Key Takeaway:** Unlike Information Retrieval (IR), which returns a ranked list of relevant documents, IE processes data into a shape that facilitates direct knowledge discovery.

---

## 4. Part-of-Speech (POS) Tagging Methods

POS tagging is the process of assigning a lexical class marker (e.g., Noun, Verb) to every word in a corpus. This is a vital sub-step for parsing and question answering.

### Evolutionary Approaches to Tagging

1. **Rule-Based:** Uses a dictionary and hand-written rules to eliminate unlikely tags based on context (e.g., eliminating a Verb tag if it follows a Determiner).
    
2. **Stochastic (Statistical):** Uses frequencies from a tagged corpus.
    
    - **Hidden Markov Models (HMM):** Finds the tag sequence that maximizes the likelihood of the word sequence.
        
3. **Neural NLP (Deep Learning):** Uses sequential models like **Bidirectional RNNs** to predict tag sequences, often utilizing Softmax activation and Adam optimizers.
    

---

## Self-Test: High-Level Synthesis Questions

1. **Contrastive Analysis:** How do NLU and NLG manage "choices" differently when processing language?
    
2. **Architectural Design:** Why is "Aggregation" a critical step in the NLG pipeline, and what would the output look like without it?
    
3. **Ambiguity Resolution:** Explain how Syntactic Ambiguity differs from Semantic Role Labeling ambiguity using specific examples from the text.
    
4. **Task Hardness:** Why is Event Extraction (EE) considered significantly harder than Named Entity Recognition (NER)?
    
5. **Model Evolution:** Compare the strengths and weaknesses of HMM-based tagging versus RNN-based tagging in handling "unknown" words or long-distance dependencies.