# Cloud Computing — Complete Virtualization Study Notes

### PES University | Dr. Prafullata Kiran Auradkar | Unit 2

---

> **How to use this notebook:** Every concept is explained from first principles, building up layer by layer. Questions and answers are embedded after each major section. Read top to bottom for a complete understanding.

---

## Table of Contents

1. [What is Virtualization?](https://claude.ai/chat/464cf182-acaf-4c74-bbd4-071c23e21243#1-what-is-virtualization)
2. [Compute / Server Virtualization](https://claude.ai/chat/464cf182-acaf-4c74-bbd4-071c23e21243#2-compute--server-virtualization)
3. [Physical Machine & Privilege Rings — The Foundation](https://claude.ai/chat/464cf182-acaf-4c74-bbd4-071c23e21243#3-physical-machine--privilege-rings--the-foundation)
4. [Types of Hypervisors (VMM)](https://claude.ai/chat/464cf182-acaf-4c74-bbd4-071c23e21243#4-types-of-hypervisors-vmm)
5. [Virtualization Software Techniques](https://claude.ai/chat/464cf182-acaf-4c74-bbd4-071c23e21243#5-virtualization-software-techniques)
    - [Direct Execution](https://claude.ai/chat/464cf182-acaf-4c74-bbd4-071c23e21243#51-direct-execution)
    - [Trap and Emulate](https://claude.ai/chat/464cf182-acaf-4c74-bbd4-071c23e21243#52-trap-and-emulate)
    - [Problems with Trap and Emulate on x86](https://claude.ai/chat/464cf182-acaf-4c74-bbd4-071c23e21243#53-problems-with-trap-and-emulate-on-x86)
    - [Strictly Virtualizable](https://claude.ai/chat/464cf182-acaf-4c74-bbd4-071c23e21243#54-strictly-virtualizable)
    - [Binary Translation](https://claude.ai/chat/464cf182-acaf-4c74-bbd4-071c23e21243#55-binary-translation)
6. [Paravirtualization and Transparent Virtualization](https://claude.ai/chat/464cf182-acaf-4c74-bbd4-071c23e21243#6-paravirtualization-and-transparent-virtualization)
    - [Full (Transparent) Virtualization](https://claude.ai/chat/464cf182-acaf-4c74-bbd4-071c23e21243#61-full-transparent-virtualization)
    - [Paravirtualization](https://claude.ai/chat/464cf182-acaf-4c74-bbd4-071c23e21243#62-paravirtualization)
    - [Comparison: Full vs Para](https://claude.ai/chat/464cf182-acaf-4c74-bbd4-071c23e21243#63-comparison-full-vs-para-virtualization)
7. [Techniques to Virtualize x86](https://claude.ai/chat/464cf182-acaf-4c74-bbd4-071c23e21243#7-techniques-to-virtualize-x86)
8. [Hardware-Assisted Virtualization (Intel VT-x / AMD-V)](https://claude.ai/chat/464cf182-acaf-4c74-bbd4-071c23e21243#8-hardware-assisted-virtualization-intel-vt-x--amd-v)
9. [Real-World Hypervisor Architectures](https://claude.ai/chat/464cf182-acaf-4c74-bbd4-071c23e21243#9-real-world-hypervisor-architectures)
    - [Xen Architecture](https://claude.ai/chat/464cf182-acaf-4c74-bbd4-071c23e21243#91-xen-architecture)
    - [KVM Architecture](https://claude.ai/chat/464cf182-acaf-4c74-bbd4-071c23e21243#92-kvm-architecture)
    - [VMware ESX Server](https://claude.ai/chat/464cf182-acaf-4c74-bbd4-071c23e21243#93-vmware-esx-server)
10. [Memory Virtualization — Shadow Page Tables](https://claude.ai/chat/464cf182-acaf-4c74-bbd4-071c23e21243#10-memory-virtualization--shadow-page-tables)
11. [Master Q&A Bank](https://claude.ai/chat/464cf182-acaf-4c74-bbd4-071c23e21243#11-master-qa-bank)

---

## 1. What is Virtualization?

### Definition

**Virtualization** is the technique of **abstracting physical hardware resources** (CPU, memory, storage, networking) into a **logical view**, so that multiple independent environments can share the same underlying hardware simultaneously.

Think of it like an apartment building: one physical building (the hardware), multiple independent apartments (the VMs), each feeling like a complete home.

### What Can Be Virtualized?

|Resource|Virtual Form|What the App Sees|
|---|---|---|
|CPU / Server|Virtual Server|Its own logical processor|
|RAM|Virtual Memory|Its own isolated memory space|
|Network|Virtual Network|Its own independent NIC|
|Disk|Virtual Storage|Its own logical storage volume|

### Why Does Virtualization Matter?

- **Increases utilization** — instead of one OS on one physical machine sitting idle 80% of the time, you run 10 VMs sharing that hardware
- **Simplifies resource management** — pool and share resources dynamically
- **Reduces downtime** — both planned (maintenance) and unplanned (failures)
- **Improves performance** — right-size resources to workloads
- **Enables Cloud Computing** — IaaS (Infrastructure as a Service) is built entirely on virtualization

---

### ❓ Q&A — Section 1

**Q1. What is virtualization in simple terms?**

> **A:** Virtualization is the process of creating a logical (virtual) version of a physical resource — such as a server, storage device, or network — so multiple users or applications can share it independently, each believing they have exclusive access.

**Q2. Name four types of resources that can be virtualized.**

> **A:** Virtual Servers (compute), Virtual Memory, Virtual Networks, Virtual Storage.

**Q3. How does virtualization relate to cloud computing?**

> **A:** Virtualization is the foundational technology of cloud computing. Cloud providers (AWS, Azure, GCP) use virtualization to partition their massive physical server farms into thousands of VMs that can be provisioned on demand to different users — this is the core of IaaS.

**Q4. What problem did virtualization solve in data centers?**

> **A:** Before virtualization, each server ran a single OS and a single application, leading to very low hardware utilization (often 5–15%). Virtualization allowed multiple workloads to run on the same hardware, dramatically improving utilization and reducing costs.

---

## 2. Compute / Server Virtualization

### Definition

**Compute virtualization** (also called server virtualization) is the methodology of **dividing the resources of a single physical server into multiple independent execution environments**, each running its own OS and applications.

### Key Concepts

- A **Virtual Machine (VM)** is the coarse-grained unit of compute virtualization.
- A VM is **logically identical to a physical machine** — it has its own virtual CPU (vCPU), virtual RAM, virtual disk, and virtual NIC.
- A VM can run an OS just as a physical machine does, with no modification (in full virtualization).

### Before vs. After Server Virtualization

**Before (Bare Metal / No Virtualization):**

```
+--------------------+
|   Application 1    |   Application 2   (conflicts possible!)
+--------------------+
|   Operating System |
+--------------------+
|      Hardware      |
+--------------------+
```

Problems:

- Single OS per machine
- Software and hardware tightly coupled
- Running multiple apps on the same OS can cause conflicts
- Underutilized resources

**After Server Virtualization:**

```
+----------+   +----------+
|  VM 1    |   |  VM 2    |   (isolated from each other)
| App      |   | App      |
| Guest OS |   | Guest OS |
+----------+   +----------+
|   Virtualization Layer (VMM / Hypervisor)  |
+--------------------------------------------+
|              Hardware                       |
+--------------------------------------------+
```

Benefits:

- VMs are **hardware-independent**
- Strong **fault and security isolation** — a crash in VM1 doesn't affect VM2
- OS and apps managed as a **single unit** (encapsulated)
- Multiple OSes on the same hardware simultaneously

### Why Use VMs? (Use Cases)

|Use Case|Explanation|
|---|---|
|**Server Consolidation**|Multiple workloads on one server → better utilization, lower costs|
|**Workload Mobility**|Move a VM from one physical host to another (live migration)|
|**Development & Test**|Spin up/tear down environments instantly|
|**Business Continuity**|Live-migrate VMs to other servers during failures|
|**OS Diversity**|Run Windows and Linux simultaneously on the same hardware|
|**Security/Isolation**|Hypervisor isolates VMs from each other and from hardware|
|**High Availability**|Live-migrate to another physical host for load balancing|
|**Rapid Provisioning**|On-demand hardware resource allocation|
|**Encapsulation**|Application + OS packaged as a single portable unit|

---

### ❓ Q&A — Section 2

**Q5. What is a Virtual Machine (VM)?**

> **A:** A VM is a logical, isolated computing environment that emulates a complete physical computer, with its own virtual CPU, memory, disk, and network interface. It can run an OS and applications as if it were a real physical machine.

**Q6. What is "encapsulation" in the context of VMs?**

> **A:** Encapsulation means the entire execution environment of an application — including the OS, application binaries, configuration files, and state — is packaged as a single unit (usually a set of files like a disk image and configuration file). This makes VMs portable and easy to backup, copy, or migrate.

**Q7. What is "live migration"?**

> **A:** Live migration is the process of moving a running VM from one physical host server to another without shutting it down, causing minimal to zero downtime. This is used for load balancing, hardware maintenance, and business continuity.

**Q8. Why does server virtualization improve hardware utilization?**

> **A:** Physical servers in traditional setups are often idle most of the time (e.g., a web server might only be 10% utilized). Virtualization allows multiple VMs to share one physical server, so the combined workload keeps the hardware busy, improving utilization from perhaps 10% to 70–80%.

---

## 3. Physical Machine & Privilege Rings — The Foundation

> This section is **critical** — you cannot understand virtualization without deeply understanding how a physical CPU handles privilege levels. Everything in virtualization builds on this.

### The Problem of Trust

Not all software should have equal power. An application should **not** be able to directly control hardware (imagine a buggy game crashing your whole machine, or a malicious app reading other programs' memory). The CPU hardware enforces this using **privilege rings**.

### x86 Privilege Rings

The x86 architecture defines **4 privilege rings (0–3)**:

```
           RING 3  ←  User Applications (least privilege)
           RING 2  ←  (rarely used in modern OSes)
           RING 1  ←  (rarely used in modern OSes)
           RING 0  ←  Operating System Kernel (most privilege / "kernel mode")
           ─────────────────────────────────────────────────
           Physical Hardware
```

- **Ring 0 (Kernel Mode / Privileged Mode):** The OS kernel runs here. It has unrestricted access to all hardware resources. Can execute ALL instructions — privileged ones like:
    
    - `HLT` (Halt the CPU)
    - `IN`/`OUT` (I/O port access)
    - `INVLPG` (Invalidate TLB entries)
    - `IRET` (Interrupt return)
    - Modifying control registers (CR0, CR3)
    - Turning off all interrupts
    - Context switching, clearing memory
- **Ring 3 (User Mode):** Applications run here. Only safe, non-system-affecting instructions are allowed:
    
    - `ADD`, `SUB`, `MUL` (arithmetic)
    - `LOAD`, `STORE` (memory access within allowed region)
    - `JMP`, `CALL` (within allowed code)

### How a System Call Works (Normal OS, No VM)

When an application needs to do something privileged (e.g., write to disk):

```
1. App (Ring 3) executes a TRAP instruction (e.g., INT 0x80 on Linux, SYSCALL on x86-64)
2. CPU hardware automatically switches to Ring 0 (kernel mode)
3. Mode bit is SET to 0 (supervisory mode)
4. OS kernel executes the requested operation (e.g., disk write)
5. OS sets mode bit back to 1 (user mode)
6. Control returns to the application
```

This mechanism ensures **no user-mode application can directly access hardware** — it must always go through the OS kernel.

### What Happens When You Execute a Privileged Instruction in Ring 3?

The CPU **does NOT execute it**. Instead, it generates a **General Protection Fault (GPF)** — a hardware trap — which alerts the OS to handle the violation. The OS can then terminate the offending application.

This is the key mechanism that virtualization exploits.

---

### ❓ Q&A — Section 3

**Q9. What are x86 privilege rings?**

> **A:** x86 CPUs define 4 privilege levels (rings 0–3). Ring 0 is the most privileged (kernel mode), where the OS runs and can execute all instructions including privileged ones. Ring 3 is the least privileged (user mode), where applications run with restricted access to hardware. Ring 1 and 2 are rarely used in modern operating systems.

**Q10. What is a General Protection Fault (GPF)?**

> **A:** A GPF is a hardware exception generated by the CPU when a program tries to execute a privileged instruction from an unprivileged ring (e.g., executing `HLT` from Ring 3). The CPU traps this to the OS kernel, which handles it — typically by terminating the offending process.

**Q11. What is a "privileged instruction"? Give examples.**

> **A:** A privileged instruction is one that can affect global system state and is only allowed to execute in Ring 0 (kernel mode). Examples include: `HLT` (halt CPU), `IN`/`OUT` (I/O access), `INVLPG` (TLB invalidation), modifying control registers (CR0, CR3), `IRET` (interrupt return), and instructions that enable/disable interrupts.

**Q12. What is the difference between a "system call" and a "hypercall"?**

> **A:** A **system call** is a mechanism by which a user-mode application (Ring 3) requests a service from the OS kernel (Ring 0). A **hypercall** is an analogous mechanism in virtualization — it is a call made by a guest OS to the hypervisor (VMM), requesting the hypervisor to perform a privileged operation on its behalf.

---

## 4. Types of Hypervisors (VMM)

### What is a VMM / Hypervisor?

A **Virtual Machine Monitor (VMM)**, also called a **hypervisor**, is a **layer of software** that creates and manages virtual machines. It:

1. Provides each VM with the **illusion of a complete physical machine**
2. **Multiplexes** physical hardware resources among multiple VMs
3. **Ensures isolation** — VMs cannot interfere with each other

### Three Requirements for a VMM (Popek & Goldberg, 1974)

These are the classical requirements that a VMM must satisfy:

1. **Equivalence / Fidelity:** The VMM must provide an environment for programs that is **essentially identical to the original machine** — programs should behave the same way as on real hardware.
2. **Performance / Efficiency:** Programs running in this virtual environment should show **at worst, only a minor reduction in performance**. Most instructions should execute at native speed.
3. **Safety / Resource Control:** The VMM must be in **complete control of system resources** — a guest VM cannot seize control of the hardware or affect other VMs.

### Type 1 Hypervisor (Bare-Metal / Native)

```
+--------+   +--------+   +--------+
| Guest  |   | Guest  |   | Guest  |
|  VM 1  |   |  VM 2  |   |  VM 3  |
+--------+   +--------+   +--------+
+-----------------------------------+
|           VMM / Hypervisor        |  ← runs directly on hardware
+-----------------------------------+
|              Hardware             |
+-----------------------------------+
```

- **Also called:** Bare-Metal VMM, Native Hypervisor
- **Runs directly on physical hardware** — there is no host OS underneath
- The VMM itself acts like the OS, managing hardware directly
- **Higher performance** — no middle OS layer
- **More secure** — smaller attack surface
- **Better scalability**
- **Examples:** VMware ESXi, Microsoft Hyper-V, Citrix XenServer, IBM CP/CMS, Xen

### Type 2 Hypervisor (Hosted)

```
+--------+   +--------+   +--------+
| Guest  |   | Guest  |   | Guest  |
|  VM 1  |   |  VM 2  |   |  VM 3  |
+--------+   +--------+   +--------+
+-----------------------------------+
|           VMM / Hypervisor        |  ← runs as an app on top of Host OS
+-----------------------------------+
|           Host OS (e.g., Windows) |
+-----------------------------------+
|              Hardware             |
+-----------------------------------+
```

- **Also called:** Hosted Hypervisor
- **Runs as an application on top of a conventional OS** (the Host OS)
- The VMM relies on the Host OS for hardware access
- **Easier to install** — you already have an OS
- **Lower performance** — extra layer of indirection through Host OS
- **Less secure** — bugs in the Host OS affect the hypervisor
- **Examples:** VMware Workstation Player, Oracle VirtualBox, Microsoft Virtual PC, QEMU

### Type 3 (Hybrid) Hypervisor

```
+----------+   +--------+   +--------+
|  Host VM |   | Guest  |   | Guest  |
|          |   |  VM 1  |   |  VM 2  |
+----------+   +--------+   +--------+
+----------+   +----------------------------+
|   VMM    |   |    VMM (hosted portion)    |
+----------+   +----------------------------+
               |         Host OS            |
+-------------------------------------------+
|                  Hardware                 |
+-------------------------------------------+
```

- Supports **both bare-metal and hosted** modes simultaneously
- **Examples:** MS Virtual Server, MS Virtual PC (older)

### Special Case: KVM (Kernel-Based Virtual Machine)

KVM is unique — the VMM is **integrated directly into the Linux kernel** as a loadable module:

- The Linux kernel IS the hypervisor
- Each VM runs as a **Linux process**, scheduled by Linux's scheduler
- Uses VT-x hardware support for CPU virtualization
- Uses EPT (Extended Page Tables) for memory virtualization
- Memory management, scheduling, device drivers — all reuse existing Linux infrastructure

### Hypervisor Type 1 vs. Type 2 — Full Comparison

|Criteria|Type 1 Hypervisor|Type 2 Hypervisor|
|---|---|---|
|**Also called**|Bare-metal or Native|Hosted|
|**Definition**|Runs directly on hardware with VMs on top|Runs as an app on a conventional OS|
|**Virtualization**|Hardware Virtualization|OS Virtualization|
|**Operation**|Guest OS and apps run on the hypervisor|Runs as a process on the host OS|
|**Scalability**|Better scalability|Limited (depends on host OS)|
|**Setup/Installation**|Requires compatible hardware|Simpler (host OS already present)|
|**System Independence**|Direct hardware access|Cannot directly access host hardware|
|**Speed**|Faster|Slower (extra OS layer)|
|**Performance**|Higher (no middle layer)|Lower (extra overhead)|
|**Security**|More secure|Less secure (host OS bugs affect everything)|
|**Examples**|VMware ESXi, Hyper-V, Citrix XenServer|VMware Workstation, VirtualBox, MS Virtual PC|

---

### ❓ Q&A — Section 4

**Q13. What are the three classical requirements for a VMM according to Popek and Goldberg?**

> **A:** (1) **Equivalence/Fidelity** — the VMM provides an environment essentially identical to the real hardware; (2) **Performance/Efficiency** — programs show at worst minor performance reduction; (3) **Safety/Resource Control** — the VMM maintains complete control of system resources.

**Q14. What is the key difference between a Type 1 and Type 2 hypervisor?**

> **A:** A Type 1 (bare-metal) hypervisor runs directly on physical hardware with no underlying OS, while a Type 2 (hosted) hypervisor runs as an application on top of a host OS. Type 1 offers better performance, security, and scalability; Type 2 is easier to install and use on a personal computer.

**Q15. How does KVM differ from a traditional Type 1 or Type 2 hypervisor?**

> **A:** KVM is unique because the hypervisor functionality is integrated directly into the Linux kernel. This means the Linux kernel itself becomes the hypervisor. Each VM appears as a regular Linux process, and KVM reuses Linux's existing scheduler, memory management, and device drivers. This gives it the performance benefits of a Type 1 hypervisor while running within the Linux OS.

**Q16. Why is a Type 1 hypervisor considered more secure than a Type 2?**

> **A:** In a Type 2 hypervisor, any vulnerability in the host OS can compromise the hypervisor and all guest VMs. In a Type 1 hypervisor, there is no host OS — the attack surface is limited to the hypervisor itself, which is a much smaller and more focused codebase.

---

## 5. Virtualization Software Techniques

Now we get into the **mechanics** of how a VMM actually intercepts and manages VM execution.

### 5.1 Direct Execution

**Idea:** Run **most instructions of the VM directly on the physical hardware**, with no translation or emulation.

```
VM Instructions → CPU executes them natively (at hardware speed)
```

- **Advantage:** Performance nearly identical to native execution (the instructions literally run on the bare CPU)
- **Challenge:** If ALL instructions run directly, the VMM loses control! A privileged instruction from the guest OS could modify global system state (e.g., turn off all interrupts, modify page tables) and break isolation.

So direct execution alone is insufficient — we need a way to intercept dangerous instructions while letting safe ones run at full speed.

---

### 5.2 Trap and Emulate

This is the **classic** software virtualization approach, and it is conceptually elegant.

**The Core Idea:**

```
Safe (user-level) instructions → execute directly on hardware (full speed)
Privileged / Sensitive instructions → TRAP to VMM → VMM emulates the effect
```

**Detailed Mechanism:**

In a virtualized environment with a traditional VMM:

- **VMM** runs in **Ring 0** (the most privileged ring)
- **Guest OS** is "deprivileged" — it runs in **Ring 1** (lower privilege than it expects)
- **Guest Applications** run in **Ring 3** (as usual)

```
RING 3  ←  Guest Applications (as normal)
RING 2  ←  (unused)
RING 1  ←  Guest OS (deprivileged! Normally expects Ring 0)
RING 0  ←  VMM / Hypervisor
─────────────────────────────────────
Physical Hardware
```

**Step-by-Step Flow:**

1. **Normal execution:** Guest app or guest OS runs instructions directly on hardware.
2. **Sensitive instruction encountered:** Guest OS tries to execute a privileged instruction (e.g., write to CR3 to update page table base).
3. **Hardware TRAP:** Since the guest OS is in Ring 1, the CPU refuses to execute the privileged instruction and generates a **General Protection Fault** (trap to Ring 0).
4. **VMM takes control:** The trap vector brings execution to the VMM in Ring 0.
5. **VMM emulates:** The VMM inspects the instruction, determines what the guest OS was trying to do, and **emulates the effect** on a virtual copy of the CPU state.
6. **Return to guest:** VMM returns control to the guest OS as if the instruction had executed normally.

**Handling Syscalls (trap from guest app to guest OS):**

```
1. Guest app (Ring 3) executes INT n (system call trap)
2. CPU traps to Ring 0 (VMM)
3. VMM doesn't know how to handle this trap → forwards to guest OS trap handler
4. Guest OS trap handler (Ring 1) handles the syscall
5. Guest OS tries to execute IRET (return from interrupt) — privileged!
6. CPU traps to VMM again
7. VMM emulates the IRET: jumps back to the guest user process
```

**Key Point:** Every privileged action by the guest OS (set IDT, set CR3, access hardware) traps to the VMM, gets emulated, and returns to the guest OS.

**Visual Diagram:**

```
Ring 3: [Guest App] --system call (INT n)--> trap to Ring 0
                                                   ↓
Ring 0:                                     [VMM] → delegates to guest OS handler
                                                   ↓
Ring 1:                                     [Guest OS trap handler] handles syscall
                                                   ↓
Ring 1:                                     [Guest OS] executes IRET → trap to Ring 0
                                                   ↓
Ring 0:                                     [VMM] emulates IRET → returns to app
                                                   ↓
Ring 3: [Guest App] resumes
```

---

### 5.3 Problems with Trap and Emulate on x86

The x86 architecture was **NOT designed with virtualization in mind**, and this caused serious problems for trap-and-emulate virtualization.

#### Problem 1: Performance Overhead

- Every privileged instruction requires a **trap** (context switch from guest to VMM and back)
- Traps are expensive — each trap can cost thousands of CPU cycles
- Modern OSes execute privileged instructions frequently (page table updates, interrupt handling, etc.)
- Result: significant slowdown

#### Problem 2: Not All Architectures Support It

- Trap-and-emulate only works if **every sensitive instruction automatically traps** when executed at a lower privilege level
- This was the requirement defined by Popek & Goldberg (1974)
- **x86 (pre-2005) does NOT meet this requirement**

#### Problem 3: Sensitive but Not Privileged Instructions (The Critical Flaw!)

This is the **fundamental problem** with x86 virtualization before hardware support:

> Some x86 instructions are **sensitive** (they affect or reveal system state) but are **NOT privileged** (they don't trap when executed in Ring 1 or Ring 3).

This means the VMM has **no way to intercept them** — they execute silently, potentially breaking the virtualization illusion.

**The Classic Example — `POPF` (Pop Flags):**

- When executed in **Ring 0:** `POPF` can modify the **Interrupt Enable Flag (IF)** in the EFLAGS register → changes whether the CPU accepts hardware interrupts (a global system state change!)
- When executed in **Ring 1 or 3:** `POPF` does **NOT trap**. The CPU silently **ignores** the attempt to modify the IF flag and just modifies the user-visible ALU flags.
- **The problem:** The guest OS, running in Ring 1 (thinking it's in Ring 0), tries to disable interrupts using `POPF`. No trap is generated. The VMM never finds out. The guest OS then behaves incorrectly because it **thinks** it disabled interrupts but it didn't.

**Another Example — `PUSHF` (Push Flags):**

- Pushes the current EFLAGS register value onto the stack
- When in Ring 1, the IF bit reflects Ring 1 privilege, not Ring 0
- The guest OS reads this and **discovers it's not actually in Ring 0** — the virtualization illusion is broken!

**The 17 Problematic x86 Instructions:**

The original x86 architecture had approximately 17 instructions that were sensitive but not privileged:

`SGDT`, `SIDT`, `SLDT`, `SMSW`, `PUSHF`, `POPF`, `LAR`, `LSL`, `VERR`, `VERW`, `POP`, `PUSH`, `CALL`, `JMP`, `INT`, `RET`, `STR`, `MOV`

> **Key Fact:** x86 ISA (pre-2005) **does NOT meet the Popek & Goldberg requirements** for virtualization because of these 17 instructions.

#### Problem 4: Guest OS Discovers Its Privilege Level

- Some x86 registers **reflect the current CPU privilege level** (e.g., the Code Segment register CS encodes the Current Privilege Level / CPL)
- When the guest OS reads CS, it sees CPL=1 (Ring 1), not CPL=0 (Ring 0) as it expects
- The guest OS **knows it's being virtualized** — the illusion is broken

#### Problem 5: Memory Protection Complexity

- Physical memory is a **shared resource** among multiple VMs
- The guest OS cannot be allowed to directly access all physical memory
- Solution: the VMM must maintain a **two-level address translation**:
    - Guest Virtual Address → Guest Physical Address (guest's page table)
    - Guest Physical Address → Host Physical Address (VMM's page table)
- This is complex to implement and has performance implications

---

### 5.4 Strictly Virtualizable

**Definition:** A processor (or mode of a processor) is called **strictly virtualizable** if, when any code is executed at a lower privilege level than it expects:

1. All instructions that **access privileged state** → **trap** to the VMM
2. All instructions either **trap** or **execute identically** (same effect as at full privilege)

This is essentially asking: "Will trap-and-emulate work perfectly on this processor?"

- **x86 before 2005:** NOT strictly virtualizable (because of the 17 problematic instructions)
- **IBM System/370:** Was strictly virtualizable (designed with virtualization in mind from the start — this is why IBM's VM/370 in 1972 was the first successful commercial virtualization)
- **x86 after VT-x/AMD-V (2005+):** Becomes strictly virtualizable with hardware assistance

---

### 5.5 Binary Translation

Binary translation was VMware's ingenious solution to the x86 virtualization problem — it allows **full virtualization of x86 without hardware support**.

**Core Idea:**

Instead of relying on sensitive instructions trapping to the VMM (which they don't always do on x86), the VMM **proactively scans the guest OS code** and **replaces problematic instructions with safe equivalents** before they execute.

```
Guest OS code → VMM scans it → Problematic instructions replaced → Safe code executes
```

This is done **dynamically, at runtime** ("on the fly"), so it's called **Dynamic Binary Translation**.

**Types of Translations:**

1. **"Ident" Translation (Identity):** Instructions that pose no problems are copied into the **translation cache** as-is. They execute at native speed.
    
2. **"Inline" Translation:** Simple problematic instructions (like `POPF`) are translated into a **short sequence of safe emulation code** that's placed directly in the translation cache. For example, `POPF` might be replaced with code that saves the new flags to a virtual EFLAGS variable in memory, checks and updates the virtual IF flag, and returns.
    
3. **"Call-out" Translation:** Complex dangerous instructions (like modifying the page table base register CR3) are replaced with a **call to the VMM**. The VMM then performs the necessary emulation (e.g., updating shadow page tables).
    

**The Translation Unit:**

- The VMM works on **basic blocks** — sequences of instructions from one point to the next branch instruction
- Each basic block is translated and stored in a **Translation Cache (TC)** / **Translation Code Cache (TCC)**
- If the same block is executed again, the cached translation is reused (no re-translation needed) → amortizes the translation cost

**Handling Branches (The Hard Part):**

```
Original code:        isPrime:   mov %ecx, %edi
                                 mov %esi, $2
                                 cmp %esi, %ecx
                                 jge prime         ← branch with original target address

Translated code:      isPrime':  mov %ecx, %edi    ; IDENT (safe, copied as-is)
                                 mov %esi, $2      ; IDENT
                                 cmp %esi, %ecx   ; IDENT
                                 jge [takenAddr]   ; JCC — translated branch (target must be patched)
                                 jmp [fallthrAddr] ; Jump to fall-through translation
```

- Translation changes the **addresses** of instructions (they now live in the translation cache, not at their original addresses)
- All branch instructions must be **patched** to point to the translated versions of their targets
- The VMM must **keep track of all branch addresses** and update them as new blocks are translated

**Key Properties of Binary Translation:**

- Works with **unmodified guest OS** (the guest OS doesn't know it's being translated)
- No hardware support required (runs on old x86 CPUs without VT-x)
- The translation cache is a performance optimization — hot paths get cached and run fast
- Higher overhead than paravirtualization or hardware-assisted virtualization, but feasible

**Used By:** VMware Workstation (on CPUs without VT-x support)

---

### ❓ Q&A — Section 5

**Q17. What is the "trap and emulate" approach to virtualization?**

> **A:** In trap-and-emulate, the guest OS runs at a lower privilege level (Ring 1) than it expects (Ring 0), while the VMM runs at Ring 0. Safe instructions execute directly on hardware. When the guest OS tries to execute a privileged instruction, the hardware generates a trap to Ring 0 (the VMM). The VMM then emulates the effect of that instruction and returns control to the guest. This gives the VMM control over all sensitive operations while allowing most code to run at native speed.

**Q18. What is the fundamental problem with trap-and-emulate on x86 (pre-2005)?**

> **A:** x86 has approximately 17 instructions that are "sensitive" (they affect or reveal privileged system state) but are NOT "privileged" (they don't generate a trap when executed in Ring 1 or Ring 3). The classic example is `POPF`, which in Ring 1 silently ignores the attempt to modify the Interrupt Enable Flag without generating a trap. The VMM therefore never gets to intercept these instructions, breaking the virtualization guarantee.

**Q19. Explain the problem with `POPF` in the context of x86 virtualization.**

> **A:** `POPF` pops a value from the stack and loads it into the EFLAGS register. In Ring 0, this can modify the Interrupt Enable Flag (IF), which controls whether hardware interrupts are delivered — a critical system operation. In Ring 1, when a guest OS (running deprivileged) executes `POPF` attempting to disable interrupts, the CPU silently ignores the IF bit modification. No trap is generated, so the VMM never intercepts it. The guest OS incorrectly believes it has disabled interrupts, leading to incorrect behavior.

**Q20. What is binary translation? How does it solve the x86 virtualization problem?**

> **A:** Binary translation is a dynamic technique where the VMM scans guest OS code before execution and translates any "sensitive but not privileged" instructions into safe equivalents. For example, `POPF` might be replaced with a call-out to the VMM or an inline emulation sequence. The translated code is cached in a Translation Cache for reuse. Because the problematic instructions are replaced before execution, they never reach the CPU directly, circumventing the x86 architectural limitation. This enables full virtualization of unmodified guest OSes on x86 without hardware VT support.

**Q21. What is a "translation cache" in binary translation?**

> **A:** A translation cache (or Translation Code Cache, TCC) is a memory region maintained by the VMM where translated code fragments (basic blocks) are stored. When a guest code block is first encountered, it is translated and stored in the cache. On subsequent executions of the same block, the cached translation is used directly, avoiding re-translation and improving performance.

**Q22. What does "strictly virtualizable" mean?**

> **A:** A processor is strictly virtualizable if, when code executes at a lower privilege level than expected: (1) all instructions that access privileged state generate a trap to the VMM, and (2) all other instructions execute identically to their normal behavior. A strictly virtualizable processor supports perfect trap-and-emulate virtualization. Pre-2005 x86 is NOT strictly virtualizable due to its sensitive-but-not-privileged instructions.

**Q23. Why was x86 not originally designed to be virtualizable?**

> **A:** x86 was designed in the 1970s–80s primarily for personal computers, where virtualization was not a priority. The designers did not foresee the need to run multiple isolated OS instances on the same hardware. As a result, some instructions were designed to behave differently based on privilege level (silently ignoring certain operations in user mode rather than trapping), which violates the requirements for clean trap-and-emulate virtualization. IBM's mainframe architecture (System/370) was intentionally designed with virtualization in mind, which is why IBM's VM/370 hypervisor worked cleanly.

---

## 6. Paravirtualization and Transparent Virtualization

These are the two high-level **virtualization strategies**, each with different tradeoffs.

### 6.1 Full (Transparent) Virtualization

**Definition:** Full virtualization (also called transparent virtualization) provides a **complete simulation of the underlying hardware**. The guest OS runs **completely unmodified** — it does not know it is being virtualized.

**How it works:**

- The VMM completely emulates all hardware behavior
- Sensitive instructions are handled via binary translation (software) or hardware trapping (VT-x)
- The guest OS behaves exactly as if it were on real hardware

**Stack:**

```
+------------------+   +------------------+
|   Applications   |   |   Applications   |
+------------------+   +------------------+
|   Guest OS       |   |   Guest OS       |  ← UNMODIFIED
| (Standard OS)    |   | (Standard OS)    |
+------------------+   +------------------+
+-------------------------------------------+
|         Hypervisor/VMM                    |
+-------------------------------------------+
|              X86 Hardware                 |
+-------------------------------------------+
```

**Key Difference from Paravirtualization:** Full virtualization handles privileged/sensitive instructions at **runtime** (as they execute), either by trapping them (hardware assist) or by translating them ahead of time (binary translation).

**Advantages:**

- Guest OS runs **without any modification** — can virtualize any standard OS including closed-source ones (e.g., Windows)
- Maximum compatibility

**Disadvantages:**

- Higher overhead compared to paravirtualization
- Binary translation is complex and adds latency

**Examples:** VMware Workstation (binary translation), Oracle VirtualBox, KVM (with VT-x)

---

### 6.2 Paravirtualization

**Definition:** Paravirtualization presents a software interface to VMs that is **similar but not identical** to the underlying hardware. The guest OS is **explicitly modified** to be aware of running in a virtual environment.

**The Key Insight:**

Instead of the hypervisor silently intercepting and emulating privileged instructions at runtime (which is complex and slow), paravirtualization **modifies the guest OS at compile time** to never issue those problematic instructions — instead replacing them with **hypercalls** to the VMM.

```
Full Virtualization:  Guest OS issues privileged instruction → VMM intercepts at runtime → emulates
Paravirtualization:   Guest OS compiled to issue hypercall directly → VMM handles it
```

**Hypercalls:**

- A **hypercall** is to the VMM what a **system call** is to the OS
- A hypercall is a direct API call from the guest OS to the VMM, requesting a privileged service
- More efficient than trapping because there's no "accidental" instruction interception — the VMM knows exactly what's being requested

**Stack:**

```
+------------------+   +------------------+
|   Applications   |   |   Applications   |
+------------------+   +------------------+
|  Modified        |   |  Modified        |  ← MODIFIED GUEST OS
|  Guest OS        |   |  Guest OS        |     (aware of virtualization)
+------------------+   +------------------+
+-------------------------------------------+
|              Hypervisor/VMM               |
+-------------------------------------------+
|              X86 Hardware                 |
+-------------------------------------------+
```

**Privilege Ring Arrangement in Paravirtualization:**

```
Ring 3:  User applications           ← Direct execution
Ring 0:  Para-virtualized Guest OS   ← Runs in Ring 0 (!) but uses hypercalls
         ─────────────────────────
         Virtualization Layer (VMM)
         ─────────────────────────
         Host Computer Hardware
```

Wait — how can the Guest OS be in Ring 0? In paravirtualization, the guest OS can be engineered to run in Ring 0 of the non-root mode (in hardware-assisted paravirtualization), OR the guest OS code is modified so that any instruction that would be problematic makes a hypercall instead, maintaining proper isolation.

**The Mechanism:**

- Guest OS is compiled with changes: every privileged/sensitive instruction is **replaced at compile time** with a hypercall
- After replacement, the modified guest OS emulates the behavior of the original
- On UNIX: a system call involves an interrupt → in para-virtualized OS, hypercalls use a dedicated service routine instead

**Performance Advantage — The Oracle Example:**

Suppose a guest OS wants to start Oracle database, which needs 1GB of virtual memory. With 4KB page size: 1GB / 4KB = **250,000 page table entries**.

|Approach|Traps Generated|
|---|---|
|Pure Trap-and-Emulate|250,000 traps (one per page table entry update)|
|Paravirtualization (Xen writeable page tables)|~1,000 traps (one per page of the page table itself)|

**How the Xen page table optimization works:**

1. Guest OS page tables are initially **read-only**
2. When guest OS tries to write to page tables → trap (just once per page)
3. Xen makes that page temporarily writable → removes it from physical page table
4. Guest OS modifies all entries in that page freely
5. When done → guest OS makes a **hypercall** to Xen
6. Xen validates all entries and puts the page back in the physical page table

Result: 1,000 traps instead of 250,000 → **250x performance improvement** for this operation!

**Paravirtualization Architecture Diagram:**

```
                  Ring 3: [User Apps]  ← Direct execution of user requests
                  Ring 2: [        ]
                  Ring 1: [        ]
                  Ring 0: [Para-virtualized Guest OS] ← Makes hypercalls
                          ──────────────────────────
                          [Virtualization Layer / VMM]
                          ──────────────────────────
                          [Host Computer Hardware]
                                   ↑
                          Hypercalls to virtualization
                          layer replace non-virtualizable
                          OS instructions
```

---

### 6.3 Comparison: Full vs. Para Virtualization

|Aspect|Full (Transparent) Virtualization|Paravirtualization|
|---|---|---|
|**Guest OS modification**|Not required — runs unmodified|Required — OS must be modified|
|**Handling of sensitive instructions**|At **runtime** (trap or binary translation)|At **compile time** (replaced with hypercalls)|
|**Performance**|Lower (runtime overhead)|Higher (hypercalls are more efficient)|
|**Compatibility**|Any OS (including closed-source Windows)|Only OSes whose source code is available|
|**Implementation complexity**|Higher (VMM must handle anything)|Lower (VMM has a well-defined API)|
|**Examples**|VMware Workstation, Oracle VirtualBox, KVM|Xen hypervisor, VMware ESX (early)|
|**Source code needed?**|No|Yes (to modify the guest OS)|

**Issues with Paravirtualization:**

1. **Compatibility & portability** — modified OS must also support unmodified mode; not all OSes can be modified (e.g., closed-source Windows)
2. **High maintenance cost** — deep OS kernel modifications must be kept in sync with OS updates
3. **Variable performance advantage** — the benefit varies greatly with workload type

**Advantages of Paravirtualization:**

- More practical and efficient than full virtualization using binary translation
- Full virtualization's binary translation is complex and slow to optimize; paravirtualization avoids this entirely
- Xen, KVM, VMware ESX all use paravirtualization concepts today

---

### ❓ Q&A — Section 6

**Q24. What is the key difference between full virtualization and paravirtualization?**

> **A:** In full virtualization, the guest OS runs completely unmodified and the VMM handles all sensitive instructions at runtime through trapping or binary translation. In paravirtualization, the guest OS source code is modified at compile time to replace problematic instructions with hypercalls to the VMM. Paravirtualization is more efficient because the hypercall interface is explicit and optimized, whereas full virtualization must intercept and emulate unexpected instructions.

**Q25. What is a hypercall?**

> **A:** A hypercall is an explicit API call from a para-virtualized guest OS to the hypervisor (VMM), analogous to how a system call is an API from a user application to the OS kernel. Instead of the guest OS executing a privileged instruction and causing a trap, it directly calls the VMM's interface to request a privileged service. This is more efficient because the VMM knows exactly what's being requested and can optimize the handling.

**Q26. Why can't paravirtualization be used with Windows?**

> **A:** Paravirtualization requires modifying the guest OS source code to replace sensitive instructions with hypercalls. Microsoft Windows is proprietary and closed-source, so its kernel source code is not generally available for modification. Therefore, full virtualization (which can work with any unmodified OS) must be used to virtualize Windows.

**Q27. Explain the Oracle example that demonstrates paravirtualization's performance advantage.**

> **A:** When starting Oracle database (needing 1GB virtual memory with 4KB pages), 250,000 page table entries must be set up. In pure trap-and-emulate, each update to the page table triggers a trap to the VMM — resulting in 250,000 traps. With Xen's paravirtualized writeable page tables, the entire 4MB page table (occupying ~1,000 physical pages) is handled per-page: the first write to each page triggers one trap, Xen briefly makes the page writeable, the guest fills all entries in that page, then makes one hypercall. Total: ~1,000 traps instead of 250,000 — a 250x reduction.

**Q28. Does paravirtualization require modification of user applications?**

> **A:** No. Only the **guest OS kernel** is modified in paravirtualization. User applications running inside the para-virtualized VM do not need any changes and work normally, because they interact with the (modified) guest OS through the standard system call interface.

---

## 7. Techniques to Virtualize x86

This section summarizes the three main techniques specifically applied to the x86 architecture:

### Technique 1: Paravirtualization

- **Strategy:** Rewrite guest OS code to be virtualizable
- **Mechanism:** Guest OS makes "hypercalls" to VMM instead of executing privileged instructions
- **Requirement:** Guest OS source code must be available and modifiable
- **Can work with unmodified OS?** ❌ No
- **Example:** Xen hypervisor (XenoLinux, XenoWindows variants)

### Technique 2: Full Virtualization (Binary Translation)

- **Strategy:** CPU instructions of the guest OS are dynamically translated to be virtualizable
- **Mechanism:** Sensitive instructions are replaced with traps to VMM before execution; all other instructions run natively
- **Requirement:** None for the guest OS (works with unmodified OSes)
- **Can work with unmodified OS?** ✅ Yes
- **Performance:** Higher overhead than paravirtualization (binary translation is expensive)
- **Example:** VMware Workstation (on non-VT-x hardware)

### Technique 3: Hardware-Assisted Virtualization

- **Strategy:** The CPU hardware itself is extended to support virtualization natively
- **Mechanism:** Intel VT-x / AMD-V extensions add a new CPU mode (VMX root / non-root) that makes ALL sensitive instructions trap correctly to the VMM
- **Requirement:** CPU must support VT-x or AMD-V (2005+ hardware)
- **Can work with unmodified OS?** ✅ Yes
- **Performance:** Best performance — no binary translation needed, hardware handles trapping
- **Example:** KVM in Linux, modern Xen, modern VMware

**The Workaround for x86-32 (Paravirtualization specifics):**

- Block all 17 problematic instructions: modify the guest OS to `#undef` (remove/replace) them
- Provide hypercall interface from guest OS to VMM as replacements
- Hypercalls are like system calls but from guest OS layer to VMM layer

---

## 8. Hardware-Assisted Virtualization (Intel VT-x / AMD-V)

This is the modern, clean solution to the x86 virtualization problem. Intel introduced **VT-x** (Virtualization Technology for x86) in 2005; AMD introduced **AMD-V** (AMD Virtualization) simultaneously.

### The Two Challenges Being Solved

1. **How to hide system/privileged state from the VM?**
    
    - A VM must think it has its own CPU registers, control registers, interrupt state, etc.
    - But it must not actually have full access to physical hardware state
2. **How to ensure a VM cannot directly change system state of the processor?**
    
    - E.g., a VM should not be able to disable hardware interrupts for the real CPU
    - Without this, a single misbehaving VM could freeze the entire system

### The Solution: VMX Mode (Virtual Machine Extensions)

Intel VT-x introduces a completely new **VMX execution mode** for the CPU. This mode has **two sub-modes** called "VMX root" and "VMX non-root":

```
+──────────────────────────────────────────────────────────────────+
│                      VMX Non-Root Mode                           │
│  (VMs run here)                                                  │
│  +────────────────────────+   +────────────────────────+         │
│  │         VM 0           │   │         VM 1           │         │
│  │  Ring 3: Guest apps    │   │  Ring 3: Guest apps    │         │
│  │  Ring 2:               │   │  Ring 2:               │         │
│  │  Ring 1:               │   │  Ring 1:               │         │
│  │  Ring 0: Guest OS      │   │  Ring 0: Guest OS      │         │
│  +────────────────────────+   +────────────────────────+         │
+──────────────────────────────────────────────────────────────────+
         ↑ VM Exit          ↓ VM Entry
+──────────────────────────────────────────────────────────────────+
│                      VMX Root Mode                               │
│  (VMM/Hypervisor runs here)                                      │
│  Ring 3: Legacy/un-virtualized user programs                     │
│  Ring 2:                                                         │
│  Ring 1:                                                         │
│  Ring 0: VMM / Hypervisor                                        │
+──────────────────────────────────────────────────────────────────+
                          Hardware
```

### Key Properties

- **Two completely separate modes:** VMX Root and VMX Non-Root
- **Each mode has its own complete set of rings (0–3)**
- **Hardware state is DUPLICATED:** each VM has its own copy of system registers (RFLAGS, CR0, CR3, etc.) maintained by the hardware
- **Hypervisor runs in VMX Root Mode, Ring 0**
- **Guest OS runs in VMX Non-Root Mode, Ring 0** — yes, Ring 0! — but this is "non-root Ring 0," which is restricted
- **Guest applications run in VMX Non-Root Mode, Ring 3**

### Why "Non-Root Ring 0" is Safe

In VMX non-root mode, **the processor's behavior is restricted and modified** to facilitate virtualization:

- Certain instructions (like `VMCALL`) and certain events cause **VM Exits** — automatic transitions from non-root mode back to the VMM in root mode
- Because these VM exits replace ordinary instruction behavior, the functionality of software in VMX non-root operation is limited
- This limitation is what allows the VMM to **retain control of processor resources**

In other words: the guest OS thinks it's in Ring 0 with full privilege, but the hardware enforces restrictions — sensitive operations cause VM exits to the VMM rather than executing directly.

### The VMCS (Virtual Machine Control Structure)

The VMCS is a **hardware data structure** (a 4KB memory region) that controls the behavior of VM transitions. It stores:

- **Guest state:** CPU register values to restore when entering the VM (VMCS guest area)
- **Host state:** CPU register values to restore when exiting to the VMM
- **Execution control fields:** Which events/instructions cause VM exits
- **Exit information:** What caused the last VM exit and relevant details

The VMCS is accessed via special instructions:

- `VMREAD` — read a field from the VMCS
- `VMWRITE` — write a field to the VMCS

### VT-x Instruction Set

|Instruction|Purpose|
|---|---|
|`VMXON`|Enable VMX mode|
|`VMXOFF`|Disable VMX mode|
|`VMLAUNCH`|Enter VMX non-root (launch a new VM)|
|`VMRESUME`|Re-enter VMX non-root (resume a VM after VM exit)|
|`VMEXIT`|(Automatic) Transition from non-root to root mode|
|`VMCALL`|Guest-initiated call to VMM (like a hypercall)|
|`VMREAD`|Read from VMCS|
|`VMWRITE`|Write to VMCS|

### KVM's Use of VT-x — Step-by-Step

```
Step 1: [VMM] initializes/modifies VMCS
        (sets up guest CPU state, host state, which events cause exits)
        
Step 2: [VMM] executes VMLAUNCH or VMRESUME
        → CPU switches to VMX non-root mode
        → Restores guest CPU state from VMCS
        
Step 3: [VM] runs until it hits a vmexit condition
        (e.g., executes CPUID, accesses unmapped memory, executes IO instruction,
         receives interrupt, executes privileged instruction in non-root mode)
        
Step 4: [CPU] automatically performs VM Exit
        → Saves guest state to VMCS
        → Restores host (VMM) state from VMCS
        → Jumps to VMM's exit handler
        
Step 5: [VMM] analyzes the exit reason (reads VMCS exit fields)
        → Takes appropriate action (emulate IO, handle interrupt, etc.)
        
Step 6: [VMM] goes back to Step 1/2 to resume the VM
```

### Current Status of Hardware Assist

- **Almost all modern VMMs use hardware assistance** (VT-x or AMD-V) today
- **KVM:** Fully integrated into Linux kernel; makes heavy use of hardware assist
- **Xen:** Originally pure paravirtualization; now uses hardware assist for full virtualization
- **VMware Workstation:** Originally used binary translation; now uses VT-x on supported hardware
- Hardware assist makes virtualization performance nearly identical to native execution for most workloads

---

### ❓ Q&A — Section 8

**Q29. What problem does Intel VT-x (hardware-assisted virtualization) solve?**

> **A:** VT-x solves the fundamental x86 virtualization problem caused by the 17 sensitive-but-not-privileged instructions. By introducing VMX root and non-root modes with duplicated hardware state, VT-x ensures that ALL sensitive instructions executed in VMX non-root mode either execute on the VM's own duplicated state (isolation) or automatically generate a VM Exit to the VMM. This makes x86 strictly virtualizable without requiring binary translation or guest OS modification.

**Q30. What are VMX Root Mode and VMX Non-Root Mode?**

> **A:** These are two new execution modes introduced by Intel VT-x. **VMX Root Mode** is where the hypervisor (VMM) runs — it's privileged and has full hardware access. **VMX Non-Root Mode** is where guest VMs run — it has its own set of rings (0–3) but with restricted behavior: certain instructions and events cause automatic "VM Exits" to the VMM. Hardware state (CPU registers, control registers) is duplicated for each mode, so VMs have their own isolated state.

**Q31. What is a VMCS?**

> **A:** The Virtual Machine Control Structure (VMCS) is a hardware data structure (4KB memory region) managed by VT-x that controls VM transitions. It stores: (1) guest CPU state to restore on VM Entry, (2) host CPU state to restore on VM Exit, (3) execution control fields specifying which events cause VM Exits, and (4) exit information describing what triggered the last VM Exit. It is accessed using `VMREAD` and `VMWRITE` instructions.

**Q32. What is a VM Exit?**

> **A:** A VM Exit is an automatic hardware-triggered transition from VMX non-root mode (where the VM runs) back to VMX root mode (where the VMM runs). VM Exits occur when specific events happen — such as executing `VMCALL`, `CPUID`, `HLT`, performing I/O operations, receiving certain interrupts, or accessing certain memory regions. On a VM Exit, the CPU saves the guest's current state to the VMCS and restores the VMM's state, then jumps to the VMM's exit handler.

**Q33. Why can the guest OS run in Ring 0 of VMX non-root mode safely?**

> **A:** In VMX non-root mode, the processor's behavior is fundamentally restricted — not all Ring 0 operations work as they would in VMX root mode. Specifically, sensitive operations cause VM Exits to the VMM rather than executing directly. The hardware maintains separate, duplicated state for the VM, so the guest OS's Ring 0 operations affect only its own isolated state, not the real hardware state. The VMM in VMX root mode retains ultimate control.

**Q34. What are the four steps of KVM's VM execution loop using VT-x?**

> **A:** (1) **VMM initializes/modifies the VMCS** — sets up guest CPU state and exit conditions. (2) **VMM executes `VMLAUNCH`/`VMRESUME`** — CPU enters VMX non-root mode, restores guest state from VMCS, and guest starts running. (3) **VM runs until a VM Exit condition is hit** — such as a sensitive instruction or I/O operation. (4) **VMM analyzes the VM Exit** — reads the exit reason from VMCS, performs appropriate emulation or handling, then loops back to step 1/2.

---

## 9. Real-World Hypervisor Architectures

### 9.1 Xen Architecture

Xen is a **Type 1 bare-metal hypervisor** that pioneered paravirtualization.

**Core Concept:**

- VMs in Xen are called **domains**
- There is one special domain called **Domain 0 (Dom0)** and one or more guest domains (**DomU**)

**Dom0 (Domain 0):**

- A special, privileged VM that runs **first** when Xen boots
- Based on **Linux** (XenoLinux) or Windows (XenoWindows)
- Has **direct access to hardware** (unlike guest domains)
- **Handles ALL I/O** on behalf of guest domains (acts as an I/O proxy)
- Contains drivers for all real hardware devices
- Manages guest domain lifecycle (create, destroy, configure)

**Guest Domains (DomU):**

- Unprivileged VMs running para-virtualized or hardware-virtualized OSes
- Cannot access hardware directly — must request I/O through Dom0 via the **Xen split-driver model**

**Xen Hypervisor (the thin layer between hardware and domains):**

- Handles everything except I/O (which Dom0 handles)
- Performs: CPU scheduling, memory management, interrupt routing
- Techniques used:
    - **Trap and Emulate** (for compatible instructions)
    - **Binary Translation** (for sensitive-but-not-privileged instructions on older hardware)
    - **Sensitive instructions replaced by hypervisor calls** (hypercalls)

**Xen Architecture Diagram:**

```
+──────────────────────────────────────────────────────────────────+
│                    DomU (Guest Domain)          DomU             │
│              App App App         App App App App App App App     │
│              XenoLinux                 XenoWindows               │
+──────────────────────────────────────────────────────────────────+
│       Dom0 (Control Domain)                                      │
│   App App App (Management Tools)                                 │
│       Domain0 (Linux-based)                                      │
+──────────────────────────────────────────────────────────────────+
│                     XEN (Hypervisor)                             │
+──────────────────────────────────────────────────────────────────+
│                     Hardware Devices                             │
+──────────────────────────────────────────────────────────────────+
```

---

### 9.2 KVM Architecture

KVM (Kernel-Based Virtual Machine) is a **Linux kernel module** that turns the Linux kernel itself into a Type 1 hypervisor.

**Architecture:**

```
+──────────────────────────────────────────────────────────────────+
│        Hypervisor Process (QEMU user-space)                      │
│   +──────────────────────────+                                   │
│   │    Guest Memory          │                                   │
│   │  VCPU   VCPU             │   Thread  Thread                  │
│   │  Thread Thread           │   (I/O workers)                   │
│   +──────────────────────────+                                   │
+──────────────────────────────────────────────────────────────────+
│          KVM (kernel module)    │    Linux Kernel                 │
│    (handles VMX, VT-x ops)      │   (scheduler, paging, drivers) │
+──────────────────────────────────────────────────────────────────+
│        CPU          CPU                  CPU                     │
│  (VT-x used) (VT-x used)              (regular)                 │
+──────────────────────────────────────────────────────────────────+
```

**Key Properties:**

- **Extends Linux kernel** to add hypervisor functionality — the KVM module adds IOCTL system calls that allow user-space (QEMU) to control VM execution
- **Leverages Linux code** for scheduling (each vCPU is a Linux thread scheduled by the kernel), memory management (VM memory is Linux process memory), device drivers, security, etc.
- **Uses VT-x and EPT** (Extended Page Tables for hardware-accelerated memory virtualization)
- **Each VM appears as a Linux process** to the kernel's scheduler and resource manager
- This design makes KVM much simpler than standalone hypervisors — it reuses millions of lines of battle-tested Linux code
- KVM is a **hardware-assisted para-virtualization tool** — it uses hardware assist (VT-x) and supports both modified (para-virtualized) and unmodified guest OSes (Windows, Linux, Solaris, etc.)

---

### 9.3 VMware ESX Server

VMware ESX is a **Type 1 bare-metal hypervisor** for enterprise x86 SMP servers, using a para-virtualization architecture.

**Architecture:**

```
+──────────────────────────────────────────────────────────────────+
│  Guest OS  Guest OS  Guest OS  Guest OS        Console OS        │
+────────────+────────+──────────+────────+─────────────────────────+
│   VMM       VMM        VMM       VMM                             │
│ ←────────────────────────────────────────────→  ↑               │
│      VMkernel                                    │               │
│ [Scheduler][Memory Mgmt][SCSI Driver][Ethernet]  │ Console       │
+──────────────────────────────────────────────────+               │
│              x86 SMP Hardware                                    │
│      CPU   CPU   CPU       Memory     Disk    NIC                │
+──────────────────────────────────────────────────────────────────+
```

**Four Components of an ESX-enabled Server:**

1. **Virtualization Layer (VMM per VM):**
    
    - Each VM has its own VMM
    - VMM layer virtualizes physical hardware resources (CPU, memory, network, disk controllers, HIDs)
    - Every VM gets its own set of virtual hardware resources
2. **Resource Manager (VMkernel):**
    
    - Allocates CPU, memory, disk I/O, and network bandwidth
    - Maps physical resources to each VM's virtual hardware set
    - Includes scheduler, memory manager, SCSI driver, Ethernet driver
3. **Hardware Interface Components:**
    
    - Device drivers for all real hardware
    - VMware ESX Server File System (VMFS) for VM disk storage
4. **Service Console:**
    
    - Responsible for **booting** the ESX system
    - **Initiates** VMM and resource manager execution
    - **Relinquishes control** to the VMM and resource manager after boot
    - Provides a management interface for system administrators

**Key Design:** ESX uses para-virtualization where the VM kernel (VMkernel) interacts **directly with hardware without involving a host OS**. This gives it maximum performance and control.

---

### ❓ Q&A — Section 9

**Q35. What is Dom0 in Xen and why is it important?**

> **A:** Dom0 (Domain 0) is a special privileged VM in the Xen hypervisor that starts first during system boot. Unlike guest domains (DomU), Dom0 has direct access to hardware devices and handles ALL I/O operations on behalf of other VMs. It is based on Linux and serves as the control domain — managing guest domain lifecycle (creation, configuration, destruction) and routing I/O between guest VMs and physical hardware via Xen's split-driver model.

**Q36. Why is KVM simpler to implement than a standalone hypervisor?**

> **A:** KVM reuses the entire Linux kernel infrastructure. CPU scheduling uses Linux's existing scheduler (each vCPU is a Linux thread), memory management uses Linux's virtual memory system (VM memory is process memory), device drivers are Linux drivers, security uses Linux's security framework, and networking uses Linux's network stack. This means the KVM team only had to implement the VMX-specific code (the VT-x mode switching and VMCS management), not a complete operating system from scratch.

**Q37. What are the four components of VMware ESX Server architecture?**

> **A:** (1) **Virtualization Layer** — per-VM VMMs that virtualize CPU, memory, and I/O devices for each VM; (2) **Resource Manager (VMkernel)** — allocates and maps physical CPU, memory, disk, and network resources to VMs; (3) **Hardware Interface Components** — device drivers and VMware ESX Server File System (VMFS); (4) **Service Console** — boots the system, initiates VMM and resource manager, and provides the administrator management interface.

---

## 10. Memory Virtualization — Shadow Page Tables

> This topic bridges the trap-and-emulate section with real-world implementation. Understanding memory virtualization is essential for complete understanding.

### The Problem

In a non-virtualized system:

```
Virtual Address (VA) → [Page Table managed by OS] → Physical Address (PA)
```

In a virtualized system, there are **two layers of address translation**:

```
Guest Virtual Address (GVA) → [Guest Page Table] → Guest Physical Address (GPA)
Guest Physical Address (GPA) → [VMM Page Table]   → Host Physical Address (HPA)
```

- **GVA:** Address used by programs in the guest OS
- **GPA:** What the guest OS thinks is a "physical" address (it's actually fake/virtual)
- **HPA:** The real physical address on the actual hardware

The guest OS manages its own page table (GVA→GPA), but GPA is not real physical memory — the VMM manages the mapping from GPA→HPA. This two-level indirection must be resolved efficiently.

### Shadow Page Tables

**Shadow page tables** are the software solution maintained by the VMM that directly maps **Guest Virtual Addresses (GVA) → Host Physical Addresses (HPA)**, combining both levels of translation into one.

**The Walk-Through (Trap and Emulate Memory Example):**

Suppose the guest OS wants to map virtual page `gvp` to physical page `gpp`:

```
Step 1: Guest OS finds unused page gpp in "VM memory" (GPA space)
Step 2: Guest OS tries to write (gvp → gpp) into its page table
        → This is a privileged operation (writing page table base) → TRAP to VMM
Step 3: VMM (hypervisor) intercepts the trap
Step 4: VMM maps gpp (guest physical) to hpp (host physical page) in its own table
Step 5: VMM updates the shadow page table: gvp → hpp (bypassing the GPA level)
Step 6: Other page table operations (reads, modifications) must also be trapped and handled
Step 7: TLB manipulation is also handled (not shown here for simplicity)
```

**Shadow Page Table Structure:**

```
Guest VM Memory:                               Physical Memory:
+──────────────────────────────+
│  Guest Virtual Memory        │                 hpp ← actual physical page
│        gvp                   │                 ↑
│                              │                 │
│  Guest Page Table:           │        VMM/Host Page Tables:
│  gpp → gvp                   │        gpp → hpp
│                              │
│  Guest Physical Mem (Fake):  │
│        gpp                   │
+──────────────────────────────+

Shadow Page Table (maintained by VMM):
gvp → hpp  ← directly maps guest virtual to host physical
```

The "Shadow Page Table in Green" (as the slides note) is the VMM-maintained table that the CPU TLB actually uses — bypassing the two-level indirection for performance.

### Hardware Solution: Extended Page Tables (EPT / Nested Paging)

Modern processors (Intel VT-x with EPT, AMD-V with NPT) provide **hardware support** for two-level page table walking:

- The CPU hardware can walk both the guest page table AND the host page table automatically
- No need for software-managed shadow page tables
- Eliminates the need to trap every guest page table modification
- Significant performance improvement over shadow page tables

This is why KVM uses **EPT** (Extended Page Tables) as mentioned in the slides.

---

### ❓ Q&A — Section 10

**Q38. What is the "two-level address translation" problem in memory virtualization?**

> **A:** In a virtualized system, the guest OS manages a mapping from Guest Virtual Addresses (GVA) to Guest Physical Addresses (GPA). However, GPA is not real physical memory — the VMM maintains a second mapping from GPA to Host Physical Addresses (HPA). This creates a two-level translation chain (GVA → GPA → HPA) that the CPU must traverse, adding overhead compared to the single-level translation in non-virtualized systems.

**Q39. What is a shadow page table?**

> **A:** A shadow page table is a data structure maintained by the VMM that combines both levels of address translation into a single direct mapping from Guest Virtual Addresses (GVA) to Host Physical Addresses (HPA). The CPU's TLB is loaded with shadow page table entries, so actual memory accesses don't need to traverse two-level translation. The VMM intercepts all guest OS modifications to its page tables (via traps) and updates the shadow page table accordingly.

**Q40. What is EPT (Extended Page Tables) and why is it better than shadow page tables?**

> **A:** EPT (Intel's Extended Page Tables, AMD's equivalent is Nested Page Tables/NPT) is a hardware feature introduced with VT-x that allows the CPU to automatically walk two levels of page tables — the guest page table AND the host page table — entirely in hardware. This eliminates the need for software-managed shadow page tables and the associated traps on every guest page table modification, significantly reducing memory virtualization overhead.

---

## 11. Master Q&A Bank

This section consolidates all questions for exam preparation, organized by difficulty.

---

### 🟢 Level 1 — Basic / Definition Questions

**Q1.** What is virtualization?

> Virtualization is the abstraction of physical hardware resources into logical views, allowing multiple independent environments to share the same physical hardware simultaneously.

**Q2.** What is a VMM / Hypervisor?

> A Virtual Machine Monitor (VMM), or hypervisor, is software that creates, runs, and manages virtual machines — providing each VM with the illusion of dedicated physical hardware while multiplexing and isolating multiple VMs on shared physical resources.

**Q3.** What is a privilege ring?

> A hardware-enforced privilege level in x86 CPUs. Ring 0 is the most privileged (kernel mode) and Ring 3 is the least privileged (user mode). The CPU uses rings to restrict which instructions can be executed by which software, enforcing isolation between OS and user applications.

**Q4.** What is a General Protection Fault?

> A GPF is a hardware exception generated when software tries to perform an operation not allowed at its current privilege level, such as executing a privileged instruction from Ring 3. The CPU traps this to the OS (or VMM) for handling.

**Q5.** What is a hypercall?

> A hypercall is a call from a para-virtualized guest OS to the hypervisor (VMM), analogous to a system call from an application to the OS kernel. Instead of executing a privileged instruction (which would cause a slow, unexpected trap), the guest OS explicitly calls the VMM's API to request a privileged service.

**Q6.** What does "unmodified guest OS" mean in the context of full virtualization?

> It means the guest OS runs exactly as it was distributed — no changes to its binary or source code are required. The VMM transparently handles all privileged instructions through trapping (hardware assist) or binary translation (software).

**Q7.** Name three examples of Type 1 hypervisors.

> VMware ESXi, Microsoft Hyper-V, Citrix XenServer, Xen.

**Q8.** Name three examples of Type 2 hypervisors.

> VMware Workstation, Oracle VirtualBox, Microsoft Virtual PC.

---

### 🟡 Level 2 — Conceptual / Explanation Questions

**Q9.** Explain why x86 (pre-2005) is not strictly virtualizable.

> Pre-2005 x86 has ~17 instructions that are "sensitive" (they read/modify system state) but "not privileged" (they do not trap when executed in Ring 1 or Ring 3). For example, `POPF` in Ring 1 silently ignores attempts to modify the Interrupt Enable Flag. The VMM never intercepts these instructions, breaking the guarantee that all sensitive operations are controlled by the VMM. Popek & Goldberg's requirements for virtualization demand that all sensitive instructions trap — x86 violates this.

**Q10.** Describe the trap-and-emulate approach, including where each software component runs.

> The VMM runs in Ring 0 (kernel mode). The guest OS is deprivileged to Ring 1. Guest applications run in Ring 3. When the guest OS executes a privileged instruction (e.g., CR3 write), the CPU (seeing it from Ring 1) generates a trap to Ring 0, transferring control to the VMM. The VMM emulates the instruction's effect on a virtual CPU state and returns control to the guest OS. Safe instructions run directly on hardware at full speed.

**Q11.** How does binary translation handle the 17 problematic x86 instructions?

> Binary translation proactively scans guest OS code basic block by basic block before execution. When a problematic instruction is encountered: (1) safe instructions are copied as-is (ident translation), (2) simple problematic ones are replaced with short inline emulation sequences, (3) complex ones are replaced with call-outs to the VMM. Translated code is cached in a Translation Cache for reuse. Branches are re-translated to point to correct translated targets. This eliminates the need for those instructions to ever reach the CPU, bypassing the x86 architectural limitation.

**Q12.** Compare the performance of trap-and-emulate vs. paravirtualization using the Oracle page table example.

> For Oracle needing 1GB virtual memory with 4KB pages: (1) Trap-and-emulate causes one trap per page table entry update = 250,000 traps. (2) Paravirtualization (Xen writeable page tables) causes one trap per page of the page table = ~1,000 traps (250x fewer). Paravirtualization batches page table modifications within each physical page and uses a single hypercall at completion, dramatically reducing VMM overhead.

**Q13.** Why does VT-x allow the guest OS to run in Ring 0 of VMX non-root mode safely?

> In VMX non-root mode, even Ring 0 operations are restricted by hardware. Sensitive instructions (like those that would modify global system state) cause automatic VM Exits to the VMM in VMX root mode rather than executing normally. Additionally, hardware state is duplicated — the guest OS's Ring 0 operations affect only its VM's private copy of CPU state (maintained in the VMCS), not real hardware. The VMM in VMX root mode always retains ultimate control over real hardware state.

---

### 🔴 Level 3 — Advanced / Application Questions

**Q14.** A student claims: "With hardware-assisted virtualization (VT-x), we no longer need paravirtualization." Evaluate this claim.

> The claim is partially true but misleading. VT-x does allow full virtualization (unmodified guest OSes) with good performance by hardware-trapping all sensitive instructions. However, paravirtualization still offers advantages: (1) Some I/O-intensive workloads benefit from paravirtualized device drivers (virtio) which are more efficient than emulated devices. (2) Paravirtualized network and storage drivers reduce the number of VM Exits for I/O. (3) Modern systems use a hybrid approach — VT-x for CPU virtualization + paravirtualized I/O drivers for storage/networking. So while VT-x eliminates the need for paravirtual CPU instruction handling, paravirtualization concepts remain valuable for I/O performance.

**Q15.** Trace the complete execution path when a guest application running in a KVM-based VM on a VT-x CPU makes a `write()` system call to write data to disk.

> (1) Guest app (VMX non-root Ring 3) executes `SYSCALL` instruction → VM Exit to KVM. Wait — actually SYSCALL doesn't always cause a VM Exit; the VMM configures the VMCS to allow the guest OS to handle syscalls internally. (2) Guest app's SYSCALL transitions to guest OS kernel (VMX non-root Ring 0) via the guest's own system call handler. (3) Guest OS processes the write() call, interacts with its virtual block device driver. (4) Guest OS issues an I/O instruction (e.g., `OUT` to a virtual I/O port) → this DOES cause a VM Exit (I/O instruction in VMX non-root mode). (5) CPU saves guest state to VMCS, restores host state, jumps to KVM's VM Exit handler. (6) KVM reads the exit reason from VMCS (I/O operation), passes control to QEMU (user-space). (7) QEMU emulates the disk I/O by making a real `write()` syscall to the Linux kernel. (8) Linux kernel handles the actual disk write via real device driver. (9) QEMU returns control to KVM. (10) KVM executes VMRESUME → CPU restores guest state from VMCS → guest OS resumes in VMX non-root mode → returns to the guest application.

**Q16.** Why does paravirtualization have variable performance advantages depending on workload?

> The performance advantage of paravirtualization comes from replacing expensive trap-and-emulate operations with cheaper hypercalls. The advantage is large for I/O-intensive and memory-management-intensive workloads (many privileged instruction executions → many traps replaced by hypercalls). For compute-bound workloads (scientific calculations, floating point math), almost all instructions are non-privileged and run at native speed in both full and para-virtualization — so the performance difference is minimal. For workloads that barely touch privileged operations, there's essentially no difference.

**Q17.** Design a complete memory virtualization scheme for a system without EPT hardware support. What are the steps the VMM takes when the guest OS tries to map a new virtual page?

> Without EPT, the VMM uses shadow page tables: (1) Guest OS's page tables are marked read-only in the VMM's control structures. (2) When guest OS writes to its page table (attempting gvp → gpp mapping), the CPU generates a page fault or protection fault → traps to the VMM. (3) VMM inspects the faulting write: recognizes it as a guest page table update. (4) VMM checks its GPA→HPA mapping: finds or allocates host physical page hpp corresponding to gpp. (5) VMM updates its shadow page table: creates entry gvp → hpp (bypassing GPA entirely). (6) VMM may also update the TLB with this mapping for performance. (7) VMM returns to the guest OS, which believes it successfully updated its page table. The guest OS never knows its "physical" addresses are not real — the shadow page table makes the illusion complete.

---

### 📝 Short Answer Questions (2-3 marks each)

|#|Question|Answer|
|---|---|---|
|S1|What is the Popek-Goldberg theorem?|States that a VMM can be efficiently implemented for an ISA if all sensitive instructions are privileged (will trap when executed at lower privilege). x86 pre-2005 violated this.|
|S2|What is a Translation Cache in binary translation?|A memory region where translated guest code blocks are stored after first translation, allowing reuse without re-translation on subsequent executions.|
|S3|What is Dom0 in Xen?|A special privileged control domain in Xen (based on Linux) that boots first, handles all I/O, and manages guest domain lifecycle.|
|S4|What does EPT stand for and which company introduced it?|Extended Page Tables, introduced by Intel as part of VT-x. AMD's equivalent is Nested Page Tables (NPT).|
|S5|What is a VMCS?|Virtual Machine Control Structure — a hardware data structure that controls VM transitions, storing guest state, host state, execution controls, and exit information.|
|S6|In what ring does the VMM run in a Type 1 hypervisor setup?|Ring 0 (VMX Root Mode Ring 0 in VT-x-based systems)|
|S7|What is "ring deprivileging"?|The technique of running the guest OS at a lower privilege ring than it expects (e.g., Ring 1 instead of Ring 0) so that its privileged instructions trap to the VMM in Ring 0.|
|S8|Name two advantages of KVM over standalone hypervisors.|(1) Reuses Linux kernel infrastructure (scheduler, memory, drivers), reducing code complexity; (2) Benefits from all Linux kernel security updates and improvements automatically.|
|S9|What is "inline translation" in binary translation?|A translation where a simple problematic instruction is replaced by a short emulation sequence placed directly in the translation cache, without calling out to the VMM.|
|S10|What happens to branches during binary translation?|Branches must be re-translated: their target addresses in the original code are replaced with addresses of translated equivalents in the translation cache. The VMM tracks branch addresses to update them as more blocks are translated.|

---

## Summary Diagram: The Big Picture

```
                     ┌─────────────────────────────────────────────────┐
                     │           x86 VIRTUALIZATION APPROACHES         │
                     └─────────────────────────────────────────────────┘
                                          │
                  ┌───────────────────────┼───────────────────────┐
                  │                       │                       │
           ┌──────▼──────┐        ┌───────▼──────┐      ┌───────▼───────┐
           │    FULL     │        │    PARA      │      │   HARDWARE   │
           │VIRTUALIZATION│        │VIRTUALIZATION│      │  ASSISTED    │
           └──────┬──────┘        └───────┬──────┘      └───────┬───────┘
                  │                       │                      │
          ┌───────┴──────┐         Guest OS          Intel VT-x / AMD-V
          │              │         Modified           VMX Root + Non-Root
    Binary Translation  Trap &    (Hypercalls)         VMCS, VM Exits
    (VMware Workstation) Emulate       │                       │
          │              │        Xen, KVM          KVM, Xen (modern),
          │              │        VMware ESX         VMware (modern)
          └──────────────┘
         No hardware needed,     Source code needed,     HW support needed,
         works on old CPUs       better performance       best performance
```

---

_Notes compiled from PES University Cloud Computing lectures by Dr. Prafullata Kiran Auradkar, Unit 2 — Hardware Virtualization, Software Techniques, Paravirtualization, and Types of Hypervisors._