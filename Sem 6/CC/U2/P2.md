# â˜ï¸ Cloud Computing â€“ Complete Virtualization Study Notes

### Covers: VM Migration | Popek-Goldberg Principles | Memory & I/O Virtualization

**Based on PES University â€“ Department of Computer Science and Engineering**  
_Dr. Prafullata Kiran Auradkar_

---

> ğŸ¯ **How to use these notes:** Every major topic is explained from scratch, with analogies, diagrams in text form, and a complete Q&A bank at the end of each section. Read sequentially for best results.

---

# ğŸ“Œ TABLE OF CONTENTS

1. [Unit 1 â€“ VM Migration](https://claude.ai/chat/35311a45-391a-4a17-ba2d-b2e0caf95488#unit-1--vm-migration)
    
    - What is VM Migration?
    - Why Do We Migrate VMs?
    - Non-Live (Cold) Migration
    - Live Migration
    - Pre-Copy Technique (All 6 Stages)
    - Post-Copy Technique (Demand Paging, Active Push, Memory Prepaging)
    - Issues: Memory, File System, Network Migration
    - Xen Live Migration Example
    - â“ Q&A Bank
2. [Unit 2 â€“ Popek and Goldberg Virtualization Principles](https://claude.ai/chat/35311a45-391a-4a17-ba2d-b2e0caf95488#unit-2--popek-and-goldberg-virtualization-principles)
    
    - Background and Context
    - Terminologies (VM, VMM, Third-Gen Machines)
    - Essential Characteristics (Equivalence, Resource Control, Efficiency)
    - Instruction Classification (Privileged, Sensitive, Safe)
    - Theorems 1, 2, and 3
    - x86 and Popek-Goldberg Compliance
    - â“ Q&A Bank
3. [Unit 3 â€“ Memory and I/O Virtualization](https://claude.ai/chat/35311a45-391a-4a17-ba2d-b2e0caf95488#unit-3--memory-and-io-virtualization)
    
    - Memory in Unvirtualized Systems (Page Tables, TLB, PTW)
    - Why Memory Virtualization is Needed
    - Two-Level Address Translation (gVA â†’ gPA â†’ sPA)
    - Shadow Page Tables (How They Work, Challenges)
    - Nested/Extended Page Tables (Hardware Assisted)
    - EPT Performance
    - I/O Virtualization (3 Methods)
    - Xen I/O Virtualization
    - â“ Q&A Bank

---

---

# UNIT 1 â€“ VM MIGRATION

---

## 1.1 What is VM Migration?

**VM Migration** is the process of **moving a running or stopped Virtual Machine (VM) from one physical host (server) to another**, along with all its state â€” memory contents, CPU state, disk, network connections, etc.

Think of it like this: Imagine you are working on your laptop and suddenly need to move to another desk without shutting down or losing your work. That is exactly what VM migration aims to achieve â€” move the "work" (the VM) without interrupting it.

---

## 1.2 Types of VM Migration

```
VM Migration
â”œâ”€â”€ Cold Migration (VM is powered OFF)
â”‚     â””â”€â”€ Can migrate across different CPU families
â”‚
â”œâ”€â”€ Non-Live / Offline Migration (VM is PAUSED)
â”‚     â””â”€â”€ State transferred, then resumed at destination
â”‚
â””â”€â”€ Live / Hot Migration (VM keeps RUNNING during transfer)
      â””â”€â”€ Example: VMware vSphere vMotion
```

### 1.2.1 Cold Migration

- The VM is completely **powered off** before migration.
- Since the VM is not running, there is no risk of data inconsistency.
- Can move across **different CPU architectures or families** because there is no live execution state to worry about.
- Simplest form but causes complete service downtime.

### 1.2.2 Non-Live (Offline) Migration

- The VM is **paused (suspended)** at the source host.
- All state (memory, CPU registers, disk) is transferred to the destination.
- The VM is **resumed** at the destination after the transfer.
- **Drawback:** The VM is completely unavailable during transfer â€” this is called **downtime**. If the transfer is large, downtime can be significant.

### 1.2.3 Live (Hot) Migration

- The VM continues to **run** during migration â€” users notice nothing.
- Memory content and all VM definition data (BIOS, devices, MAC addresses, etc.) are transferred while the VM is alive.
- **No dropped network connections** â€” active TCP sessions remain open.
- Applications continue without interruption.
- **Example:** VMware vSphere **vMotion**.

---

## 1.3 Why Do We Migrate VMs?

There are many business and technical reasons to migrate VMs:

### ğŸ”§ 1. Hardware Maintenance

- When a physical server needs repair, upgrade, or replacement, VMs on it must be moved to other machines so the server can be taken offline safely.
- Live migration allows this without any service disruption.

### âš–ï¸ 2. Load Balancing

- If one server is overloaded (high CPU, high memory) while another is idle, VMs can be moved to distribute load evenly.
- Live migration enables **dynamic, real-time load balancing** across a cluster of nodes.

### ğŸ“ˆ 3. Scaling to Changing Workloads

- When an application suddenly needs more CPU or RAM than the current host has, the VM is migrated to a more capable physical server.
- Supports **auto scale-up** and **auto scale-down** of virtual clusters.

### ğŸ’° 4. Server Consolidation

- Many servers run at very low utilization (e.g., 10-20% CPU use).
- VMs can be consolidated onto fewer physical machines, allowing unused servers to be **powered off**, saving electricity and hardware costs.

### ğŸŒ 5. Workload Mobility (Compliance)

- Some applications have **legal or regulatory requirements** about where data must be stored (e.g., data must remain in a specific country).
- VMs may need to be moved to different physical locations to comply with these rules.

### âš¡ 6. Performance Optimization

- Migrate VMs away from servers with clogged networks or over-utilized resources to ones with better availability.
- This directly shortens **response time** for users.

### ğŸŒ± 7. Energy Efficiency (Sustainability)

- Data centers are major energy consumers.
- Migration can **consolidate VMs** onto fewer servers so others can enter low-power sleep states.
- Saves energy from servers, cooling/air-conditioning, and power delivery.

### ğŸ”„ 8. Availability and Business Continuity

- If a server is about to fail (hardware warning, temperature alerts), VMs can be proactively migrated to healthy servers.
- Prevents **unplanned downtime** due to equipment failures.

---

## 1.4 Non-Live (Cold/Offline) Migration â€” In Detail

```
SOURCE HOST                          DESTINATION HOST
+-----------+                        +-----------+
|  VM (PAUSED)  |  ----transfer---> |  VM (RESUME)  |
+-----------+                        +-----------+
```

**Process:**

1. The VM is **suspended** (paused) â€” all CPU state is frozen.
2. All memory pages are transferred to the destination (only once).
3. The VM's state is **resumed** on the destination.

**Advantages:**

- Simple to implement.
- Memory pages only need to be copied **once** (no dirty page re-copying).
- Migration time is **short and predictable**.

**Disadvantages:**

- The application/service is **completely unavailable** during the migration phase.
- Causes **larger downtime** â€” unacceptable for critical services.

---

## 1.5 Live Migration â€” Overview and Objectives

Live migration is the preferred method for production systems because applications have **availability expectations** â€” users expect zero or near-zero downtime.

**Core Idea:** Migrate the VM while it keeps running, such that the user application on the VM doesn't even know it was moved.

**Key Objectives:**

1. Use bandwidth efficiently to optimize application performance during transfer.
2. Minimize **application downtime** (ideally to milliseconds).
3. Reduce the **total transfer time** to the minimum possible.
4. Migrate without significantly impacting the user application.

**Key Challenge:** VM migration is **resource-intensive** â€” it requires:

- CPU cycles (to manage migration logic)
- Cache memory
- Memory capacity (for dirty page tracking)
- Network bandwidth (to transfer memory pages)

> âš ï¸ **SLA Impact:** Migration can degrade application performance. This is a real problem when Service Level Agreements (SLAs) and critical business objectives must be met.

**Two Live Migration Techniques:**

1. **Pre-Copy** â€” copy memory to destination while VM still runs at source
2. **Post-Copy** â€” move CPU state first, then fetch memory on demand

---

## 1.6 Pre-Copy Live Migration (Deep Dive)

**Core Idea:** The VM keeps running at the source. Memory pages are iteratively copied to the destination in rounds. At the end, the VM is briefly paused, final state is transferred, and execution resumes at the destination.

```
VM RUNNING ON HOST A
         |
         v
  [Stage 0: Pre-Migration]
  - VM active on Host A
  - Destination host may be pre-selected
  - Block devices mirrored, free resources maintained
         |
         v
  [Stage 1: Reservation]
  - Initialize a container (resources) on the target host
  - Reserve CPU, memory, network for the VM
         |
         v
  [Stage 2: Iterative Pre-Copy]  <--- OVERHEAD DUE TO COPYING
  - Enable shadow paging (track dirty pages)
  - Round 1: Copy ALL memory pages to destination
  - Round 2: Re-copy only pages that changed (dirty pages) in Round 1
  - Round 3: Re-copy dirty pages from Round 2
  - ... repeat until dirty set is small enough or threshold reached
         |
         v
  [Stage 3: Stop and Copy]  <--- DOWNTIME (VM OUT OF SERVICE)
  - Suspend VM on Host A
  - Copy final dirty pages + CPU state + device state
  - Generate ARP to redirect network traffic to Host B
  - Synchronize all remaining VM state to Host B
         |
         v
  [Stage 4: Commitment]
  - VM state on Host A is released (Host A is free)
         |
         v
  [Stage 5: Activation]
  VM RUNNING NORMALLY ON HOST B
  - VM starts on Host B
  - Connects to local devices
  - Resumes normal operation
```

### Stage-by-Stage Explanation:

#### Stage 0: Pre-Migration

- The VM is still running normally on Host A.
- An alternate physical host (Host B) may be pre-selected based on available resources.
- Block devices (disks) may be mirrored for consistency.
- Free resources are maintained on Host B.

#### Stage 1: Reservation

- A **container** (a reserved environment with allocated CPU and memory) is initialized on Host B.
- This is like booking a hotel room before you arrive â€” you know the room will be ready.

#### Stage 2: Iterative Pre-Copy

- **Shadow paging** is enabled: the VMM starts tracking which memory pages the VM is writing to (these become "dirty pages").
- **Round 1:** ALL memory pages of the VM are transferred to Host B. Meanwhile, the VM keeps running and modifying pages.
- **Round 2:** Only the pages that were modified (dirtied) since Round 1 are transferred again.
- This continues until either:
    - A **fixed threshold** (maximum number of rounds) is reached, or
    - The **dirty set becomes small enough** to transfer quickly in a final stop-and-copy.
- **Key insight:** The VM is NOT paused during this phase. It keeps running. The copying happens in the background.

#### Stage 3: Stop and Copy (The Downtime)

- The VM is **briefly suspended** on Host A â€” this is the **downtime window**.
- The last remaining dirty pages, CPU state, register values, and device states are all copied to Host B.
- An **ARP (Address Resolution Protocol) announcement** is sent to tell the network that the VM's MAC address is now at Host B (redirecting traffic).
- All remaining VM state is synchronized.

#### Stage 4: Commitment

- Host B confirms it has received all necessary state.
- The VM state on Host A is **released** (Host A's resources are freed).

#### Stage 5: Activation

- The VM **starts running** on Host B.
- It connects to local devices.
- Network traffic is now flowing to Host B.
- Normal operation resumes â€” users never noticed.

### Pre-Copy: Advantages vs. Disadvantages

|Aspect|Detail|
|---|---|
|âœ… **Low Downtime**|Only the final stop-and-copy phase causes downtime (milliseconds to seconds)|
|âœ… **Good for low write workloads**|If the VM doesn't write much memory, the dirty set shrinks quickly|
|âŒ **High total migration time**|Repeated re-copying of dirty pages means more bandwidth used overall|
|âŒ **Bad for write-heavy workloads**|If the VM writes a lot, the dirty set never shrinks â€” migration may not converge|

**Examples:** KVM, Xen, and VMware hypervisors all use the pre-copy technique.

---

## 1.7 Post-Copy Live Migration (Deep Dive)

**Core Idea:** Opposite of pre-copy. The CPU/processor state is transferred to the destination **first**. The VM starts running on the destination immediately. Memory pages are then fetched from the source **on demand** as the VM needs them.

```
SOURCE HOST                          DESTINATION HOST
+-----------+                        +-----------+
|   VM      |                        |           |
|           |  --[CPU state]-->      | VM STARTS |
|  (still   |                        | RUNNING   |
|   has     |  <--[page fault]--     |           |
|  memory)  |  --[missing page]-->   |           |
|           |                        |           |
|           |  --[active push]-->    |           |
+-----------+                        +-----------+
```

**Steps:**

1. **Initiation:** Migration starts, VM state is extracted.
2. **Send VM State to Target:** CPU registers, device state transferred to destination.
3. **Receiver (destination) resumes VM:** VM starts running on Host B immediately.
4. **Memory page fetching:** As the VM executes on Host B and needs memory pages that haven't arrived yet, **page faults** occur.
5. **Page fault handling:** Missing pages are fetched from Host A on demand.
6. **Active fetching of dirty pages:** Host B also proactively fetches memory pages from Host A.
7. When all pages are copied, migration is complete.

### Techniques to Handle Page Faults:

#### ğŸ”µ Demand Paging

- When VM on Host B tries to access a page it doesn't have â†’ **page fault**.
- The fault is handled by fetching the required page from Host A.
- Simple, but slow â€” every missing page access stalls the VM until the page arrives.
- Degrades application performance but is the simplest approach.

#### ğŸŸ¢ Active Push

- Host A **proactively sends** pages to Host B even before they are explicitly requested.
- This removes **residual dependencies** on Host A.
- If a page fault still occurs for a page being actively pushed, demand paging handles it.
- **Key property:** Pages are sent **only once** â€” either via active push OR via demand paging, never both.

#### ğŸŸ¡ Memory Prepaging

- Assumes **temporal and spatial locality** of memory access.
- Predicts which pages the VM is likely to need next based on past access patterns.
- Proactively pushes those pages to Host B before they are needed.
- More efficient â€” reduces the number of page faults by anticipating the VM's needs.

### Post-Copy: Disadvantages

|Disadvantage|Explanation|
|---|---|
|âŒ **Page fault overhead**|Every missing page access stalls the VM â€” affects performance|
|âŒ **Network overhead**|Large amount of data (memory + CPU state) needs to move|
|âŒ **Repetitive page fault detection**|The cycle of detecting and resolving faults is complex and adds overhead|
|âŒ **VM vulnerable if source fails**|If Host A goes down before all pages are copied, the VM on Host B loses data|

---

## 1.8 Issues to Be Handled with VM Migration

### 1.8.1 Memory Migration Issue

- Modern VMs can have memory ranging from **hundreds of MB to several GB**.
- Transferring this efficiently is non-trivial.

**Internet Suspend-Resume (ISR) Technique:**

- Exploits **temporal locality**: the memory state of a VM when it resumes is likely very similar to its state when it was last suspended.
- "Temporal locality" means the memory pages that change are only those modified since the VM was last suspended.
- Uses a **tree structure of small subfiles** that exists in both the suspended and resumed VM instances.
- Only the **changed files (delta)** are transferred, not the entire memory.
- **Caveat:** While this saves bandwidth, the downtime may still be high.

### 1.8.2 File System Migration Issue

- VMs need a consistent view of the file system regardless of which physical host they are on.
- Two approaches:

**Approach 1 â€” Virtual Disk Migration:**

- Each VM has its own **virtual disk** containing its file system.
- During migration, the virtual disk contents are transferred along with other VM state.
- With modern high-capacity network links, this is feasible.

**Approach 2 â€” Global/Shared File System:**

- All possible host machines share a **common network file system** (e.g., NFS, SAN).
- All files are accessible from any host over the network.
- **No file copying is needed** during migration â€” the VM simply connects to the same shared file system from the new host.

### 1.8.3 Network Migration Issue

- The migrating VM must **maintain all open network connections** â€” active TCP sessions, ongoing transfers, etc.
- This cannot rely on forwarding at the old host (because the old host is being vacated).

**Solution â€” Virtual IP and MAC Addresses:**

- Each VM is assigned a **virtual IP address** (separate from the physical host's IP).
- Each VM has its own **virtual MAC address**.
- The **VMM (Virtual Machine Monitor)** maintains a mapping of these virtual addresses to the actual underlying hardware.
- When a VM migrates, it **carries its virtual IP and MAC address with it**.
- An ARP announcement tells the network about the new physical location.
- Result: External systems continue communicating with the same virtual IP â€” they don't know the VM moved.

---

## 1.9 Xen Live Migration â€” Detailed Example

**Xen** is an open-source hypervisor. Let's see how it implements live migration:

```
SOURCE NODE                              TARGET NODE
+----------------------------------+    +----------------------------------+
|  Dom0                            |    |  Dom0                            |
|  +----------------+    +------+  |    |  +----------------+    +------+  |
|  | Migration      |    | CBC  |  |    |  | Migration      |    | CBC  |  |
|  | Daemon         |<---| comp.|  |    |  | Daemon         |<---| comp.|  |
|  +-------^--------+    +------+  |    |  +-------^--------+    +------+  |
|          |  VM                   |    |          |                        |
|  +-------+--------+              |    |          |                        |
|  | Dirty  | Shadow|              |    |          |                        |
|  | Bitmap | Page  |              |    |          |                        |
|  |        | Table |              |    |          |                        |
|  +---VMM--+-------+              |    |  +--------VMM---+                 |
+----------------------------------+    +----------------------------------+
           Hardware ======= [NETWORK] ======= Hardware
```

**Components Explained:**

### Dom0 (Domain 0)

- In Xen, **Dom0** is the privileged management VM.
- It has full access to hardware and runs Xen management tools.
- It performs tasks like: create, terminate, and migrate VMs (called "DomU" â€” user domains).

### Migration Daemon

- Runs inside Dom0 on both source and target nodes.
- Responsible for orchestrating the migration process.
- Sends and receives VM state data.

### CBC (Characteristic Based Compression)

- A compression algorithm that **adaptively compresses memory pages** before transferring them.
- **Adaptive compression** means: it changes the compression algorithm based on the type of data being compressed (text vs. binary vs. zeros).
- Reduces the amount of data transmitted over the network.

### Shadow Page Tables + Dirty Bitmap

- The **VMM layer** maintains **shadow page tables** to track which memory pages the VM modifies during precopy rounds.
- Every time a page is written, a flag is set in a **dirty bitmap**.
- At the start of each precopy round:
    1. The dirty bitmap is sent to the migration daemon.
    2. The bitmap is cleared.
    3. Shadow page tables are **destroyed and re-created** for the next round.
- Pages flagged in the bitmap are **extracted, compressed via CBC**, and sent to the destination.
- At the destination, data is **decompressed** and placed in memory.

**Flow Summary:**

1. Source Dom0 initiates migration.
2. Shadow page tables track dirty pages.
3. Pages are extracted, CBC-compressed, sent over network.
4. Target Dom0 receives, decompresses, places in memory.
5. After iterations, VM is briefly paused, final state sent.
6. Target VM activates; source VM is destroyed.

---

## â“ Q&A BANK â€” Unit 1: VM Migration

---

**Q1. What is VM Migration and why is it fundamental to cloud computing?**

**A:** VM Migration is the process of moving a VM (including its memory, CPU state, disk, and network connections) from one physical host to another. It is fundamental to cloud computing because it enables:

- Zero-downtime maintenance of physical servers
- Dynamic load balancing across the data center
- Automatic scaling of resources
- Energy efficiency through server consolidation
- Business continuity in case of hardware failure Without migration, cloud providers could not guarantee SLAs or offer the elasticity that defines cloud services.

---

**Q2. What is the key difference between Non-Live and Live migration?**

**A:**

- **Non-Live:** VM is paused/suspended before migration. State is transferred. VM resumes at destination. Causes significant downtime. Simple to implement.
- **Live:** VM keeps running during migration. Memory is transferred iteratively while the VM executes. Only a very brief pause occurs at the end. Downtime is minimal (milliseconds). More complex.

---

**Q3. Explain all 6 stages of the Pre-Copy live migration technique.**

**A:**

1. **Pre-Migration (Stage 0):** VM is active on Host A; destination is pre-selected; resources are prepared.
2. **Reservation (Stage 1):** A container with required resources is initialized on Host B.
3. **Iterative Pre-Copy (Stage 2):** All memory pages copied first; then dirty pages are re-copied in subsequent rounds until the dirty set is small enough.
4. **Stop and Copy (Stage 3):** VM is paused; final dirty pages, CPU state, device state are transferred; ARP sent to redirect network traffic.
5. **Commitment (Stage 4):** VM state on Host A is released; Host A's resources are freed.
6. **Activation (Stage 5):** VM starts on Host B, connects to devices, resumes normal operation.

---

**Q4. What is a "dirty page" in the context of pre-copy migration?**

**A:** A dirty page is a memory page that has been **modified (written to) by the VM** after the last round of copying. Because it has changed since it was last transferred, it must be re-copied to the destination in the next round to ensure consistency. Dirty pages are the reason pre-copy must iterate â€” the VM keeps creating new dirty pages while old ones are being transferred.

---

**Q5. Why might pre-copy migration fail to converge?**

**A:** Pre-copy can fail to converge if the VM is **write-intensive** â€” meaning it dirtifies memory pages faster than the network can transfer them. In each round, more new dirty pages are created than are cleared by copying. This is called the "dirty page chasing" problem. In such cases, the hypervisor eventually forces a stop-and-copy, accepting a longer downtime as a trade-off.

---

**Q6. Compare Pre-Copy and Post-Copy migration techniques.**

**A:**

|Criteria|Pre-Copy|Post-Copy|
|---|---|---|
|**VM Location During Transfer**|Runs on source (Host A)|Moves to destination (Host B) immediately|
|**Memory Transfer**|Iterative, before final move|On-demand after VM moves|
|**Downtime**|Very short (only final stop-and-copy)|Essentially zero initial downtime|
|**Page Faults**|No page faults during migration|Frequent page faults as VM accesses unmigrated pages|
|**Risk of Source Failure**|Low (VM still on source until end)|High (if source fails, VM loses unmigrated pages)|
|**Best For**|Low-write-intensity workloads|Workloads where quick VM relocation is priority|

---

**Q7. What are the three techniques used in Post-Copy to handle page faults?**

**A:**

1. **Demand Paging:** Pages are fetched from the source only when the VM generates a page fault (accesses a missing page). Simple but slow.
2. **Active Push:** Source proactively sends pages to destination even without explicit requests. Pages are transferred only once â€” via active push or demand paging, never both.
3. **Memory Prepaging:** Uses prediction based on access locality to proactively send pages likely to be needed soon. Reduces the number of page faults.

---

**Q8. How does network continuity get maintained during VM migration?**

**A:** Each VM is assigned a **virtual IP address** and **virtual MAC address** that are separate from the physical host's addresses. When the VM migrates, it carries these virtual addresses with it. The VMM sends an **ARP announcement** to the network, updating the mapping of the VM's MAC address to its new physical location. External systems keep using the same virtual IP â€” they communicate seamlessly without knowing the VM changed locations.

---

**Q9. What is the role of Dom0 in Xen live migration?**

**A:** In Xen, Dom0 is the privileged management domain. During live migration, Dom0 on the **source node** runs the migration daemon that reads dirty page bitmaps, compresses memory using CBC, and sends data. Dom0 on the **target node** receives, decompresses, and places memory into the target VM's address space. Dom0 is essential because regular guest domains (DomU) do not have the hardware access privileges needed to perform migration.

---

**Q10. What is the Internet Suspend-Resume (ISR) technique and what property does it exploit?**

**A:** ISR is a memory migration technique that exploits **temporal locality** â€” the observation that when a VM resumes, its memory state is highly similar to when it was last suspended. The difference (delta) is only what the VM computed since last suspension. ISR uses a tree of small subfiles to represent VM memory and transfers only the changed files, significantly reducing the amount of data that must be copied. The downside is that downtime may still be high because the VM is suspended during the transfer.

---

**Q11. Why is VM migration considered resource-intensive?**

**A:** VM migration is resource-intensive because:

- It consumes **network bandwidth** to transfer memory pages.
- It uses **CPU cycles** for dirty page tracking, compression, and migration control logic.
- It uses **cache memory** for the shadow page tables and dirty bitmaps.
- It uses **memory capacity** at both source and destination simultaneously.
- It can cause **performance degradation** of running applications due to competition for these resources.

---

**Q12. What happens during Stage 3 (Stop and Copy) of pre-copy migration and why is it called "downtime"?**

**A:** In Stage 3, the VM is **suspended** (paused) on Host A. During this pause:

- The final set of dirty memory pages is transferred.
- CPU state (registers, program counter) is transferred.
- Device states (network, disk) are synchronized.
- An ARP broadcast redirects network traffic to Host B.

This is called "downtime" because the VM is completely unavailable to its users during this phase. The goal is to make this pause as short as possible â€” ideally imperceptible (milliseconds). The earlier iterative pre-copy rounds exist precisely to shrink the final dirty set so Stage 3 can complete quickly.

---

---

# UNIT 2 â€“ POPEK AND GOLDBERG VIRTUALIZATION PRINCIPLES

---

## 2.1 Background and Context

### The Virtualization Challenge

Before we can understand Popek and Goldberg, we need to understand **why virtualization is hard**.

Recall the layered structure:

```
[Guest Application]
       |
[Guest Operating System]
       |
[Hypervisor / VMM]   â† This is the virtualization layer
       |
[Physical Hardware]
```

The **Hypervisor (VMM)** must:

1. Give each VM the illusion it has its own dedicated hardware.
2. Protect VMs from interfering with each other.
3. Maintain control over physical resources.
4. Do all this efficiently â€” without making VMs slow.

### How Does the VMM Work?

The VMM uses two key techniques:

**Trap and Emulate:**

- When the Guest OS tries to execute an instruction that affects hardware (a "sensitive" instruction), the hardware **traps** (transfers control) to the VMM.
- The VMM **emulates** the effect of that instruction in software, maintaining the illusion for the VM.
- Then control returns to the Guest OS.

**Binary Translation:**

- When an instruction cannot be trapped (cannot be intercepted), the VMM **scans and rewrites** the guest's binary code before execution, replacing problematic instructions with safe alternatives.

### The Core Problem

For "Trap and Emulate" to work, **every instruction that could affect system state must generate a trap when executed in user mode** (so the VMM can intercept it). If even one such instruction runs silently without trapping, the VMM loses control â€” and virtualization breaks.

Gerald Popek and Robert Goldberg formalized this requirement in **1974** in their landmark paper "Formal Requirements for Virtualizable Third Generation Architectures."

---

## 2.2 Terminologies Defined by Popek and Goldberg

### Virtual Machine (VM)

> A complete compute environment with its own **isolated** processing capabilities, memory, and communication channels, as created by the VMM.

Key property: **"A virtual machine is an efficient, isolated duplicate of the physical machine."**

- **Efficient:** Programs run nearly as fast as on real hardware.
- **Isolated:** VMs cannot interfere with each other.
- **Duplicate:** The VM behaves like the real machine.

### Virtual Machine Monitor (VMM) / Hypervisor

> System software that creates and manages virtual machines.

A VMM must have three properties:

1. **Efficient:** All "safe" guest instructions run directly on hardware â€” no emulation overhead.
2. **Omnipotent:** Only the VMM can manipulate sensitive hardware state â€” the guest cannot secretly control resources.
3. **Undetectable:** A guest OS cannot detect that it is running on a VMM rather than real hardware.

### Third Generation Machines

Popek and Goldberg's analysis applies to **"third generation computers"**, which have:

- At least **two operating modes:** user mode and supervisor (kernel) mode.
- Some instructions are **restricted to supervisor mode** only (they trap if attempted in user mode).
- Memory addressing via a **relocation register**.
- Ability to perform table lookups (page tables).

This is exactly how modern x86, ARM, and most processors work.

---

## 2.3 Essential Characteristics of a VMM

Popek and Goldberg defined three essential properties a VMM must provide:

### Characteristic 1: Equivalence (Isolation)

> The VMM must provide an environment for programs that is essentially **identical** to the original machine.

In other words: A program running inside a VM should behave **exactly the same** as if it were running directly on the hardware.

- **Why important?** Applications must not need modification to run on a VM.
- **Protection perspective:** VMs must be isolated from each other â€” one VM's behavior must not affect another's.

### Characteristic 2: Resource Control

> The VMM must be in **total control** of all virtualized resources.

This means:

- Programs inside VMs must NOT be able to access resources **not explicitly allocated** to them.
- The VMM must be able to **reclaim** allocated resources at any time.
- No guest can hoard or steal resources.

### Characteristic 3: Efficiency

> The great majority of machine instructions must execute **without VMM intervention** (without trapping).

In other words: only a small subset of instructions (the sensitive ones) should go through the VMM. The rest should run at native hardware speed.

- **Why important?** If every instruction trapped to the VMM, VMs would be 100x slower than native.
- The efficiency requirement means: only when something sensitive is happening should the VMM step in.

---

## 2.4 Instruction Classification

This is one of the most important and examined parts of Popek-Goldberg theory. All instructions are classified into three types:

```
ALL INSTRUCTIONS
â”œâ”€â”€ PRIVILEGED INSTRUCTIONS
â”‚     â””â”€â”€ Cause a TRAP if processor is NOT in privileged (kernel) mode
â”‚
â”œâ”€â”€ SENSITIVE INSTRUCTIONS (subset of all instructions)
â”‚   â”œâ”€â”€ Behavior-Sensitive
â”‚   â”‚     â””â”€â”€ Result/behavior DEPENDS on processor mode or hardware config
â”‚   â”‚         (e.g., reading CPU privilege level register)
â”‚   â”‚         x86 examples: POP, PUSH, CALL, JMP, INT n, RET, LAR, LSL, VERR, VERW, MOV
â”‚   â”‚
â”‚   â””â”€â”€ Control-Sensitive
â”‚         â””â”€â”€ Attempt to CHANGE hardware configuration or resource allocation
â”‚             x86 examples: PUSHF, POPF, SGDT, SIDT, SLDT, SMSW
â”‚
â””â”€â”€ SAFE INSTRUCTIONS
      â””â”€â”€ NOT sensitive â€” can run directly without VMM intervention
```

### Privileged Instructions â€” Explained

- These instructions cause a **hardware trap** whenever they are executed outside of **Ring 0 (kernel/supervisor mode)**.
- Examples: instructions that change interrupt enable flags, load/store page table base registers, etc.
- **Trap and Emulate** relies on all sensitive instructions being privileged.

### Sensitive Instructions â€” Explained

**Behavior-Sensitive Instructions:**

- Their result or behavior **differs depending on the processor mode** (kernel vs. user mode).
- **Problem:** If a Guest OS runs these in user mode (as the VMM forces it to), it gets the wrong result.
- Example on x86: `PUSH %cs` â€” the %cs register contains bits indicating the current privilege level. A guest OS in user mode that pushes %cs sees "Ring 3", but it thinks it's in Ring 0. This reveals to the guest that it's running in a VM â€” violating the "Undetectable" property.

**Control-Sensitive Instructions:**

- Attempt to **change the system's resource configuration** â€” modify system registers, change page table pointers, etc.
- These directly affect how hardware resources are managed.
- Example on x86: `POPF` â€” modifies the eflags register, including the interrupt enable bit. In Ring 0 it works; in Ring 3, the CPU silently ignores the interrupt enable change â€” this is incorrect behavior for the guest.

### Safe Instructions

- All instructions that are **not sensitive** â€” they do not depend on privilege mode and do not change system configuration.
- These run directly on hardware at full native speed.
- Most arithmetic operations (ADD, SUB, MUL), data movement between registers, etc. are safe.

---

## 2.5 The Three Theorems of Popek and Goldberg

### Theorem 1 (Most Important)

> **"For any conventional third generation computer, a VMM may be constructed if the set of sensitive instructions for that computer is a subset of the set of privileged instructions."**

**What this means in plain English:**

- Every instruction that could break virtualization (sensitive instructions) must also be an instruction that traps when executed in user mode (privileged instruction).
- If this is true, then every time the guest OS tries to do something sensitive, the hardware automatically traps to the VMM â€” and the VMM can intercept and handle it correctly.
- Non-privileged (safe) instructions run directly on hardware at native speed.

**Venn Diagram:**

```
ALL INSTRUCTIONS
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                     â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚   â”‚   PRIVILEGED INSTRUCTIONS  â”‚    â”‚
â”‚   â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚    â”‚
â”‚   â”‚   â”‚   SENSITIVE      â”‚     â”‚    â”‚
â”‚   â”‚   â”‚   INSTRUCTIONS   â”‚     â”‚    â”‚
â”‚   â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚    â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚   SAFE (non-sensitive, non-priv.)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

For a VMM to exist: Sensitive âŠ† Privileged
```

If this condition is met: **Trap and Emulate works perfectly.**

### Theorem 2 (Recursive Virtualization)

> **"A conventional third generation computer is recursively virtualizable if it is: (a) virtualizable, and (b) a VMM without any timing dependencies can be constructed for it."**

**What this means:**

- **Recursive virtualization** = the ability to run a VM **inside a VM** (nested virtualization).
- This requires the system to be virtualizable AND the VMM itself to be able to run inside a VM.
- If architectures cannot be virtualized the classic way, **binary translation** is used â€” which replaces sensitive instructions that don't generate traps with equivalent safe instruction sequences.

**Binary Translation:**

- Instead of relying on traps, the VMM scans the guest's code before execution.
- It finds sensitive instructions that won't trap and replaces them with calls to VMM routines.
- Used by VMware to handle x86 before hardware virtualization support was added.

### Theorem 3 (Hybrid VMM)

> **"A hybrid virtual machine monitor may be constructed for any conventional third generation machine in which the set of user-sensitive instructions are a subset of the set of privileged instructions."**

**What this means:**

- Even if not all sensitive instructions are privileged, if the **user-mode subset** of sensitive instructions is privileged, a hybrid VMM can be built.
- A hybrid VMM uses a combination of trap-and-emulate (for instructions that trap) and binary translation (for those that don't).

---

## 2.6 Does x86 Support Popek-Goldberg Virtualization?

### x86 Privilege Ring Model

```
             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
             â”‚   Ring 3 (Least Privileged)â”‚  â† User Applications
             â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
             â”‚   â”‚     Ring 2       â”‚     â”‚  â† (Rarely used - device drivers)
             â”‚   â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚     â”‚
             â”‚   â”‚   â”‚  Ring 1  â”‚   â”‚     â”‚  â† (Rarely used)
             â”‚   â”‚   â”‚ â”Œâ”€â”€â”€â”€â”€â”€â” â”‚   â”‚     â”‚
             â”‚   â”‚   â”‚ |Ring 0| â”‚   â”‚     â”‚  â† OS Kernel (Most Privileged)
             â”‚   â”‚   â”‚ â””â”€â”€â”€â”€â”€â”€â”˜ â”‚   â”‚     â”‚
             â”‚   â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚     â”‚
             â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Normal Setup:**

- Kernel runs in **Ring 0**.
- Applications run in **Ring 3**.
- Rings 1 and 2 are rarely used (originally intended for device drivers).

**For Virtualization â€” The Problem:**

The VMM needs to run in Ring 0 (only way to control hardware). But then the Guest OS must be pushed to Ring 1 or Ring 3.

**Key Issue:** x86 originally had **17 sensitive instructions that were NOT privileged** â€” they didn't trap in user mode, meaning the VMM couldn't intercept them.

### Example 1: The PUSH Instruction

- `PUSH %cs` pushes the CS register (Code Segment) onto the stack.
- The CS register contains 2 bits that indicate the **current privilege level (CPL)**.
- A Guest OS running in Ring 3 could execute `PUSH %cs` and read those 2 bits.
- It would see CPL = 3 (not 0), revealing that it is NOT in kernel mode â€” breaking the "Undetectable" property.
- **For this to be virtualizable:** `PUSH %cs` should have caused a trap in Ring 3, allowing the VMM to push a fake CS value showing CPL = 0. But the original x86 did NOT trap on this.

### Example 2: PUSHF / POPF Instructions

- `PUSHF` and `POPF` read/write the **EFLAGS register** (which contains system flags including the interrupt enable flag).
- Bit 9 of EFLAGS: **Interrupt Flag (IF)** â€” controls whether hardware interrupts are processed.
- In Ring 0: `POPF` can set or clear bit 9 freely.
- In Ring 3: The CPU **silently ignores** attempts to change bit 9 via `POPF` â€” no trap, no error, just silently does nothing.
- **Problem:** The Guest OS tries to enable/disable interrupts using POPF, but the change is ignored. The Guest OS thinks it worked, but the hardware didn't actually change. The VMM also wasn't notified.
- **For this to be virtualizable:** `POPF` should trap in Ring 3, letting the VMM track interrupt enable/disable requests and manage them correctly.

### Summary: x86 and Popek-Goldberg

|Property|Original x86|
|---|---|
|Meets Theorem 1?|âŒ NO â€” 17 sensitive instructions are not privileged|
|Can virtualization still work?|âœ… YES â€” but requires workarounds|
|Workarounds used|Binary Translation (VMware), Paravirtualization (Xen), or Hardware Extensions|
|Hardware Fix|Intel VT-x / AMD-V added hardware virtualization support, making ALL sensitive instructions trap|

**The 17 non-PG-virtualizable x86 instructions:** `SGDT, SIDT, SLDT, SMSW, PUSHF, POPF, LAR, LSL, VERR, VERW, POP, PUSH, CALL, JMP, INT, RET, STR, MOV`

Most of these simply **reveal** that the kernel is running in user mode (PUSHF). Some **execute incorrectly** (POPF â€” silently ignoring interrupt flag changes).

---

## â“ Q&A BANK â€” Unit 2: Popek and Goldberg

---

**Q1. What problem were Popek and Goldberg trying to solve in their 1974 paper?**

**A:** They were formalizing the conditions under which a Virtual Machine Monitor (VMM) can be correctly and efficiently built for a given computer architecture. Not all architectures are equally amenable to virtualization. They wanted to define precisely when an architecture guarantees that trap-and-emulate virtualization will work correctly, without requiring binary translation or other workarounds.

---

**Q2. State Theorem 1 of Popek and Goldberg and explain its significance.**

**A:** Theorem 1: "For any conventional third generation computer, a VMM may be constructed if the set of sensitive instructions for that computer is a subset of the set of privileged instructions."

Significance: This theorem tells us exactly what a processor architecture must guarantee for standard trap-and-emulate virtualization to work. If every sensitive instruction is privileged (i.e., will trap when executed outside kernel mode), then the VMM will always be notified when the guest does something that needs intervention. Non-sensitive instructions run at native speed. This gives us both correctness (VMM catches everything important) and efficiency (most code runs without VMM overhead).

---

**Q3. Define and distinguish between Privileged, Behavior-Sensitive, Control-Sensitive, and Safe instructions.**

**A:**

- **Privileged:** Cause a hardware trap when executed outside supervisor mode. The VMM relies on these traps to intercept critical operations.
- **Behavior-Sensitive:** Their result or effect depends on the processor's current mode or configuration. If a guest OS executes these in user mode, it gets wrong results (e.g., reading a privilege level register returns the wrong level).
- **Control-Sensitive:** Attempt to change the system's resource configuration, such as modifying system registers or page table pointers.
- **Safe:** Not sensitive. They don't depend on mode and don't change system configuration. They run directly on hardware at full speed.

---

**Q4. What are the three essential characteristics of a VMM according to Popek and Goldberg?**

**A:**

1. **Equivalence:** A program inside a VM must behave identically to a program running directly on the hardware (isolation and protection).
2. **Resource Control:** The VMM must have total control over all resources. Guests cannot access resources not allocated to them, and the VMM can reclaim any resource.
3. **Efficiency:** The vast majority of instructions must execute without VMM intervention, at native hardware speed.

---

**Q5. Why does the x86 architecture fail to meet Theorem 1 of Popek-Goldberg?**

**A:** x86 has at least 17 sensitive instructions that are NOT privileged â€” they do not cause a hardware trap when executed in user mode (Ring 3). For example, `PUSHF` reveals the current privilege level without trapping, and `POPF` silently ignores attempts to change the interrupt flag in Ring 3. Since these sensitive instructions don't trap, the VMM cannot intercept them. This violates the requirement that sensitive âŠ† privileged, breaking the trap-and-emulate model.

---

**Q6. What is Binary Translation and when is it used?**

**A:** Binary Translation is a technique used when an architecture cannot support pure trap-and-emulate virtualization (i.e., some sensitive instructions don't generate traps). The VMM scans the guest OS's binary code before execution. When it finds a sensitive instruction that wouldn't trap, it replaces it with a sequence of instructions that calls VMM routines to handle the operation correctly. VMware used binary translation extensively for x86 virtualization before Intel VT-x and AMD-V were introduced.

---

**Q7. What is Recursive Virtualization (Theorem 2)?**

**A:** Recursive virtualization is the ability to run a virtual machine inside another virtual machine (nested VMs). Theorem 2 states that this is possible if the system is (a) virtualizable (meets Theorem 1) and (b) a VMM can be built for it without timing dependencies. Timing dependencies would mean the VMM relies on specific instruction execution times, which breaks when the VMM itself runs inside another VM.

---

**Q8. Explain the role of x86 privilege rings in the context of virtualization.**

**A:** x86 has 4 privilege rings (0â€“3). Ring 0 is most privileged (kernel/hypervisor), Ring 3 is least privileged (user applications). Only Ring 0 can execute privileged instructions. For virtualization, the VMM must run in Ring 0. The Guest OS is pushed to Ring 1 or Ring 3. The problem is that x86 page tables only distinguish "supervisor" (Rings 0â€“2) vs. "user" (Ring 3), meaning if the Guest OS is in Ring 1, it still has supervisor-level memory access â€” weak isolation. Additionally, sensitive instructions run in Ring 1/3 may not trap, breaking VMM control.

---

**Q9. What is a Hybrid VMM (Theorem 3)?**

**A:** A hybrid VMM is one that combines trap-and-emulate with binary translation. Theorem 3 says a hybrid VMM can be built if the user-mode subset of sensitive instructions is a subset of privileged instructions. For instructions that trap correctly, trap-and-emulate is used. For instructions that don't trap (some in kernel mode), binary translation is used. This hybrid approach allowed VMware to virtualize x86 before hardware support was available.

---

**Q10. Why must a VMM be "undetectable" and how does the PUSH instruction on x86 violate this?**

**A:** A VMM must be undetectable because if a guest OS knows it's running in a VM, it might behave differently â€” potentially bypassing security controls or exploiting the virtualization layer. The `PUSH %cs` instruction violates this on x86 by allowing a guest in Ring 3 to read the current privilege level from the CS register. Since the guest is running in Ring 3 (not Ring 0), it sees CPL=3 instead of the expected CPL=0, revealing that it's not actually in kernel mode â€” thereby detecting the VMM.

---

**Q11. What modern hardware extensions fixed x86's Popek-Goldberg compliance issues?**

**A:** Intel introduced **Intel VT-x (Virtualization Technology for IA-32/64)** and AMD introduced **AMD-V (AMD Virtualization)**. These extensions added a new VMX (Virtual Machine Extension) mode to x86 that: (1) provides a dedicated root mode for the hypervisor, (2) makes ALL sensitive instructions generate traps in guest mode, (3) adds VMCS (Virtual Machine Control Structure) for managing VM state efficiently, and (4) reduces the overhead of VM entry/exit transitions. With these extensions, x86 now effectively meets Popek-Goldberg requirements.

---

---

# UNIT 3 â€“ MEMORY AND I/O VIRTUALIZATION

---

## 3.1 Memory in an Un-virtualized System (Recap)

Before we virtualize memory, let's understand how it works normally.

### Virtual Address Space

Every process has its own **virtual address space** â€” an illusion of having a large, private memory area. In reality, the OS maps virtual addresses to physical RAM addresses using **page tables**.

```
Process A's Virtual Space        Process B's Virtual Space
[Page 0] [Page 1] [Page 2]       [Page 0] [Page 1] [Page 2]
    |          |        |              |         |         |
    v          v        v              v         v         v
Physical Memory (RAM): [Frame 3] [Frame 7] [Frame 1] [Frame 5] ...
```

### Page Tables

- A **page table** is a data structure maintained by the OS.
- Maps Virtual Page Numbers (VPN) â†’ Physical Frame Numbers (PFN).
- The CPU's **CR3 register** (Control Register 3) points to the base of the page table.

### Address Translation Process

```
Virtual Address (VA)
        |
        v
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”
    â”‚  TLB  â”‚ â† Translation Lookaside Buffer
    â””â”€â”€â”€â”€â”€â”€â”€â”˜
     Hit? â†’ Physical Address (fast!)
     Miss? â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  PTW          â”‚ â† Page Table Walker (hardware on x86/ARM)
    â”‚  (walks the   â”‚
    â”‚   page table) â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
            |
            v
     Physical Address (PA)
```

### TLB (Translation Lookaside Buffer)

- A **hardware cache** of recent VAâ†’PA translations.
- If the translation is in the TLB (TLB hit), address translation is almost instant.
- If not (TLB miss), the PTW walks the page table in memory â€” slower.

### Page Table Walker (PTW)

- **Software PTW** (e.g., SPARC): An OS handler that reads the page table. Slow.
- **Hardware PTW** (e.g., x86, ARM): Hardware generates memory accesses to walk the page table automatically on a TLB miss. Much faster.
- The walk uses the CR3 register to find the page table root, then traverses multi-level entries to find the physical frame.

---

## 3.2 Why Do We Need Memory Virtualization?

In a virtualized environment, multiple VMs share the same physical RAM. Without memory virtualization:

- Guest OSes would directly control physical memory â†’ they could interfere with each other.
- One VM could read or write another VM's memory.
- The hypervisor could not enforce isolation.

**Three Requirements:**

1. **VMs and Guest OSes must NOT have direct access to physical (host) memory.**
2. **Only the Hypervisor/VMM should manage host physical memory.**
3. **Guest OS must be made to believe it is accessing physical memory directly** â€” it must see a continuous "physical" address space even though it's not real.

**The Solution:** Introduce a **new layer of address translation**:

- Guest OS manages **Guest Virtual â†’ Guest Physical** translation.
- Hypervisor manages **Guest Physical â†’ System (Host) Physical** translation.

---

## 3.3 Two-Level Address Translation

```
gVA â”€â”€[Guest Page Table]â”€â”€â†’ gPA â”€â”€[Host Page Table / EPT]â”€â”€â†’ sPA
(Guest Virtual Address)   (Guest Physical Address)      (System Physical Address)
```

- **gVA (Guest Virtual Address):** The address a user application inside the VM uses. Managed by the Guest OS.
- **gPA (Guest Physical Address):** The "physical" address the Guest OS thinks it has. It's actually virtual â€” not real hardware memory.
- **sPA (System/Host Physical Address):** The actual physical RAM address on the real hardware.

Every memory access from a VM application must go through **two levels of translation** â€” first through the Guest's page table, then through the Hypervisor's page table.

This is the central challenge of memory virtualization: doing this efficiently.

---

## 3.4 Shadow Page Tables â€” Software Approach

### Core Idea

The hypervisor creates a **shadow page table (sPT)** for each guest process. The shadow page table **combines** the two levels of translation into one: it directly maps **gVA â†’ sPA** (skipping the intermediate gPA step for actual address lookups).

```
              gVA
               |
               |â”€â”€â”€â”€ Guest Page Table â”€â”€â”€â”€â†’ gPA (Guest Physical)
               |                               |
               |                               |
               |â”€â”€â”€â”€ Shadow Page Table â”€â”€â”€â”€â”€â”€â”€â”€â†’ sPA (System Physical)
                      (VMM maintains this)
```

The hardware PTW is pointed at the shadow page table (via CR3). So the hardware does ONE translation (gVA â†’ sPA) instead of two, at native speed.

### How Shadow Page Tables Work

1. The Guest OS sets up its own page table (gPT: gVA â†’ gPA).
2. The Hypervisor intercepts this and builds the shadow page table (sPT: gVA â†’ sPA) by combining the Guest's mapping with the Hypervisor's own mapping of gPA â†’ sPA.
3. The Hypervisor makes CR3 point to the shadow page table (not the guest's page table).
4. The hardware PTW walks the shadow page table on TLB misses â€” it directly resolves gVA to sPA.
5. The shadow page table is **invisible to the guest** â€” it doesn't know it exists.

### Keeping Shadow Page Tables Consistent

The challenge: whenever the Guest OS modifies its page table, the shadow page table must be updated too.

**Solution:** Mark Guest OS page tables as **read-only**.

- Any write to the guest page table generates a **page fault**.
- The page fault traps to the Hypervisor.
- The Hypervisor updates the shadow page table to reflect the change.
- Then the Hypervisor allows the guest's write to proceed.

### Virtual CR3 vs Real CR3

```
Virtual CR3 (guest thinks this is real CR3) â”€â”€â†’ Guest Page Tables
                                                      â†‘
Real CR3 (actual hardware register) â”€â”€â†’ Shadow Page Tables
```

The guest's CR3 is "virtual" â€” the hypervisor intercepts any update to CR3 and redirects the real hardware CR3 to point to the corresponding shadow page table.

### Challenges and Overheads of Shadow Page Tables

|Challenge|Impact|
|---|---|
|**Many Page Faults**|Applications that modify page tables frequently cause many faults, each requiring VMM intervention|
|**Context Switch Overhead**|Every time the Guest OS switches processes (context switch), CR3 changes, trapping to the VMM to switch shadow page tables|
|**TLB Flush on World-Switch**|Every time control switches between VM and VMM, the TLB must be flushed (since they use different address spaces) â€” performance hit|
|**Memory Overhead**|The VMM must maintain shadow page tables for EVERY process in EVERY VM â€” huge memory footprint|
|**Complexity**|Keeping shadow and guest page tables perfectly synchronized is complex and error-prone|

Shadow paging was the primary technique before hardware support was available. It works but has significant overhead.

---

## 3.5 Nested / Extended Page Tables â€” Hardware-Assisted Approach

### Core Idea

Instead of the software-maintained shadow page table, **tell the hardware about both levels of translation** and let the hardware page walker handle both in one pass.

Modern processors (Intel VT-x with EPT, AMD-V with Nested Page Tables) added hardware support for this:

```
gVA â”€â”€[gPT]â”€â”€â†’ gPA â”€â”€[nPT/EPT]â”€â”€â†’ sPA
```

The hardware PTW knows about both the Guest Page Table (gPT) and the Nested/Extended Page Table (nPT/EPT). It performs a **two-dimensional page walk** automatically.

### How Nested/Extended Page Tables Work

**Two sets of CR3-like registers:**

- **gCR3:** Points to the Guest Page Table (gPT). Guest OS controls this.
- **nCR3 (or EPTP):** Points to the Nested/Extended Page Table (nPT/EPT). Hypervisor controls this.

On a TLB miss, the hardware performs a **two-dimensional walk:**

1. Walks the gPT using gCR3 to translate gVA â†’ gPA.
2. But every physical address encountered during the gPT walk is itself a gPA â€” which must be translated via the nPT using nCR3 to get the actual memory address for reading the gPT entry.
3. This continues until the final gPA is found, then the nPT translates gPA â†’ sPA.

### Key Differences from Shadow Page Tables

|Aspect|Shadow Page Tables|Nested/Extended Page Tables|
|---|---|---|
|**Type**|Software-only technique|Hardware-assisted|
|**Who manages?**|Hypervisor creates and maintains|Guest OS manages gPT; Hypervisor manages nPT|
|**Page Fault on Guest PT Modify?**|Yes â€” every guest PT write traps to VMM|No â€” guest freely updates its own gPT|
|**CR3 Changes**|Every guest CR3 change traps to VMM|No traps needed|
|**TLB Flush**|Required on every world-switch|Not always needed|
|**Memory Footprint**|One shadow PT per guest process|One nPT per VM (not per process!)|
|**Performance**|Higher overhead|Better overall performance|

### EPT (Extended Page Table) â€” Intel's Implementation

```
Guest Linear Address
        |
        v (using CR3 / gCR3)
  Guest IA-32 Page Tables
        |
        v (outputs gPA)
  Extended Page Tables (EPT)  â† controlled by VMM via EPTP in VMCS
        |
        v (outputs sPA / Host Physical Address)
  Host Physical Memory
```

**Key EPT Properties:**

- EPT is a new page-table structure under VMM control.
- **EPT Base Pointer (EPTP):** A new field in the VMCS (Virtual Machine Control Structure) that points to the EPT. The VMCS is a data structure that stores all VM-specific state.
- EPT is activated when the VM (guest) is running, deactivated when the hypervisor takes over.
- Guest has full, unrestricted control over its own IA-32 page tables â€” no VMM intervention needed.
- **No VM exits** (no traps to VMM) for guest page faults, INVLPG (TLB invalidation), or CR3 changes.

**INVLPG handling:** When a guest removes a mapping and executes INVLPG to flush the TLB entry, the hypervisor intercepts this, removes the corresponding entry in the shadow page table (sPT), and executes its own INVLPG. With EPT, this is handled more automatically.

### EPT Performance

|Metric|Value|
|---|---|
|Typical EPT benefit vs. shadow paging|Up to 20% performance improvement|
|Outlier cases (e.g., fork-heavy workloads)|Over 40% improvement|
|Benefit grows with|Number of virtual CPUs|

**Secondary benefits of EPT:**

- No need for complex shadow page table virtualization algorithms.
- **Reduced memory footprint:** Shadow paging requires one shadow PT per guest user process; EPT requires only ONE EPT per entire VM.
- Simpler VMM code â†’ fewer bugs.

---

## 3.6 I/O Virtualization

### What is I/O Virtualization?

**I/O Virtualization (IOV)** uses software to abstract upper-layer protocols from physical I/O connections, managing the routing of I/O requests between virtual devices inside VMs and the shared physical hardware.

In simple terms: multiple VMs need to use the same physical NIC (network card) or disk. I/O virtualization makes each VM think it has its own dedicated device.

### Three Approaches to I/O Virtualization

```
I/O Virtualization Approaches
â”œâ”€â”€ 1. Full Device Emulation
â”‚       VMM software emulates a complete known device
â”‚
â”œâ”€â”€ 2. Para-Virtualization (Split Driver Model)
â”‚       Guest uses modified drivers that talk to backend in Dom0
â”‚
â””â”€â”€ 3. Direct I/O (Passthrough)
        VM gets direct hardware access (near-native performance)
```

---

### Approach 1: Full Device Emulation

**Concept:** The VMM software emulates a **well-known, real-world device** in software. The guest OS's standard device driver talks to this virtual device as if it were real hardware. The VMM software actually handles the I/O requests.

```
Guest OS
   |
Guest Device Driver (unmodified â€” thinks it talks to real hardware)
   |
Virtual Device (emulated in VMM software)
   |  â† VMM intercepts all I/O port access, DMA, interrupts
   |
I/O Stack (VMM)
   |
Real Physical Device Driver (VMM side)
   |
Physical Hardware
```

**What the VMM emulates:**

- Device enumeration (listing available devices)
- Device identification
- Interrupt handling
- DMA (Direct Memory Access) operations

**Advantages:**

- Guest OS needs NO modification â€” uses existing standard drivers.
- Can emulate devices that don't physically exist (e.g., a specific NIC model).

**Disadvantages:**

- Every I/O operation traps to the VMM â†’ HIGH OVERHEAD.
- Many layers to traverse â†’ lower performance.

---

### Approach 2: Para-Virtualization (Split Driver / Hosted Model)

**Concept:** Instead of emulating a device, install a **simplified virtual device** and split the driver into two parts:

- **Frontend driver:** Runs inside the Guest OS (DomU in Xen terminology). Receives I/O requests from guest applications and passes them to the backend.
- **Backend driver:** Runs in the privileged Domain 0 (Dom0 in Xen). Handles actual I/O on the real hardware.

They communicate via a **block of shared memory** â€” much faster than going through the VMM's emulation layer.

```
Guest OS (DomU)                    Host OS / Dom0
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Guest App       â”‚               â”‚  Backend Driver                 â”‚
â”‚  Guest Dev Driverâ”‚               â”‚  (manages real device)          â”‚
â”‚  Frontend Driver â”‚â†â”€shared memâ”€â”€â†’â”‚  I/O Stack                      â”‚
â”‚  Virtual Device  â”‚               â”‚  Physical Device Driver         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                            |
                                    Physical Hardware
```

**Advantages:**

- Better performance than full emulation (shared memory vs. trap-based).
- Reduced VMM intervention.

**Disadvantages:**

- Guest OS needs **modified drivers** (frontend drivers) â€” not standard out-of-the-box drivers.
- Higher CPU overhead than Direct I/O.
- Guest OS must be aware it's virtualized (modified drivers).

**Used by:** Xen hypervisor (paravirtualization is a core Xen concept).

---

### Approach 3: Direct I/O (Passthrough)

**Concept:** Bypass the VMM's I/O layer entirely. Assign a physical device **directly** to a specific VM. The VM's device driver talks to the hardware directly, with hardware-enforced memory protection.

```
Guest OS
   |
Guest Device Driver
   |
Device Manager (in VMM â€” configures assignment)
   |
Physical Hardware (directly accessed by VM)
```

**How It Works:**

- **Intel VT-x:** Allows a VM to have direct access to a physical address space.
- **Intel VT-d (Virtualization Technology for Directed I/O):** Adds an **IOMMU (I/O Memory Management Unit)** that restricts which physical memory ranges a device can access â€” preventing a rogue device in one VM from accessing another VM's memory.
- The VMM uses VT-x and VT-d to configure address translation for I/O operations.
- The device driver within the VM writes directly to device registers and configures DMA descriptors without VMM involvement.

**Advantages:**

- **Near-native performance** â€” almost as fast as running on bare metal.
- Very low CPU overhead.
- Bypasses VMM I/O emulation stack entirely.

**Disadvantages:**

- **Limited scalability:** A physical device can only be assigned to ONE VM at a time.
- Reduces flexibility â€” can't easily share a NIC between multiple VMs simultaneously.
- Some advanced features (like live migration) become more complex.

---

### Comparison of I/O Virtualization Approaches

|Criterion|Full Emulation|Para-Virtualization|Direct I/O|
|---|---|---|---|
|**Guest driver**|Standard (unmodified)|Modified (frontend)|Standard (unmodified)|
|**Performance**|Lowest|Medium|Near-native (highest)|
|**VMM involvement**|Every I/O op|Shared memory|Minimal (setup only)|
|**Scalability**|High (share device easily)|High|Low (1 VM per device)|
|**Guest OS aware of VM?**|No|Yes|No|
|**Implementation complexity**|Medium|High|Medium|
|**Examples**|QEMU emulation|Xen virtio, KVM virtio|Intel VT-d passthrough|

---

## 3.7 Xen I/O Virtualization (Specific Implementation)

**Xen's Approach:** Xen uses **Para-Virtualization** for I/O â€” it does NOT emulate hardware devices.

**Key Design Decisions in Xen I/O:**

1. **Device Abstraction:** Xen exposes simple, clean **device abstractions** (not emulations of real hardware) to guest VMs. This keeps the interface simple and efficient.
    
2. **Shared Memory Buffers:** I/O data is transferred to/from guest VMs via Xen using **shared memory buffers**. No data is copied through the hypervisor â€” it's just shared memory pointers, making it very fast.
    
3. **Virtualized Interrupts:** Xen provides a lightweight event delivery mechanism instead of hardware interrupts:
    
    - Events update a **bitmap in shared memory** â€” the guest checks this bitmap.
    - Guest OSes can register **optional callback handlers** that are called when events arrive.
    - This avoids the overhead of real hardware interrupt handling.

---

## 3.8 Advantages of I/O Virtualization (General)

|Advantage|Explanation|
|---|---|
|**Flexibility**|Hardware independence â€” abstract upper-layer protocols from physical connections; easier to provision than physical NICs/HBAs|
|**Cost Minimization**|Uses fewer cables, cards, and switch ports without sacrificing I/O performance|
|**Increased Density**|More virtual connections in the same physical space|
|**Cable Reduction**|Reduces the jungle of cables connecting servers to storage and network|

---

## â“ Q&A BANK â€” Unit 3: Memory and I/O Virtualization

---

**Q1. Why can't a Guest OS directly manage physical memory in a virtualized environment?**

**A:** If a Guest OS directly controlled physical memory, it would have unrestricted access to all of physical RAM â€” including memory belonging to other VMs and the hypervisor itself. This would violate isolation (one VM could read or corrupt another VM's data) and security. The hypervisor must be the sole manager of actual physical memory, with each Guest OS only seeing a "guest physical" address space that is actually managed and enforced by the hypervisor.

---

**Q2. Explain the three levels of addresses in memory virtualization.**

**A:**

- **gVA (Guest Virtual Address):** The virtual address a user-space application inside the VM uses. The Guest OS manages the translation gVAâ†’gPA via its own page tables (exactly like a normal OS would manage VAâ†’PA).
- **gPA (Guest Physical Address):** The "physical" address that the Guest OS believes it is working with. This is not real hardware memory â€” it's a virtualized physical space managed by the hypervisor.
- **sPA (System/Host Physical Address):** The actual physical RAM address on the host machine. The hypervisor manages gPAâ†’sPA translation.  
    Every memory access by a VM application requires two levels of translation: gVAâ†’gPA (by guest) and gPAâ†’sPA (by hypervisor).

---

**Q3. What is a Shadow Page Table and how does it work?**

**A:** A shadow page table (sPT) is maintained by the hypervisor for each guest process. It **combines both levels of address translation** (gVAâ†’gPA and gPAâ†’sPA) into a single page table that maps gVA directly to sPA. The hypervisor makes the hardware CR3 register point to the shadow page table (not the guest's page table). So the hardware PTW only does one walk (gVAâ†’sPA) at native speed. The guest's page table is marked read-only; any write to it generates a page fault that traps to the VMM, which then updates the shadow page table to match.

---

**Q4. What are the key performance problems with shadow page tables?**

**A:**

1. **Page fault overhead:** Every time the guest modifies its page table, a page fault traps to the VMM, adding latency.
2. **Context switch overhead:** Every process context switch in the guest changes CR3, requiring the VMM to switch which shadow page table CR3 points to â€” another trap.
3. **TLB flush on world-switch:** When control switches between VM and VMM, the entire TLB must be flushed (since they have different address spaces), degrading cache performance.
4. **Memory overhead:** The VMM must maintain a shadow page table for EVERY process in EVERY guest VM â€” this can use significant RAM.

---

**Q5. How do Nested/Extended Page Tables (EPT/NPT) solve the shadow page table problems?**

**A:** Nested/Extended Page Tables expose both levels of translation to the hardware PTW. The hardware can now walk both the guest page table and the nested page table in a single hardware operation (a "two-dimensional page walk"). This means:

- Guest OS can freely modify its own page tables without trapping to the VMM.
- No VM exits for guest page faults, CR3 changes, or INVLPG.
- The VMM maintains only ONE nPT/EPT per entire VM (not per process), drastically reducing memory overhead.
- Overall performance improves by up to 20â€“40%.

---

**Q6. What is the VMCS and how does it relate to EPT?**

**A:** The VMCS (Virtual Machine Control Structure) is a hardware data structure (introduced with Intel VT-x) that stores all the state needed to run a specific VM â€” guest CPU state, host CPU state, and VM execution controls. The EPTP (EPT Base Pointer) is a field in the VMCS that points to the root of the Extended Page Table for that VM. When the processor enters VM mode (VM entry), it reads the EPTP from the VMCS and uses it for all guest physical to host physical address translations.

---

**Q7. Compare full device emulation, para-virtualization, and direct I/O for I/O virtualization.**

**A:**

- **Full Device Emulation:** The VMM emulates a standard hardware device in software. The guest uses unmodified standard drivers. Simple for the guest but high overhead since every I/O operation traps to the VMM. Lowest performance.
- **Para-Virtualization (Split Driver):** Uses a frontend driver in the guest and a backend driver in Dom0/hypervisor communicating via shared memory. Requires modified guest drivers. Better performance than emulation, but guest is "aware" it's virtualized.
- **Direct I/O (Passthrough):** Physical device assigned directly to a VM. Guest driver accesses hardware directly with near-native performance. Requires IOMMU (e.g., Intel VT-d) for safety. Best performance but each device can only be assigned to ONE VM.

---

**Q8. What is the IOMMU and why is it essential for Direct I/O?**

**A:** An IOMMU (Input-Output Memory Management Unit) is a hardware unit that controls which physical memory addresses a device (or VM) can access via DMA (Direct Memory Access). In Direct I/O, a physical device is given to a VM. Without an IOMMU, that device's DMA operations could access any physical memory â€” including other VMs' memory and the hypervisor's memory. The IOMMU enforces DMA address translation and access controls, ensuring that the directly-assigned device can only access the memory allocated to its owning VM. Intel VT-d is Intel's IOMMU implementation.

---

**Q9. How does Xen implement I/O virtualization and why does it avoid hardware emulation?**

**A:** Xen uses para-virtualization for I/O (the split driver model). It avoids hardware emulation because emulation is slow â€” every I/O port access or DMA operation must be intercepted, decoded, and emulated in software, creating high overhead. Instead, Xen exposes simple, clean device abstractions via shared memory buffers. The frontend driver in the guest writes I/O requests to shared memory; the backend driver in Dom0 processes them directly. Event notification uses a lightweight bitmap in shared memory instead of hardware interrupts. This eliminates the emulation overhead while maintaining VM isolation.

---

**Q10. What is "two-dimensional page walking" in the context of nested page tables?**

**A:** Two-dimensional page walking is how the hardware PTW handles nested page tables. During a TLB miss, the PTW must translate gVA to sPA. It starts by walking the guest page table (gPT) to find gPA. But the gPT entries are stored at gPA addresses â€” which themselves need to be translated to sPA to actually read them from RAM. So for every level of the gPT walk, the PTW also walks the nested page table (nPT) to translate each encountered gPA to sPA. This creates a two-dimensional walk: the gPT walk in one dimension and the nPT walk in the other. For a 4-level page table, this can result in up to 24 memory accesses in the worst case, compared to 4 for a non-virtualized walk.

---

**Q11. What are the advantages of I/O virtualization from a data center perspective?**

**A:**

- **Flexibility:** Hardware independence allows faster server provisioning and easier repurposing.
- **Cost Reduction:** Fewer physical cables, NIC cards, and switch ports reduce hardware and management costs.
- **Increased Density:** More virtual I/O connections in the same physical space allows higher server utilization.
- **Cable Reduction:** Converged I/O (one physical cable carrying multiple virtual connections) dramatically simplifies data center wiring, reducing installation time and failure points.

---

**Q12. Why does shadow page table performance degrade more in a multi-vCPU environment compared to EPT?**

**A:** In a multi-vCPU environment, multiple virtual CPUs can be running guest processes simultaneously and modifying their page tables concurrently. Each modification requires a trap to the VMM, which must then synchronize and update the corresponding shadow page table. This creates lock contention and serialization â€” only one vCPU can update the shadow page tables at a time. Additionally, TLB flushes become more expensive as they must be coordinated across multiple vCPUs (TLB shootdowns). EPT eliminates most of these traps since each vCPU can update its own guest page tables freely, and the hardware handles the two-dimensional walk without VMM intervention. This is why EPT's benefit grows with the number of virtual CPUs.

---

---

# ğŸ—ºï¸ COMPREHENSIVE QUICK-REFERENCE SUMMARY

## VM Migration Quick Reference

|Type|VM State|Downtime|Complexity|
|---|---|---|---|
|Cold Migration|Powered off|Long|Low|
|Non-Live Migration|Paused|Medium-Long|Medium|
|Live (Pre-Copy)|Running|Very short (last stage)|High|
|Live (Post-Copy)|Runs on dest immediately|Near-zero initial|Very High|

## Popek-Goldberg Quick Reference

|Instruction Type|Definition|Must Trap?|
|---|---|---|
|Privileged|Traps if not in supervisor mode|Always (by hardware)|
|Behavior-Sensitive|Result depends on mode|Must trap for VMM|
|Control-Sensitive|Changes system configuration|Must trap for VMM|
|Safe|Not sensitive|No (runs directly)|

**Theorem 1:** VMM possible iff Sensitive âŠ† Privileged  
**Theorem 2:** Recursive VMM possible if virtualizable + no timing deps  
**Theorem 3:** Hybrid VMM possible if user-sensitive âŠ† privileged

## Memory Virtualization Quick Reference

|Technique|Type|Who Manages|VM Exits for PT Writes?|Memory Cost|
|---|---|---|---|---|
|Shadow Page Tables|Software|VMM|Yes (every write)|High (per-process)|
|Nested/Extended PT|Hardware|Guest + Hypervisor|No|Low (per-VM)|

## I/O Virtualization Quick Reference

|Approach|Driver|Performance|Scalability|Guest OS Modified?|
|---|---|---|---|---|
|Full Emulation|Standard|Low|High|No|
|Para-Virt.|Modified (frontend)|Medium|High|Yes|
|Direct I/O|Standard|Near-native|Low|No|

---

_End of Notes â€” PES University Cloud Computing, Unit 2 Virtualization_