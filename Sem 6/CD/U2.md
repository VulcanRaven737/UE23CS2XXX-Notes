# ðŸ“˜ Chapter 4 â€“ Syntax Analysis: Master Study Notebook

### Compiler Design (UE23CS341B) | Complete Deep-Dive Notes + Q&A

---

> **Philosophy of these notes:** Every concept is taught from scratch â€” why it exists, what problem it solves, how it works formally, worked examples, and a rich Q&A bank after each topic to cement your understanding. Nothing is skipped.

---

# TABLE OF CONTENTS

| #   | Topic                                           |
| --- | ----------------------------------------------- |
| 1   | What is Syntax Analysis?                        |
| 2   | Context-Free Grammars (CFG)                     |
| 3   | Ambiguity, Precedence & Associativity           |
| 4   | Eliminating Left Recursion                      |
| 5   | Left Factoring                                  |
| 6   | The Role of the Parser & Parser Types           |
| 7   | Syntax Error Handling                           |
| 8   | Error Recovery Strategies                       |
| 9   | Top-Down Parsing & Recursive Descent            |
| 10  | FIRST and FOLLOW Sets                           |
| 11  | LL(1) Grammars & Predictive Parsing             |
| 12  | Predictive Parsing Table Construction           |
| 13  | Non-Recursive Predictive (Table-Driven) Parsing |
| 14  | Error Recovery in Predictive Parsing            |
| 15  | Bottom-Up Parsing & Shift-Reduce                |
| 16  | LR(0) Items, Closure & GOTO                     |
| 17  | SLR Parsing                                     |
| 18  | LR Parsing Algorithm                            |
| 19  | LR(0) Parsing                                   |
| 20  | Canonical LR / CLR(1) Parsing                   |
| 21  | LALR Parsing                                    |
| 22  | Viable Prefixes & Valid Items                   |
| 23  | Comparison of All Parsers                       |
| 24  | Parser Generators â€“ YACC/Bison                  |

---

# TOPIC 1 â€” WHAT IS SYNTAX ANALYSIS?

## 1.1 Motivation

Every programming language has **precise structural rules**. Consider C:

- A **program** is made of **functions**
- A **function** contains **declarations** and **statements**
- A **statement** contains **expressions**
- An **expression** is composed of **terms**, **operators**, and **identifiers**

This hierarchical nesting is exactly what **syntax** describes. The job of the **Syntax Analyzer (Parser)** is to verify that the sequence of tokens from the lexer follows these structural rules and to build a **parse tree** representing the program's structure.

```
Source Code
     â†“
[Lexical Analyzer (Lexer)] â”€â”€â†’ Token Stream: id, +, id, *, id, $
     â†“
[Syntax Analyzer (Parser)] â”€â”€â†’ Parse Tree + Error Reports
     â†“
[Semantic Analyzer]
     â†“
...rest of compiler
```

> **Note:** _Syntax_ = rules about structure. _Semantics_ = rules about meaning. A syntactically correct program can still be semantically wrong (e.g., calling a function with wrong argument types).

## 1.2 Why Context-Free Grammars?

Syntax is specified using **Context-Free Grammars (CFGs)** written in **BNF/EBNF notation**. CFGs are ideal because:

- Programming language constructs are **hierarchical** and **recursive** (e.g., expressions nested inside expressions)
- CFGs capture exactly this kind of nested structure
- CFGs are powerful enough to describe most syntactic constructs yet tractable for efficient parsing

## 1.3 Benefits of Grammar-Based Syntax Specification

|Benefit|Description|
|---|---|
|**1. Precise Specification**|Gives an unambiguous, human-readable definition of the language's syntax|
|**2. Automatic Parser Construction**|Tools (YACC, Bison) generate parsers automatically from grammars|
|**3. Reveals Ambiguities**|The process of building a parser exposes ambiguities missed in informal specs|
|**4. Structured Translation**|Grammar structure guides how to generate correct object code|
|**5. Error Detection**|Any deviation from the grammar is detected as a syntax error|
|**6. Language Evolution**|New constructs added as new grammar rules â€” cleanly integrated|

---

## âœ… Q&A â€” TOPIC 1

**Q1. What is the primary job of the parser (syntax analyzer)?**

> **A:** The parser takes the token stream from the lexical analyzer and verifies that the stream can be generated by the grammar of the source language. For valid programs, it builds a parse tree; for invalid programs, it reports syntax errors and attempts recovery.

**Q2. Why can't a lexer alone detect all syntax errors?**

> **A:** The lexer only checks individual tokens (like checking that `123abc` is not a valid identifier). It has no memory of context. A parser sees the _sequence_ of tokens and can detect structural errors like a missing semicolon, mismatched braces, or `if` without a condition â€” all structurally illegal even though individual tokens may be valid.

**Q3. What's the difference between syntax and semantics? Give an example.**

> **A:** **Syntax** is about structure; **semantics** is about meaning. For example, `int x = "hello";` may be syntactically correct (it has the right structure for a declaration) but is semantically wrong (type mismatch). Conversely, `int = 5 x;` is syntactically wrong (wrong order) regardless of types.

**Q4. Why are Context-Free Grammars used instead of Regular Expressions for syntax?**

> **A:** Regular expressions (and finite automata) cannot handle nested/recursive structures â€” e.g., matching parentheses `(((...)))` arbitrarily deep requires counting, which DFAs can't do. CFGs can easily express `E â†’ (E)` allowing arbitrary nesting. Syntax inherently requires this recursive power.

**Q5. Name two real-world parsers and their types.**

> **A:** 1) `javac` (Java compiler) uses a hand-written recursive-descent (LL-style) parser. 2) GCC and Clang (for C/C++) use LR-variant parsers (LALR/GLR).

**Q6. What is BNF notation?**

> **A:** BNF (Backus-Naur Form) is the standard notation for writing CFG rules. Example: `<expr> ::= <expr> + <term> | <term>`. EBNF extends BNF with `*`, `+`, `?` for repetition/optionality.

---

# TOPIC 2 â€” CONTEXT-FREE GRAMMARS (CFG)

## 2.1 What is a CFG?

A **Context-Free Grammar** is called "context-free" because its production rules apply **regardless of the surrounding context** â€” a nonterminal can always be replaced by its production body, no matter what symbols appear around it.

## 2.2 Formal Definition

A CFG is a 4-tuple **G = (V, T, P, S)** where:

|Component|Symbol|Meaning|Example|
|---|---|---|---|
|Nonterminals|V|Variables representing syntactic categories|{E, T, F}|
|Terminals|T|Actual tokens from the lexer|{id, +, *, (, )}|
|Productions|P|Rules of the form A â†’ Î±|E â†’ E + T|
|Start Symbol|S|The top-level nonterminal (S âˆˆ V)|E|

**Key constraint:** Every production head is a **single nonterminal** (this is what "context-free" means â€” no context on either side of the head symbol).

## 2.3 Productions (Rules)

A production has the form: `Head â†’ Body`

- **Head**: Single nonterminal `A`
- **Body**: String of terminals and/or nonterminals (Î± âˆˆ (V âˆª T)*)

**Example grammar for arithmetic expressions:**

```
E â†’ E + T | T
T â†’ T * F | F
F â†’ (E) | id
```

This says:

- An **Expression** is an expression plus a term, or just a term
- A **Term** is a term times a factor, or just a factor
- A **Factor** is a parenthesized expression or an identifier

## 2.4 Derivations â€” The Core Concept

A **derivation** is the process of producing a terminal string from the start symbol by repeatedly replacing nonterminals with their production bodies.

The notation `A â‡’ Î±` means "A **directly derives** Î±" (one step replacement). The notation `A â‡’* Î±` means "A **derives** Î± in zero or more steps".

**Example â€” Derive `id * id + id`:**

```
E
â‡’ E + T              (using E â†’ E + T)
â‡’ T + T              (using E â†’ T)
â‡’ T * F + T          (using T â†’ T * F)
â‡’ F * F + T          (using T â†’ F)
â‡’ id * F + T         (using F â†’ id)
â‡’ id * id + T        (using F â†’ id)
â‡’ id * id + F        (using T â†’ F)
â‡’ id * id + id       (using F â†’ id)
```

## 2.5 Leftmost vs Rightmost Derivations

|Type|Rule|Symbol|Used By|
|---|---|---|---|
|**Leftmost**|Replace **leftmost** nonterminal each step|â‡’lm|Top-down parsers|
|**Rightmost**|Replace **rightmost** nonterminal each step|â‡’rm|Bottom-up parsers|

**Same string, different derivation styles:**

```
Leftmost derivation of id + id * id:
E â‡’lm E+T â‡’lm T+T â‡’lm F+T â‡’lm id+T â‡’lm id+T*F â‡’lm id+F*F â‡’lm id+id*F â‡’lm id+id*id

Rightmost derivation of id + id * id:
E â‡’rm E+T â‡’rm E+T*F â‡’rm E+T*id â‡’rm E+F*id â‡’rm E+id*id â‡’rm T+id*id â‡’rm F+id*id â‡’rm id+id*id
```

> **Key connection:** Bottom-up (LR) parsers construct the **rightmost derivation in reverse**. When they "reduce", they are un-doing a rightmost derivation step.

## 2.6 Parse Trees

A **parse tree** is a visual depiction of a derivation. It shows the hierarchical structure.

**Properties:**

1. Root = start symbol
2. Leaves = terminals (or Îµ)
3. Interior nodes = nonterminals
4. Children of node `A` (left to right) spell out the body of some production `A â†’ â€¦`
5. Reading leaves left-to-right gives the original string (**yield** of the tree)

**Example â€” Parse tree for `9 - 5 + 2`** with grammar `list â†’ list + digit | list - digit | digit`, `digit â†’ 0|1|...|9`:

```
          list
         /  |  \
       list  -  digit
      / |  \       \
   list  +  digit   5
     |          \
   digit          2
     |
     9
```

Leaves read left-to-right: `9 - 5 + 2` âœ“

## 2.7 C Language Grammar (Brief Illustration)

The full grammar of C can be written as a CFG. Here is a simplified version showing the concept:

```
P     â†’ S
S     â†’ Declr; S | Assign; S | if (Cond) {S} S | while (Cond) {S} S
      | if (Cond) {S} else {S} S | for (Assign; Cond; UnaryExpr) {S} S
      | return E; S | Î»
Declr â†’ Type ListVar
Type  â†’ int | float | char | double
Assign â†’ id = E
Cond  â†’ E RelOp E
RelOp â†’ < | > | <= | >= | == | !=
```

---

## âœ… Q&A â€” TOPIC 2

**Q1. What makes a grammar "context-free"?**

> **A:** Every production rule has a single nonterminal on the left-hand side (head). The rule can be applied regardless of what symbols surround that nonterminal â€” hence "context-free." Compare with context-sensitive grammars where the rule application depends on neighboring symbols.

**Q2. What is the difference between a terminal and a nonterminal?**

> **A:** **Terminals** are actual tokens that appear in the input (e.g., `id`, `+`, `;`). They cannot be expanded further. **Nonterminals** are syntactic variables representing categories of strings (e.g., `E`, `T`, `stmt`). They get replaced by production bodies during derivation. The final string consists entirely of terminals.

**Q3. Why must every CFG have a start symbol?**

> **A:** The start symbol defines "what we're trying to derive." A parse begins with the start symbol and ends when we have a string of only terminals. Without a designated start symbol, we wouldn't know where derivation begins.

**Q4. What is a sentential form?**

> **A:** A **sentential form** is any string of terminals and nonterminals that can be derived from the start symbol (including the start symbol itself). Example: in `E â‡’ E+T â‡’ T+T`, the strings `E`, `E+T`, and `T+T` are all sentential forms. A sentential form consisting only of terminals is called a **sentence** (or string in the language).

**Q5. What is the language L(G) generated by grammar G?**

> **A:** `L(G)` is the set of all strings of terminals that can be derived from the start symbol: `L(G) = {w âˆˆ T* | S â‡’* w}`. Only terminal strings are considered â€” intermediate sentential forms with nonterminals are not in L(G).

**Q6. Is the derivation order (leftmost vs rightmost) significant for the final string?**

> **A:** No â€” for an unambiguous grammar, both leftmost and rightmost derivations of the same string produce the same **parse tree**. The derivation order only affects which nonterminal gets replaced at each step, not the final structure.

**Q7. What is the relationship between a parse tree and a derivation?**

> **A:** Every derivation corresponds to a parse tree and vice versa. The parse tree hides the sequential order of derivation steps and shows only the hierarchical structure. For an unambiguous grammar, a string has exactly one parse tree regardless of whether we use leftmost or rightmost derivation.

**Q8. Can the body of a production be empty?**

> **A:** Yes. A production `A â†’ Îµ` (epsilon/lambda) means A can derive the empty string. This is called a **null production** or **epsilon production**. It allows optional constructs in the language.

---

# TOPIC 3 â€” AMBIGUITY, PRECEDENCE & ASSOCIATIVITY

## 3.1 What is Ambiguity?

A grammar is **ambiguous** if some string in its language has **more than one parse tree** (equivalently: more than one leftmost derivation, or more than one rightmost derivation).

**Example â€” Ambiguous arithmetic grammar:**

```
E â†’ E + E | E - E | E * E | E / E | E ^ E | (E) | num | id
```

The string `id + id * id` has **two parse trees**:

```
Parse Tree 1: ((id + id) * id)       Parse Tree 2: (id + (id * id))
       E                                      E
     / | \                                  / | \
    E  *  E                                E  +  E
   /|\    |                                |    /|\
  E + E  id                               id  E  *  E
  |   |                                       |     |
  id  id                                      id    id
```

These represent **different computations** â€” Tree 1 computes addition first, Tree 2 multiplies first. Ambiguity is fatal for compilers because the meaning is undefined!

## 3.2 Precedence

**Precedence** determines which operator binds tighter when different operators appear without parentheses.

In math: `a + b * c` means `a + (b * c)` because `*` has higher precedence than `+`.

**Encoding precedence in a grammar:**

- **Lower in the grammar hierarchy = higher precedence**
- Each precedence level gets its own nonterminal
- Higher precedence operators appear closer to the leaves (deeper in the tree)

**Unambiguous grammar with precedence:**

```
E â†’ E + T | E - T | T      â† + and - (lowest precedence)
T â†’ T * F | T / F | F      â† * and / (medium precedence)
F â†’ D ^ F | D              â† ^ (higher precedence)
D â†’ (E) | num | id         â† atoms (highest â€” deepest level)
```

Now `id + id * id` can only parse as `id + (id * id)` â€” the `*` is forced deeper.

## 3.3 Associativity

**Associativity** determines grouping when the **same operator** appears multiple times:

- **Left-associative:** `a - b - c = (a - b) - c`
- **Right-associative:** `a ^ b ^ c = a ^ (b ^ c)`

**Encoding in grammar:**

- **Left recursion** â†’ **Left associativity** (production grows to the left)
- **Right recursion** â†’ **Right associativity** (production grows to the right)

```
E â†’ E + T     â† LEFT recursive â†’ + is LEFT-associative
F â†’ D ^ F     â† RIGHT recursive â†’ ^ is RIGHT-associative
```

**Why does this work?**

- `E â†’ E + T` forces `a + b + c` to parse as `(a + b) + c` because:
    - `E â‡’ E + T â‡’ (E + T) + T â‡’ (T + T) + T = (a + b) + c`
- `F â†’ D ^ F` forces `a ^ b ^ c` to parse as `a ^ (b ^ c)` because:
    - `F â‡’ D ^ F â‡’ a ^ (D ^ F) â‡’ a ^ (b ^ c)`

## 3.4 The Standard Unambiguous Expression Grammar

This grammar (Grammar 4.1) is used throughout the chapter for LR parsing:

```
E â†’ E + T | T       â† + is left-associative; lower precedence
T â†’ T * F | F       â† * is left-associative; higher precedence
F â†’ (E) | id        â† atoms
```

For top-down parsing (LL), the left-recursive version is converted to Grammar 4.2:

```
E  â†’ T E'
E' â†’ + T E' | Îµ
T  â†’ F T'
T' â†’ * F T' | Îµ
F  â†’ (E) | id
```

---

## âœ… Q&A â€” TOPIC 3

**Q1. Why is ambiguity a problem for compilers?**

> **A:** An ambiguous grammar gives multiple parse trees for one string, meaning the program has multiple possible meanings. The compiler can't know which computation to perform. For example, `a + b * c` under an ambiguous grammar could evaluate as `(a+b)*c` or `a+(b*c)` â€” producing different results. Compilers need exactly one interpretation.

**Q2. How does left recursion encode left associativity?**

> **A:** With `E â†’ E + T`, parsing `a + b + c` produces: `E â‡’ E + T â‡’ (E + T) + T`. The leftmost E keeps expanding leftward, creating a left-leaning tree. Reading this left-to-right means additions associate to the left: `(a + b) + c`.

**Q3. How does the grammar `F â†’ D ^ F` make `^` right-associative?**

> **A:** With right recursion, `a ^ b ^ c` parses as: `F â‡’ D ^ F â‡’ a ^ (D ^ F) â‡’ a ^ (b ^ F) â‡’ a ^ (b ^ c)`. The rightmost F keeps expanding rightward, creating a right-leaning tree: `a ^ (b ^ c)`.

**Q4. In the grammar `E â†’ E+T | T`, `T â†’ T*F | F`, `F â†’ (E) | id` â€” why does `*` have higher precedence than `+`?**

> **A:** Because `*` is handled deeper in the hierarchy (at the `T` level) compared to `+` (at the `E` level). Nonterminals deeper in the grammar are reduced first during parsing, making their operators bind tighter. So `id + id * id` parses as `id + (id * id)` â€” `*` binds its operands before `+` gets to them.

**Q5. Can an ambiguous grammar ever be used for parsing?**

> **A:** Technically yes, but only by adding explicit disambiguation rules outside the grammar (e.g., declaring precedence and associativity in YACC/Bison). The grammar remains formally ambiguous but the parser uses extra rules to choose which parse tree to build. The better solution is to rewrite the grammar to be unambiguous.

**Q6. What is the difference between ambiguity and nondeterminism in parsing?**

> **A:** **Ambiguity** is a property of the grammar â€” it can produce multiple parse trees for one string. **Nondeterminism** in parsing means the parser can't decide which action to take with the given lookahead. An ambiguous grammar always leads to a nondeterministic parser, but a non-ambiguous grammar can also be nondeterministic for certain parser types (e.g., not LL(1) even though unambiguous).

**Q7. Can a left-recursive grammar be ambiguous?**

> **A:** Yes. Ambiguity is about multiple parse trees, not about recursion direction. However, a grammar that is both left-recursive AND right-recursive for the same operator (like `E â†’ E+E`) is typically ambiguous.

---

# TOPIC 4 â€” ELIMINATING LEFT RECURSION

## 4.1 What is Left Recursion?

A grammar is **left-recursive** if a nonterminal `A` can derive a string beginning with `A` itself:

**Direct left recursion:** `A â†’ AÎ±` (A appears directly as leftmost symbol) **Indirect left recursion:** `A â†’ BÎ³` and `B â†’ AÎ´` (A appears after one or more steps)

**Example of direct left recursion:**

```
E â†’ E + T | T
```

Here `E â†’ E + T` is directly left-recursive because E is the first symbol in the body.

## 4.2 Why is Left Recursion Deadly for Top-Down Parsers?

When a recursive descent parser tries to expand `E`:

1. It calls procedure `parse_E()`
2. Inside `parse_E()`, it immediately calls `parse_E()` again (because `E â†’ E + T`)
3. Which calls `parse_E()` againâ€¦
4. **â†’ Infinite recursion, stack overflow!**

The parser never reads any input before getting stuck.

## 4.3 Algorithm for Eliminating Direct Left Recursion

**Given pattern:**

```
A â†’ AÎ±â‚ | AÎ±â‚‚ | ... | AÎ±â‚™ | Î²â‚ | Î²â‚‚ | ... | Î²â‚˜
```

Where Î±'s are the "left-recursive tails" and Î²'s are the "non-left-recursive alternatives."

**Transformation:**

```
A  â†’ Î²â‚ A' | Î²â‚‚ A' | ... | Î²â‚˜ A'
A' â†’ Î±â‚ A' | Î±â‚‚ A' | ... | Î±â‚™ A' | Îµ
```

**Intuition:** Instead of `A` growing LEFT by prepending something, we let `A` start with a `Î²` (a non-recursive base) and then grow RIGHT by repeatedly appending via `A'`. The `A' â†’ Îµ` allows the recursion to terminate.

**Step-by-step example â€” Eliminate left recursion from `E â†’ E + T | T`:**

```
Pattern match:
  A  = E
  Î±â‚ = + T    (the tail of the left-recursive production)
  Î²â‚ = T      (the non-left-recursive alternative)

Result:
  E  â†’ T E'
  E' â†’ + T E' | Îµ
```

**Full transformation of Grammar 4.1 â†’ Grammar 4.2:**

```
Original (Grammar 4.1):          Transformed (Grammar 4.2):
E â†’ E + T | T              â†’     E  â†’ T E'
T â†’ T * F | F              â†’     E' â†’ + T E' | Îµ
F â†’ (E) | id                     T  â†’ F T'
                                  T' â†’ * F T' | Îµ
                                  F  â†’ (E) | id
```

## 4.4 Algorithm for Eliminating Indirect Left Recursion

Indirect left recursion is trickier. Example:

```
A â†’ Bxy | x
B â†’ CD
C â†’ A | c
D â†’ d
```

Here `C â†’ A â†’ Bxy â†’ CDxy â†’ ...` eventually starts with C again (indirect).

**Algorithm (order nonterminals Aâ‚, Aâ‚‚, ..., Aâ‚™):**

```
for i = 1 to n:
    for j = 1 to i-1:
        Replace each production Aáµ¢ â†’ Aâ±¼ Î³
        with Aáµ¢ â†’ Î´â‚Î³ | Î´â‚‚Î³ | ... (where Aâ±¼ â†’ Î´â‚|Î´â‚‚|... are Aâ±¼'s current productions)
    Eliminate any direct left recursion in Aáµ¢
```

**Worked example step-by-step:**

_Initial grammar:_

```
A â†’ Bxy | x       (A is Aâ‚)
B â†’ CD            (B is Aâ‚‚)
C â†’ A | c         (C is Aâ‚ƒ)
D â†’ d             (D is Aâ‚„)
```

_Process A (no left recursion, skip)_

_Process B (substitute A's productions):_ â€” B doesn't start with A or B, skip

_Process C (substitute B's productions for the B in C â†’ Bxy is not present):_

- C â†’ A | c â†’ substitute A: C â†’ Bxy | x | c
- C starts with B now; substitute B: C â†’ CDxy | x | c â†’ **direct left recursion found!**
- Eliminate it: C' = C, Î± = Dxy, Î²â‚ = x, Î²â‚‚ = c
    - `C â†’ xC' | cC'`
    - `C' â†’ DxyC' | Îµ`

_Final grammar:_

```
A  â†’ Bxy | x
B  â†’ CD
C  â†’ xC' | cC'
C' â†’ DxyC' | Îµ
D  â†’ d
```

## 4.5 Key Exercises (from syllabus)

**Exercise 1: Eliminate left recursion from `S â†’ Sa | Sb | c`**

```
A = S, Î±â‚ = a, Î±â‚‚ = b, Î² = c

S  â†’ c S'
S' â†’ a S' | b S' | Îµ
```

**Exercise 2: Eliminate left recursion from `A â†’ Ba | d`, `B â†’ Ab | c`**

```
Step 1 â€” substitute A in B:
B â†’ Ab | c â†’ (Ba | d)b | c â†’ Bab | db | c  (direct left recursion in B!)

Step 2 â€” eliminate direct left recursion in B:
B  â†’ db B' | c B'
B' â†’ ab B' | Îµ

Final:
A  â†’ Ba | d
B  â†’ dbB' | cB'
B' â†’ abB' | Îµ
```

---

## âœ… Q&A â€” TOPIC 4

**Q1. Why does left recursion cause top-down parsers to loop infinitely?**

> **A:** A top-down parser implements each nonterminal as a function. When it processes `A`, it looks at the production `A â†’ AÎ±` and immediately calls the function for `A` again â€” without consuming any input first. This recursive call chain never terminates because the input pointer never advances.

**Q2. Does eliminating left recursion change the language generated?**

> **A:** No. The original and transformed grammars generate exactly the same language. The transformation only changes the **structure** of derivations, not the set of strings produced. For example, `E â†’ E+T | T` and `E â†’ TE', E' â†’ +TE' | Îµ` both generate the same set of arithmetic expressions.

**Q3. What is the key structural difference after eliminating left recursion?**

> **A:** Left-recursive grammars grow parse trees to the LEFT (left-leaning). After elimination, the new grammar `A â†’ Î²A', A' â†’ Î±A' | Îµ` grows to the RIGHT (right-leaning through `A'`). The associativity is preserved in the semantics even though the tree shape changes.

**Q4. After eliminating left recursion from `E â†’ E+T | T`, do we get `E â†’ TE', E' â†’ +TE' | Îµ`. What does `E'` represent intuitively?**

> **A:** `E'` represents "zero or more additional `+T` suffixes." It captures the tail of an expression. `E` starts with a term `T`, then `E'` optionally appends `+T` as many times as needed. This is like a loop: `E' â†’ +TE'` means "add one more term, then continue." `E' â†’ Îµ` means "stop."

**Q5. Can ALL left-recursive grammars be transformed into non-left-recursive ones?**

> **A:** Yes, for grammars without cycles (A â‡’+ A) and Îµ-productions in specific positions. The algorithm shown works for any context-free grammar without such special cases. In practice, almost all programming language grammars can be handled.

**Q6. Is the following grammar left-recursive? `A â†’ bC`, `B â†’ Ad`, `C â†’ Ba`**

> **A:** Yes â€” indirect left recursion. Trace: `A â†’ bC â†’ bBa â†’ bAda â†’ ...` â€” A eventually starts with A. You'd need the indirect elimination algorithm to fix this.

**Q7. After eliminating left recursion, how does the parse tree change?**

> **A:** The shape changes but the language is the same. For `a + b + c`:
> 
> - Original left-recursive tree: `((a + b) + c)` â€” left-leaning
> - After elimination: the tree through `E'` chain is right-leaning in shape, but **the semantic actions** still compute `(a + b) + c` correctly because the `+` signs appear in the right order.

---

# TOPIC 5 â€” LEFT FACTORING

## 5.1 The Problem: Common Prefixes

When two or more productions for the same nonterminal share a **common prefix**, a top-down parser with only 1-symbol lookahead can't decide which one to use.

**Example:**

```
stmt â†’ if (expr) stmt else stmt
      | if (expr) stmt
```

After seeing `if`, `(`, `expr`, `)`, `stmt` â€” the parser sees the next token. If it's `else`, use the first rule. If not, use the second. But by then, the parser has already matched `if (expr) stmt` and needs to decide now! With only 1-symbol lookahead at the start, it can't know if an `else` will come.

**More extreme example:** `S â†’ a | ab | abc | abcd` After seeing `a`, the parser can't decide if the string is just `a`, or starts `ab`, or `abc`, or `abcd`.

**Formal pattern:**

```
A â†’ Î±Î²â‚ | Î±Î²â‚‚
```

When the parser sees a token from FIRST(Î±), it can't choose between `Î±Î²â‚` and `Î±Î²â‚‚` â€” both start with Î±.

## 5.2 Why is This a Problem?

**For LL parsing:** If FIRST(Î±Î²â‚) and FIRST(Î±Î²â‚‚) overlap, the parsing table would have two entries in the same cell â†’ conflict â†’ not LL(1).

**For recursive descent:** The parser may try one rule, fail (consume some input), then need to backtrack â€” which is **exponentially slow** in the worst case (e.g., `S â†’ a | ab | abc | abcd` on input `abcd` â€” tries `a`, fails, tries `ab`, fails, tries `abc`, fails, tries `abcd`, succeeds).

## 5.3 The Solution: Left Factoring

**Left factoring** pulls out the shared prefix and defers the decision to a new nonterminal.

**Transformation:**

```
A â†’ Î±Î²â‚ | Î±Î²â‚‚   becomes:   A â†’ Î± A'
                              A' â†’ Î²â‚ | Î²â‚‚
```

**Effect:** After parsing `Î±`, the parser looks at 1 symbol to choose between `Î²â‚` and `Î²â‚‚`. The decision is **deferred** until after the common prefix `Î±` is consumed.

> **Key insight:** Left factoring converts LL(k) (needing k symbols) to LL(1) (needing only 1 symbol).

## 5.4 Algorithm (Rule of Thumb)

For each nonterminal:

1. List all its productions
2. Find the **longest common prefix** shared by the **maximum** number of alternatives
3. Factor that prefix out â†’ create a new nonterminal for the suffixes
4. Repeat recursively on the new grammar until no more common prefixes exist

## 5.5 Worked Examples

**Example 1: `A â†’ aAB | aBc | aAc`** (all share `a`)

```
Step 1 (factor out a):
A  â†’ a A'
A' â†’ AB | Bc | Ac    â† still has common prefixes! (AB and Ac share A)

Step 2 (factor out A from A'):
A  â†’ a A'
A' â†’ A D | Bc
D  â†’ B | c           â† D handles the suffix after A

Final left-factored grammar:
A  â†’ aA'
A' â†’ AD | Bc
D  â†’ B | c
```

**Example 2: `S â†’ a | ab | abc | abcd`**

```
Step 1 (factor out a):
S  â†’ a S'
S' â†’ b | bc | bcd | Îµ    â† S' alternatives {b, bc, bcd} share b; Îµ is standalone

Step 2 (factor out b from {b, bc, bcd}):
S  â†’ a S'
S' â†’ b A | Îµ
A  â†’ c | cd | Îµ           â† A alternatives {c, cd} share c; Îµ standalone

Step 3 (factor out c from {c, cd}):
S  â†’ a S'
S' â†’ b A | Îµ
A  â†’ c B | Îµ
B  â†’ d | Îµ               â† Final! No more common prefixes.
```

**Example 3: `S â†’ aAd | aB`, `A â†’ a | ab`, `B â†’ ccd | ddc`**

```
S has common prefix a:
S  â†’ a S'
S' â†’ Ad | B             â† S' looks at FIRST(Ad) = {a} and FIRST(B) = {c, d} â†’ no conflict

A has common prefix a:
A  â†’ a A'
A' â†’ b | Îµ              â† FIRST(b) = {b}, FOLLOW(A') uses Îµ

Final:
S  â†’ aS'       S' â†’ Ad | B
A  â†’ aA'       A' â†’ b | Îµ
B  â†’ ccd | ddc
```

**Example 4: `S â†’ bSSaaS | bSSaSb | bSb | a`** (complex)

```
Step 1 (b is prefix of first 3):
S  â†’ bSS' | a
S' â†’ SaaS | SaSb | b     â† S' has common prefix S

Step 2 (factor out S from {SaaS, SaSb}):
S  â†’ bSS' | a
S' â†’ SA | b
A  â†’ aaS | aSb           â† FIRST(aaS) = {a}, FIRST(aSb) = {a} â€” still conflict!

Step 3 (A has common prefix a):
A  â†’ aA''
A'' â†’ aS | Sb            â† FIRST(aS) = {a}, FIRST(Sb) = {depends on FIRST(S)}

Final left-factored grammar:
S  â†’ bSS' | a
S' â†’ SA | b
A  â†’ aA''
A'' â†’ aS | Sb
```

## 5.6 Important Notes

- Left factoring **avoids backtracking** in recursive descent parsing
- Left factoring converts LL(k) â†’ LL(1) by reducing lookahead required
- Sometimes left factoring alone isn't enough â€” you may also need to eliminate left recursion
- The order matters: usually **eliminate left recursion first**, then **apply left factoring**

---

## âœ… Q&A â€” TOPIC 5

**Q1. What is left factoring and what problem does it solve?**

> **A:** Left factoring is a grammar transformation that removes common prefixes from multiple alternatives of a nonterminal. It solves the problem of a predictive (LL) parser not being able to choose between two productions using only 1-symbol lookahead â€” because both productions start with the same token(s).

**Q2. How does left factoring convert LL(k) to LL(1)?**

> **A:** By pulling out the common prefix Î± and creating a new nonterminal A' for the suffixes, we ensure that after reading Î±, the next token uniquely identifies which suffix to use (since FIRST(Î²â‚) and FIRST(Î²â‚‚) are disjoint after factoring). This means only 1 symbol of lookahead is needed at the decision point.

**Q3. Does left factoring change the language?**

> **A:** No. Left factoring is purely a syntactic transformation. The original and transformed grammars accept exactly the same set of strings. Only the structure of derivations changes.

**Q4. What is "deferring the decision" in left factoring?**

> **A:** With `A â†’ Î±Î²â‚ | Î±Î²â‚‚`, when the parser sees the first token of Î±, it can't decide between Î²â‚ and Î²â‚‚. By transforming to `A â†’ Î±A'`, `A' â†’ Î²â‚ | Î²â‚‚`, the parser first parses the entire Î±, and only then â€” at A' â€” uses the next input token to choose between Î²â‚ and Î²â‚‚. The decision is deferred to after Î± is consumed.

**Q5. Can left factoring always make a grammar LL(1)?**

> **A:** Not always. Left factoring removes common prefix conflicts (FIRST/FIRST conflicts), but a grammar may still have FIRST/FOLLOW conflicts (from nullable productions) or may simply not be LL(k) for any finite k (e.g., the dangling-else grammar). Left factoring is a necessary but not sufficient condition for LL(1).

**Q6. What happens if you don't apply left factoring to a recursive descent parser?**

> **A:** The parser may need to backtrack â€” trying one production, partially matching, failing, then trying another. This can be exponentially slow. For example, `S â†’ a | ab | abc | abcd` on input `abcd` would try `a` (match 1 token, fail at lookahead), then `ab` (match 2, fail), then `abc` (match 3, fail), finally `abcd` (success). Backtracking is generally avoided in production compilers.

**Q7. After left factoring `stmt â†’ if(E) stmt else stmt | if(E) stmt`, what do we get?**

> **A:**
> 
> ```
> stmt  â†’ if(E) stmt rest
> rest  â†’ else stmt | Îµ
> ```
> 
> The parser always matches `if(E) stmt`, then checks the next token at `rest`: if it's `else`, use `rest â†’ else stmt`; otherwise use `rest â†’ Îµ`.

---

# TOPIC 6 â€” THE ROLE OF THE PARSER & PARSER TYPES

## 6.1 Position in the Compiler

```
[Lexer] â†’ tokens â†’ [Parser] â†’ parse tree/AST â†’ [Semantic Analysis] â†’ ...
```

The parser:

1. **Receives tokens** from the lexer (one at a time)
2. **Verifies** the token stream against the grammar
3. **Builds a parse tree** for well-formed programs
4. **Reports syntax errors** for malformed programs
5. **Attempts error recovery** to continue processing after errors

## 6.2 LL vs LR Parsers â€” Complete Comparison

|Feature|LL Parser|LR Parser|
|---|---|---|
|**Direction**|Top-Down (root to leaves)|Bottom-Up (leaves to root)|
|**Derivation**|Leftmost|Rightmost (in reverse)|
|**Alias**|Predictive Parser|Shift-Reduce Parser|
|**Stack initially contains**|Start symbol|Nothing (empty)|
|**Stack finally contains**|Empty (success)|Start symbol (success)|
|**Tree traversal**|Pre-order (left to right)|Post-order|
|**When to apply production**|Before seeing matching input (predict)|After seeing full body on stack (reduce)|
|**Grammar class**|Must be left-recursion-free|Can have left recursion|
|**Lookahead use**|To predict which production to expand|To decide when to reduce|
|**Real examples**|javac (Java), hand-written parsers|GCC, Clang (C/C++)|

## 6.3 LL vs LR Grammar (Same Language, Different Form)

Both grammars below generate the same arithmetic expression language, but are structured for different parsers:

```
LL Grammar (for top-down):        LR Grammar (for bottom-up):
E  â†’ T E'                          E â†’ E + T | T
E' â†’ + T E' | Îµ                    T â†’ T * F | F
T  â†’ F T'                          F â†’ (E) | id | num
T' â†’ * F T' | Îµ
F  â†’ (E) | id | num
```

The LL grammar has **no left recursion**, uses right-recursive `E'` and `T'` to handle associativity. The LR grammar uses **left recursion** naturally to encode left associativity.

## 6.4 Universal Parsers

|Parser|Handles|Time Complexity|Use|
|---|---|---|---|
|**CYK**|Any CFG (in CNF)|O(nÂ³)|Theoretical, NLP|
|**Earley**|Any CFG|O(nÂ³) worst, better for many|Theoretical|

These are too slow for production compilers (which need O(n) or near-linear time).

## 6.5 PEG Parsers

**Parsing Expression Grammars** are a modern alternative. Python (CPython 3.9+) uses a PEG parser.

Differences from CFG:

- Uses **ordered choice** `/` instead of `|` (tries alternatives in order, first match wins)
- Never ambiguous (by definition of ordered choice)
- But not exactly equivalent to CFGs

## 6.6 Summary of Parser Classes

```
                     All Context-Free Languages
                           /           \
              Deterministic CFL      (Ambiguous/
                    |                 Hard grammars)
              /     |     \
           LR(k)  LALR   SLR(1)
              |
           LL(k)
              |
           LL(1)
```

> The hierarchy: LL(1) âŠ‚ SLR(1) âŠ‚ LALR(1) âŠ‚ LR(1) âŠ‚ All Deterministic CFLs

---

## âœ… Q&A â€” TOPIC 6

**Q1. What does "LL" stand for and what does each L mean?**

> **A:** **L**eft-to-right scan of input, **L**eftmost derivation. The optional number `k` in LL(k) means k symbols of lookahead. LL(1) is the most common: scan left-to-right, produce a leftmost derivation, with 1 symbol of lookahead.

**Q2. What does "LR" stand for and what does each letter mean?**

> **A:** **L**eft-to-right scan of input, **R**ightmost derivation in reverse. The number `k` in LR(k) means k symbols of lookahead. An LR parser reads input left to right but constructs the rightmost derivation by reducing (undoing rightmost steps in reverse order).

**Q3. Why can LR parsers handle left-recursive grammars but LL parsers cannot?**

> **A:** LL parsers predict: they expand a nonterminal before matching any input for it. With `A â†’ AÎ±`, the parser would call procedure `A` immediately, creating infinite recursion. LR parsers recognize from the bottom: they first read and shift tokens, then reduce when a complete rule body is found. They never need to predict â€” they recognize `A â†’ AÎ±` only after the entire `AÎ±` string is already on the stack.

**Q4. Which is more powerful â€” LL(1) or LR(1)?**

> **A:** LR(1) is strictly more powerful. Every LL(1) grammar is also LR(1), but there are LR(1) grammars that are not LL(1). For instance, left-recursive grammars are LR(1) but not LL(1). LR parsers can handle a larger class of languages.

**Q5. Why does the Java compiler use a recursive-descent (LL-style) parser while C compilers use LR parsers?**

> **A:** Java's syntax was deliberately designed to be LL(1)-friendly â€” constructs like `if`, `while`, `class` have keywords that uniquely identify the structure with 1-symbol lookahead. This allows a simple, hand-written recursive-descent parser. C's syntax, especially expressions with operators, complex declarations, and ambiguities (like the dangling-else), benefits from the more powerful LR parsing which handles these cases automatically.

**Q6. What is the "viable prefix property" and which parsers have it?**

> **A:** The viable prefix property means the parser detects an error as soon as the input **cannot** be extended to a valid program â€” without reading past the error point. Both LL(1) and LR parsers have this property. They report errors at the **earliest possible** point in left-to-right scanning.

---

# TOPIC 7 â€” SYNTAX ERROR HANDLING

## 7.1 Types of Programming Errors

|Error Type|When Caught|Description|Example|
|---|---|---|---|
|**Lexical**|Lexer phase|Token doesn't match any pattern|`"hello` (unmatched quote), `@#$`|
|**Syntactic**|Parser phase|Token stream violates grammar|Missing `;`, unmatched `{`, `case` without `switch`|
|**Semantic**|Semantic phase|Structurally OK but type/scope error|`void f() { return 5; }`, using undeclared variable|
|**Logical**|Runtime/Testing|Syntactically valid but wrong intent|`if (a = b)` instead of `if (a == b)` in C|

## 7.2 Responsibilities of the Parser's Error Handler

A good parser error handler must:

1. **Report** the presence of errors clearly and accurately (line number, position, what was expected)
2. **Recover** from each error quickly enough to detect subsequent errors
3. Add **minimal overhead** to correct-program processing

## 7.3 Error Reporting

**Minimum requirement:** Report the _place_ in the source where the error was detected.

> âš ï¸ Note: The place where an error is _detected_ may differ from where it actually _occurred_. The actual error might have happened a few tokens earlier.

**Typical error message format:**

```
Line 42: error: expected ';' before 'int'
```

## 7.4 Early Error Detection

Both LL and LR parsers have the **viable prefix property**: they detect errors at the **earliest possible moment** â€” as soon as the stream of tokens cannot be extended to a valid program. No valid symbol is shifted past the error point.

---

## âœ… Q&A â€” TOPIC 7

**Q1. Why is it important to recover from errors rather than stopping at the first one?**

> **A:** If a compiler stops at the first error, the programmer must fix it, recompile, see the next error, fix it, recompile, etc. This is slow and frustrating. By recovering and continuing, the compiler reports multiple errors in one run, saving time. The drawback is potential **cascaded errors** â€” spurious errors caused by the first real error.

**Q2. Why might the detected error location differ from the actual error location?**

> **A:** Syntax errors often manifest several tokens after they actually occur. For example, in `x = 5 + ;`, the parser doesn't realize anything is wrong until it sees the `;` after `+` â€” but the real error is the missing expression. The parser reports the `;` as unexpected, but the programmer's mistake was not writing a right-hand expression.

**Q3. What is a cascaded error and why is it a problem?**

> **A:** A cascaded error is an error reported by the parser that is caused by a previous real error, not a genuine mistake. For example, if a variable is undeclared (one real error), the parser may report that variable as an error every time it appears. The programmer sees 10 error messages but there's only 1 root cause. Good error recovery minimizes cascaded errors.

**Q4. Classify these errors: (a) `int 5x;` (b) `x = y +;` (c) `int x = "hello";` (d) `if (x = 5) { ... }`**

> **A:** (a) **Lexical** â€” `5x` is not a valid identifier or number. (b) **Syntactic** â€” missing expression after `+`. (c) **Semantic** â€” assigning a string to an `int`. (d) **Logical** â€” uses `=` (assignment) instead of `==` (comparison); syntactically valid in C.

---

# TOPIC 8 â€” ERROR RECOVERY STRATEGIES

## 8.1 Decision: Stop or Continue?

|Strategy|Behavior|Advantage|Disadvantage|Example Language|
|---|---|---|---|---|
|**Stop immediately**|Report 1 error, quit|No cascaded errors|Only 1 error per compile|Python (syntax errors)|
|**Continue with recovery**|Report multiple errors|See all errors at once|Risk of cascaded errors|C compilers|

## 8.2 What is Error Recovery?

**Error recovery** = adjusting the input stream (or parser state) so parsing can continue after an unexpected token.

**Three types of adjustments:**

- **Delete** one or more input tokens
- **Insert** a missing token
- **Substitute** one token for another

## 8.3 Strategy 1: Panic-Mode Recovery

**Idea:** On error, discard input tokens one-at-a-time until a **synchronizing token** is found, then resume.

**Synchronizing tokens** are usually:

- Statement terminators: `;`, `}`
- Block delimiters: `end`, `begin`
- Declaration keywords: `int`, `if`, `while`

**Algorithm:**

```
Error detected
  â†“
while next_token âˆ‰ synchronizing_set:
    discard next_token
  â†“
Resume parsing from the current parser state
```

**Example:**

```c
a = b + c      // no semicolon! Error here.
d = *e + f ;   // e is not a pointer â€” another error
```

In panic mode, the compiler discards `b + c` and `d = *e + f` until the `;` is found.

**Pros:** Simple to implement, guaranteed to terminate (no infinite loops). **Cons:** Large chunks of input may be skipped without checking for errors within them.

## 8.4 Strategy 2: Phrase-Level Recovery

**Idea:** Make a **local correction** to the remaining input to allow parsing to continue.

**Common local corrections:**

- Replace `,` with `;` â†’ `scanf("%d", &x)` â†’ `scanf("%d", &x);`
- Delete an extraneous `;` or `}`
- Insert a missing `;`

**Example:**

```c
scanf("%d", &x)       // missing semicolon â†’ insert one
scanf("%d" &x);       // missing comma â†’ insert one
{... {... }...}}      // extra } â†’ delete it
```

**Caution:** Must ensure corrections don't cause infinite loops (e.g., endlessly inserting tokens).

## 8.5 Strategy 3: Error Productions

**Idea:** If common errors are predictable, augment the grammar with **error productions** that match those erroneous constructs.

**Example:** For unary operators in `E â†’ +E | -E`:

We know programmers might accidentally write `a = *b` or `a = /b` (using * or / as unary operators â€” illegal). We augment:

```
E â†’ +E | -E | *A | /A
A â†’ E
```

When `*A` is matched, the parser generates:

> "Error: '*' cannot be used as a unary operator. Did you mean something else?"

Parsing can then **continue** from that point.

**Advantage:** Parser gives helpful, specific error messages for known mistakes. **Disadvantage:** Need to anticipate all common errors in advance.

## 8.6 Strategy 4: Global Correction

**Idea:** Find the **minimum-edit-distance** correction to transform the erroneous input into a valid string.

**Process:**

- Given: erroneous input `x` and grammar `G`
- Find: valid string `y` such that the edit distance (insertions + deletions + substitutions) from `x` to `y` is minimized

**Reality:** Algorithms exist (e.g., Aho-Peterson), but the time and space cost is too high for practical compilers. **Currently only of theoretical interest.**

---

## âœ… Q&A â€” TOPIC 8

**Q1. What is panic-mode recovery and why is it called "panic"?**

> **A:** In panic mode, when an error is found, the parser "panics" â€” it throws away input tokens until it finds a "safe" synchronizing token where it can confidently resume. The name reflects the drastic, non-surgical approach: just skip everything until you find solid ground.

**Q2. How are synchronizing tokens chosen for panic-mode recovery?**

> **A:** Synchronizing tokens are chosen based on their clear, unambiguous role in the program. Semicolons (`;`) end statements; closing braces (`}`) end blocks. When found, the parser knows exactly what context it's resuming into. In predictive parsing, FOLLOW(A) tokens serve as synchronizing tokens for nonterminal A.

**Q3. What is the risk of phrase-level recovery?**

> **A:** The risk is infinite loops. If the parser keeps inserting tokens to continue, it might fall into a cycle of inserting the same token, failing, inserting again, etc. Careful implementation must ensure that after any correction, at least one input token is consumed before the next correction.

**Q4. Why is global correction not used in practice?**

> **A:** The algorithms to find globally minimum-cost corrections have time complexity O(n Ã— |G|) or worse, and require storing large tables proportional to the grammar size Ã— input length. For real programming languages with hundreds of grammar rules and thousands-of-line programs, this is far too expensive. The theoretical benefit (perfect corrections) doesn't justify the cost.

**Q5. When would error productions be a good choice?**

> **A:** When you know exactly what errors your users frequently make. For example, a teaching compiler for beginners might include error productions for common mistakes like forgetting parentheses in `if` statements: `if x > 5 then ...` (Python style in a C compiler). The compiler can recognize this pattern and say "did you mean `if (x > 5)`?"

**Q6. Which error recovery strategy does Python use?**

> **A:** Python uses **immediate stop on syntax errors** â€” it reports the first syntax error and terminates parsing. This avoids cascaded errors but means programmers see only one error per run. Python's rationale: its indentation-based syntax makes recovery extremely difficult, and the errors are usually so specific that one at a time is practical.

---

# TOPIC 9 â€” TOP-DOWN PARSING & RECURSIVE DESCENT

## 9.1 Top-Down Parsing â€” Two Views

**View 1:** Building a parse tree **from the root down** to the leaves, creating nodes in **pre-order** (depth-first, left-to-right).

**View 2:** Finding a **leftmost derivation** of the input string. At each step, we expand the leftmost nonterminal.

**The core challenge:** When we have nonterminal `A` on the stack and we need to apply `A â†’ Î±`, which production of A do we choose? The **FIRST** set helps us answer this.

## 9.2 Recursive Descent Parsing (RDP)

**Concept:** Write one function/procedure for each nonterminal in the grammar. Each function tries to match the RHS of some production for that nonterminal.

**Structure:**

- **Matching a terminal:** Compare with input token; if match, consume token (advance input pointer); else report error
- **Matching a nonterminal:** Call the corresponding function (may be recursive)

**Works well for:** LL(1) grammars (left-factored, no left recursion, no ambiguity)

**Example grammar:**

```
S â†’ cAd
A â†’ aB
B â†’ b | Îµ
```

**Pseudocode:**

```python
def match(t):          # match terminal t
    if current_token == t:
        advance()      # consume token
    else:
        error("expected " + t)

def parse_S():
    match('c')         # expect 'c'
    parse_A()          # call A's procedure
    match('d')         # expect 'd'

def parse_A():
    match('a')         # expect 'a'
    parse_B()          # call B's procedure

def parse_B():
    if current_token == 'b':   # lookahead: if 'b', use B â†’ b
        match('b')
    else:                       # else use B â†’ Îµ (consume nothing)
        pass
```

## 9.3 Worked Example 1: Parsing `cad` with `S â†’ cAd`, `A â†’ aB`, `B â†’ b | Îµ`

|Expansion So Far|Input Remaining|Action Taken|
|---|---|---|
|S|cad|Start: expand S â†’ cAd|
|cAd|cad|**match c** (c=c âœ“)|
|caBd|cad|Expand A â†’ aB|
|caBd|ad (c consumed)|**match a** (a=a âœ“)|
|cad (Bâ†’Îµ)|d|Lookahead=d, not b â†’ expand B â†’ Îµ|
|cad|d|**match d** (d=d âœ“) â†’ **SUCCESS!**|

## 9.4 Worked Example 2: Parsing `ibta` with `S â†’ iCtSeS | iCtS | a`, `C â†’ b`

After left-factoring:

```
S â†’ iCtSA | a
A â†’ eS | Îµ
C â†’ b
```

|Expansion So Far|Input Remaining|Action|
|---|---|---|
|S|ibta|Lookahead=i â†’ expand S â†’ iCtSA|
|iCtSA|ibta|match `i`|
|iCbSA?|bta|Expand C â†’ b; match `b`|
|ibtSA|ta|match `t`|
|ibtaA|a|Expand S â†’ a; match `a`|
|ibta(Aâ†’Îµ)|$|Lookahead=$, not e â†’ expand A â†’ Îµ â†’ **SUCCESS!**|

## 9.5 RDP Without Left Factoring â€” Why It Fails

**Grammar without left factoring:** `S â†’ iCtSeS | iCtS | a`

When the parser sees `i`, it can't choose between `iCtSeS` and `iCtS` with just 1-symbol lookahead. It would need to parse `iCtS` first, then check if the next symbol is `e`. Without left factoring:

- No LL(1) table can be built (conflict in M[S, i])
- RDP would require backtracking (inefficient)

This is exactly why we left-factor first!

## 9.6 Advantages and Disadvantages of RDP

|Advantages|Disadvantages|
|---|---|
|Easy to write by hand|Can't handle left-recursive grammars|
|Code structure mirrors grammar structure|Requires backtracking if not LL(1)|
|Excellent error messages|Less powerful than LR parsers|
|Java (javac), older Python use it|Less automatic than table-driven|
|Good for languages designed for it|Grammar must be restructured|

---

## âœ… Q&A â€” TOPIC 9

**Q1. In recursive descent parsing, what happens when a function for nonterminal A is called?**

> **A:** The function for A looks at the current input token (lookahead), uses it to select the appropriate production for A (using FIRST sets), then executes the production body: for each symbol in the body, it either calls `match(terminal)` to consume a token, or calls the function for that nonterminal recursively.

**Q2. Why is RDP called "recursive descent"?**

> **A:** "Recursive" because the functions call each other recursively â€” a function for nonterminal A may call functions for B and C, which may in turn call A's function again (in non-left-recursive grammars). "Descent" because the parse tree is built from the root (top) downward, and the function call stack mirrors the parse tree structure.

**Q3. How does a recursive descent parser know which production to apply for a nonterminal?**

> **A:** It uses the current input token (lookahead) and the FIRST sets. If production `A â†’ Î±` is chosen when the lookahead is in FIRST(Î±), the parser picks that production. For LL(1) grammars, the lookahead uniquely determines the production (no ambiguity). This is why the grammar must have disjoint FIRST sets for all alternatives.

**Q4. What does the `match(t)` function do, and what happens if the match fails?**

> **A:** `match(t)` checks if the current input token equals `t`. If it matches, it consumes the token (advances the input pointer to the next token). If it doesn't match, it's a syntax error â€” the parser reports an error message like "expected `t`, found `x`."

**Q5. Can RDP handle grammars with Îµ-productions?**

> **A:** Yes. For a production `A â†’ Îµ`, the corresponding function checks: if the current lookahead is in FOLLOW(A), then apply `A â†’ Îµ` (consume nothing, just return). This is why FOLLOW sets are needed â€” they tell us when it's safe to use Îµ.

**Q6. What is the key structural rule for writing RDP code from a grammar?**

> **A:** One function per nonterminal. Inside each function: for each symbol in the production body, if it's a terminal call `match(symbol)`, if it's a nonterminal call its function. For multiple alternatives, use `if/else if` on the current lookahead to select the right production.

---

# TOPIC 10 â€” FIRST AND FOLLOW SETS

## 10.1 Why FIRST and FOLLOW?

**Purpose:** To enable a predictive parser to decide which production to apply using only the next input token (1-symbol lookahead).

- **FIRST** helps choose the right production based on what the first token could be
- **FOLLOW** helps decide when to use Îµ-productions (use `A â†’ Îµ` when the next token can follow A)

**Practical benefit in compiler design:**

> If the compiler knows "the first character of the string produced when production A â†’ Î± is applied" and compares it to the current input token, it can immediately choose the right production without guessing.

**Example:** `S â†’ cAd`, `A â†’ bc | a`, Input: `cad`

- After matching `c` in `S â†’ cAd`, the next input is `a`
- FIRST(bc) = {b}, FIRST(a) = {a}
- Current token `a` is in FIRST(a) â†’ choose `A â†’ a` (not `A â†’ bc`)

## 10.2 FIRST Sets

**Definition:** `FIRST(X)` = the set of terminals `a` such that `X â‡’* a...` (X can derive a string beginning with a). If `X â‡’* Îµ`, then `Îµ âˆˆ FIRST(X)`.

### Computing FIRST â€” Complete Rules

**Rule 1:** If X is a terminal: `FIRST(X) = {X}`

**Rule 2:** If X â†’ Îµ is a production: add `Îµ` to `FIRST(X)`

**Rule 3:** If X â†’ a... (starts with a terminal `a`): add `a` to `FIRST(X)`

**Rule 4 (General):** If X â†’ Yâ‚Yâ‚‚Yâ‚ƒ...Yâ‚–:

```
1. Add all of FIRST(Yâ‚) except Îµ to FIRST(X)
2. If Îµ âˆˆ FIRST(Yâ‚): add all of FIRST(Yâ‚‚) except Îµ to FIRST(X)
3. If Îµ âˆˆ FIRST(Yâ‚) and Îµ âˆˆ FIRST(Yâ‚‚): add all of FIRST(Yâ‚ƒ) except Îµ to FIRST(X)
...
k. If Îµ âˆˆ FIRST(Yáµ¢) for ALL i = 1 to k: add Îµ to FIRST(X)
```

**Intuition for Rule 4:** The first terminal in `Yâ‚Yâ‚‚...Yâ‚–` comes from Yâ‚ unless Yâ‚ can vanish (derive Îµ), in which case the first terminal comes from Yâ‚‚, and so on.

### FIRST Example 1: Expression Grammar (4.2)

```
Grammar:
E  â†’ T E'
E' â†’ + T E' | Îµ
T  â†’ F T'
T' â†’ * F T' | Îµ
F  â†’ (E) | id
```

```
FIRST(F)  = {(, id}              [F â†’ (E): starts with (; F â†’ id: starts with id]
FIRST(T') = {*, Îµ}               [T' â†’ *FT': starts with *; T' â†’ Îµ: add Îµ]
FIRST(T)  = FIRST(FT')
           = FIRST(F)             [since Îµ âˆ‰ FIRST(F)]
           = {(, id}
FIRST(E') = {+, Îµ}               [E' â†’ +TE': starts with +; E' â†’ Îµ: add Îµ]
FIRST(E)  = FIRST(TE')
           = FIRST(T)             [since Îµ âˆ‰ FIRST(T)]
           = {(, id}
```

**Summary table:**

|Symbol|FIRST|
|---|---|
|E|{(, id}|
|E'|{+, Îµ}|
|T|{(, id}|
|T'|{*, Îµ}|
|F|{(, id}|

### FIRST Example 2: ABCDE Grammar

```
S â†’ ABCDE
A â†’ a | Îµ
B â†’ b | Îµ
C â†’ c | Îµ
D â†’ d | Îµ
E â†’ e | Îµ
```

```
FIRST(A) = {a, Îµ}
FIRST(B) = {b, Îµ}
FIRST(C) = {c, Îµ}
FIRST(D) = {d, Îµ}
FIRST(E) = {e, Îµ}
FIRST(S) = FIRST(ABCDE):
  â†’ add FIRST(A) - Îµ = {a}
  â†’ Îµ âˆˆ FIRST(A) â†’ add FIRST(B) - Îµ = {b}
  â†’ Îµ âˆˆ FIRST(B) â†’ add FIRST(C) - Îµ = {c}
  â†’ Îµ âˆˆ FIRST(C) â†’ add FIRST(D) - Îµ = {d}
  â†’ Îµ âˆˆ FIRST(D) â†’ add FIRST(E) - Îµ = {e}
  â†’ Îµ âˆˆ FIRST(E) â†’ add Îµ
FIRST(S) = {a, b, c, d, e, Îµ}
```

## 10.3 FOLLOW Sets

**Definition:** `FOLLOW(A)` = the set of terminals `a` such that `S â‡’* ...Aa...` (a can appear immediately to the right of A in some sentential form).

Special: `$ âˆˆ FOLLOW(S)` where S is the start symbol ($ = end-of-input marker).

> **Important:** Îµ is never in a FOLLOW set. FOLLOW contains only actual terminals and $.

### Computing FOLLOW â€” Rules

For each production `B â†’ Î±AÎ²` in the grammar:

1. Add `FIRST(Î²) - {Îµ}` to `FOLLOW(A)`
2. If `Îµ âˆˆ FIRST(Î²)` (or Î² = Îµ): add `FOLLOW(B)` to `FOLLOW(A)`

**Algorithm:** Initialize `FOLLOW(S) = {$}`. Apply the rules above repeatedly until no FOLLOW set changes.

### FOLLOW Example: Expression Grammar (4.2)

```
E  â†’ T E'
E' â†’ + T E' | Îµ
T  â†’ F T'
T' â†’ * F T' | Îµ
F  â†’ (E) | id
```

**Computing FOLLOW(E):**

- E is start symbol â†’ add $: `FOLLOW(E) = {$}`
- F â†’ (E): E appears before `)` â†’ add `FIRST())` = {)} â†’ `FOLLOW(E) = {), $}`

**Computing FOLLOW(E'):**

- E â†’ TE': E' is at the end of E's body â†’ add FOLLOW(E) = {), $} â†’ `FOLLOW(E') = {), $}`

**Computing FOLLOW(T):**

- E â†’ TE': T appears before E', FIRST(E') = {+, Îµ}
    - Add FIRST(E') - Îµ = {+} â†’ FOLLOW(T) âˆ‹ +
    - Îµ âˆˆ FIRST(E') â†’ add FOLLOW(E) = {), $} â†’ `FOLLOW(T) = {+, ), $}`

**Computing FOLLOW(T'):**

- T â†’ FT': T' is at the end â†’ add FOLLOW(T) = {+, ), $} â†’ `FOLLOW(T') = {+, ), $}`

**Computing FOLLOW(F):**

- T â†’ FT': F appears before T', FIRST(T') = {*, Îµ}
    - Add FIRST(T') - Îµ = {*} â†’ FOLLOW(F) âˆ‹ *
    - Îµ âˆˆ FIRST(T') â†’ add FOLLOW(T) = {+, ), $} â†’ `FOLLOW(F) = {*, +, ), $}`

**Summary table:**

|Symbol|FIRST|FOLLOW|
|---|---|---|
|E|{(, id}|{), $}|
|E'|{+, Îµ}|{), $}|
|T|{(, id}|{+, ), $}|
|T'|{*, Îµ}|{+, ), $}|
|F|{(, id}|{*, +, ), $}|

## 10.4 Exercise: Grammar S â†’ L=R | R, L â†’ *R | id, R â†’ L

```
FIRST(L) = {*, id}    [L â†’ *R: starts with *; L â†’ id: starts with id]
FIRST(R) = FIRST(L) = {*, id}    [R â†’ L; L can start with * or id]
FIRST(S) = FIRST(L=R) âˆª FIRST(R) = {*, id} âˆª {*, id} = {*, id}
```

```
FOLLOW(S) = {$}
FOLLOW(R) = ?
  - In S â†’ L=R: R is at end â†’ add FOLLOW(S) = {$}
  - In L â†’ *R: R is at end â†’ add FOLLOW(L)
  FOLLOW(L) = ?
    - In S â†’ L=R: L appears before =, so add FIRST(=R) - Îµ = {=}
    - In S â†’ R â†’ L: L is end â†’ add FOLLOW(R)
    - (circular: FOLLOW(L) and FOLLOW(R) depend on each other)
  Solve: FOLLOW(L) = {=, $}, FOLLOW(R) = {=, $} (since L â†’ *R means FOLLOW(R) âŠ‡ FOLLOW(L))
```

**Final:**

|Symbol|FIRST|FOLLOW|
|---|---|---|
|S|{*, id}|{$}|
|L|{*, id}|{=, $}|
|R|{*, id}|{=, $}|

---

## âœ… Q&A â€” TOPIC 10

**Q1. What is FIRST(X) informally?**

> **A:** FIRST(X) is the set of tokens that can "start" any string derivable from X. It answers: "if I'm about to parse X, what token might I see first?"

**Q2. Why might Îµ be in a FIRST set? Give an example.**

> **A:** Îµ is in FIRST(X) if X can derive the empty string â€” i.e., X can "disappear." Example: `A â†’ Îµ` gives FIRST(A) = {Îµ}. Or: `A â†’ BC`, `B â†’ Îµ`, `C â†’ Îµ` gives FIRST(A) = {Îµ}. This matters when deciding if an Îµ-production should be applied.

**Q3. Why is Îµ never in a FOLLOW set?**

> **A:** FOLLOW(A) contains the tokens that appear immediately **after** A in actual input. Since Îµ represents "nothing," it can't physically appear after A in the token stream. FOLLOW contains actual terminals and the end-marker $, but never Îµ.

**Q4. How do you compute FIRST of a string XYZ (multiple symbols)?**

> **A:** Start with FIRST(X). If X can derive Îµ (i.e., Îµ âˆˆ FIRST(X)), also include FIRST(Y). If both X and Y can derive Îµ, also include FIRST(Z). If all of X, Y, Z can derive Îµ, include Îµ in the result.

**Q5. In `T â†’ FT'`, `T' â†’ *FT' | Îµ`, why is FOLLOW(T') = FOLLOW(T)?**

> **A:** Because T' appears at the end of T's production body `T â†’ FT'`. Anything that can follow T can follow T' as well. When T' derives Îµ (disappears), T essentially ends where T would have ended â€” so whatever follows T also follows T'.

__Q6. Why do we add FOLLOW(B) to FOLLOW(A) when B â†’ Î±AÎ² and Î² â‡’_ Îµ?_*

> **A:** If Î² can derive Îµ, then A might be the last symbol in B's expansion. Anything that can follow B can then also immediately follow A. So FOLLOW(B) âŠ† FOLLOW(A).

**Q7. Compute FIRST and FOLLOW for: `S â†’ aSb | Îµ`**

> **A:** FIRST(S): from `S â†’ aSb`: starts with `a` â†’ {a}. From `S â†’ Îµ`: add Îµ â†’ **FIRST(S) = {a, Îµ}**. FOLLOW(S): Start symbol â†’ {$}. From `S â†’ aSb`: S appears before `b` â†’ add {b}. Also S appears after `a`, but we want what follows S, which is `b` or end. **FOLLOW(S) = {b, $}**.

**Q8. What is the practical significance of FIRST in a predictive parser?**

> **A:** Given nonterminal A with productions `A â†’ Î±â‚ | Î±â‚‚ | ... | Î±â‚™`, when the current lookahead is token `t`, the parser applies production `A â†’ Î±áµ¢` if `t âˆˆ FIRST(Î±áµ¢)`. This makes prediction deterministic â€” no guessing or backtracking needed (for LL(1) grammars where FIRST sets are disjoint).

**Q9. Why might FOLLOW matter for Îµ-productions specifically?**

> **A:** For `A â†’ Îµ`, the production body is empty â€” FIRST(Îµ) = {Îµ}. This doesn't tell us when to apply it. We use FOLLOW(A): if the current lookahead `t âˆˆ FOLLOW(A)`, we apply `A â†’ Îµ` (because the next token is something that legitimately follows A, so A must be done = it derived Îµ).

---

# TOPIC 11 â€” LL(1) GRAMMARS & PREDICTIVE PARSING

## 11.1 What is Predictive Parsing?

**Predictive parsing** is a top-down parsing method that:

- Makes **no backtracking** decisions
- Uses 1 symbol of lookahead to **predict** exactly which production to apply
- Is equivalent to constructing a leftmost derivation
- Works on **LL(1) grammars**

## 11.2 LL(1) Definition â€” Formal

A grammar G is **LL(1)** if and only if whenever `A â†’ Î± | Î²` are two distinct productions, **all of the following hold:**

**Condition 1 â€” No FIRST/FIRST conflict:**

```
FIRST(Î±) âˆ© FIRST(Î²) = âˆ…
```

**Condition 2 â€” No FIRST/FOLLOW conflict:**

```
If Îµ âˆˆ FIRST(Î±):  FIRST(Î²) âˆ© FOLLOW(A) = âˆ…
If Îµ âˆˆ FIRST(Î²):  FIRST(Î±) âˆ© FOLLOW(A) = âˆ…
```

> In other words: for each pair of alternatives, their effective "starter sets" must be completely disjoint. The "effective starter set" of `Î±` is FIRST(Î±) if Îµ âˆ‰ FIRST(Î±), and FIRST(Î±) âˆª FOLLOW(A) if Îµ âˆˆ FIRST(Î±).

## 11.3 FIRST/FIRST Conflict Example

```
S â†’ Xb | Yc
X â†’ a
Y â†’ a
```

`FIRST(Xb) = {a}` and `FIRST(Yc) = {a}` â†’ **conflict!**

Seeing `a` on the input, the parser can't decide: use `S â†’ Xb` or `S â†’ Yc`? Both start with `a`. This grammar is **NOT LL(1)**.

## 11.4 FIRST/FOLLOW Conflict Example

```
S â†’ AB
A â†’ fe | Îµ
B â†’ fg
```

`FIRST(fe) = {f}`. `Îµ âˆˆ FIRST(A)` via `A â†’ Îµ`. `FOLLOW(A) = FIRST(B) = {f}`.

Seeing `f` on the input: use `A â†’ fe` (because f âˆˆ FIRST(fe)) OR use `A â†’ Îµ` and let B handle f (because f âˆˆ FOLLOW(A))? The parser can't decide. **NOT LL(1)**.

## 11.5 Checking if a Grammar is LL(1) â€” Examples

### Example G1: `S â†’ Aa`, `A â†’ bAb | Îµ`

```
FIRST(bAb) = {b}
FIRST(Îµ) = {Îµ}
FOLLOW(A): A appears before a in S â†’ Aa â†’ add {a}
           A appears before b in A â†’ bAb â†’ add FIRST(b) = {b}
           FOLLOW(A) = {a, b}

Condition 2: Îµ âˆˆ FIRST(Îµ) â†’ check FIRST(bAb) âˆ© FOLLOW(A) = {b} âˆ© {a, b} = {b} â‰  âˆ…
```

**G1 is NOT LL(1)** â€” FIRST/FOLLOW conflict on `b`.

### Example G2: `S â†’ Aa`, `A â†’ Abb | Îµ`

```
FIRST(Abb): A can start with Abb â†’ need FIRST(A) first
  A â†’ Abb: starts with A (which is nullable via A â†’ Îµ), then b â†’ FIRST(A) = {b, Îµ}
  Wait: FIRST(Abb):
    Add FIRST(A) - Îµ = {b}. Îµ âˆˆ FIRST(A) â†’ add FIRST(b) = {b} â†’ FIRST(Abb) = {b}
FOLLOW(A) = {b, a} (same analysis as G1)
Condition 2: Îµ âˆˆ FIRST(Îµ) â†’ check FIRST(Abb) âˆ© FOLLOW(A) = {b} âˆ© {b, a} = {b} â‰  âˆ…
```

**G2 is NOT LL(1).**

### Example G3: `S â†’ A | xb`, `A â†’ aAb | B`, `B â†’ x`

```
FIRST(A):
  A â†’ aAb: starts with a â†’ {a}
  A â†’ B â†’ x: starts with x â†’ {x}
  FIRST(A) = {a, x}
FIRST(xb) = {x}

Condition 1 for S: FIRST(A) âˆ© FIRST(xb) = {a, x} âˆ© {x} = {x} â‰  âˆ…
```

**G3 is NOT LL(1)** â€” when parser sees `x`, it can't choose between `S â†’ A` and `S â†’ xb`.

### Example: `X â†’ YaYb | ZbZa`, `Y â†’ Îµ`, `Z â†’ Îµ`

```
FIRST(YaYb):
  Y â†’ Îµ: Îµ âˆˆ FIRST(Y) â†’ skip Y, look at next symbol a â†’ FIRST(YaYb) = {a}
FIRST(ZbZa):
  Z â†’ Îµ: Îµ âˆˆ FIRST(Z) â†’ skip Z, look at b â†’ FIRST(ZbZa) = {b}

Condition 1: {a} âˆ© {b} = âˆ… âœ“
```

**This grammar IS LL(1)!**

## 11.6 LL(k) â€” Beyond LL(1)

Some grammars require `k > 1` symbols of lookahead:

**Example: `S â†’ abB | aaA`, `A â†’ c | d`, `B â†’ d`**

With 1-symbol lookahead: seeing `a`, can't choose. Need 2 symbols: if `ab` â†’ use `abB`; if `aa` â†’ use `aaA`. This is **LL(2)**.

**Example where no finite k works:**

```
S â†’ iCtS | iCtSeS | a   (dangling-else)
```

To choose between `iCtS` and `iCtSeS`, you need to know if an `e` (else) will follow the inner `S`. But `S` can generate arbitrarily long strings, so no fixed `k` suffices. **Not LL(k) for any finite k.**

Similarly: `E â†’ T+E | T`, `T â†’ id | id*T | (E)` â€” to distinguish `T+E` from `T`, you'd need to read past all of `T`, which can be unbounded.

---

## âœ… Q&A â€” TOPIC 11

**Q1. State the two conditions for a grammar to be LL(1).**

> **A:** For every pair of distinct productions `A â†’ Î± | Î²`: (1) **FIRST/FIRST:** FIRST(Î±) and FIRST(Î²) must be disjoint. (2) **FIRST/FOLLOW:** If Î± can derive Îµ, then FIRST(Î²) and FOLLOW(A) must be disjoint; and if Î² can derive Îµ, then FIRST(Î±) and FOLLOW(A) must be disjoint.

**Q2. Can an ambiguous grammar be LL(1)? Why?**

> **A:** No. If a grammar is ambiguous, some string has two leftmost derivations. That means at some point, there are two different choices for expanding a nonterminal that can produce the same first token â€” a FIRST/FIRST conflict. This violates LL(1) condition 1.

**Q3. Can a left-recursive grammar be LL(1)? Why?**

> **A:** No. Left recursion causes infinite loops in top-down parsers, and also creates FIRST set conflicts. For `A â†’ AÎ± | Î²`, FIRST(AÎ±) includes FIRST(A) which includes FIRST(Î²), so FIRST(AÎ±) âˆ© FIRST(Î²) â‰  âˆ…. This violates condition 1.

**Q4. What does it mean for a grammar to not be LL(k) for any finite k?**

> **A:** It means no matter how many symbols we look ahead, there will always exist some point in parsing where the lookahead window doesn't uniquely determine the correct production. The dangling-else is an example â€” the distance to the distinguishing `else` is unbounded, so no fixed window covers it.

**Q5. Is every LL(1) grammar also LR(1)?**

> **A:** Yes. Every LL(1) grammar is also LR(1) (and in fact SLR(1) and LALR(1)). The LR parser classes are strictly more powerful than LL(1). So LL(1) âŠ‚ SLR(1) âŠ‚ LALR(1) âŠ‚ LR(1).

**Q6. Determine if `A â†’ aA | Îµ` is LL(1).**

> **A:** FIRST(aA) = {a}. FIRST(Îµ) = {Îµ}. The productions are `A â†’ aA` and `A â†’ Îµ`. Condition 1: FIRST(aA) âˆ© FIRST(Îµ) = {a} âˆ© âˆ… = âˆ… âœ“ (Îµ itself isn't a terminal). Condition 2: Îµ âˆˆ FIRST(Îµ) â†’ check FIRST(aA) âˆ© FOLLOW(A). If FOLLOW(A) contains `a`, we have a conflict. For a grammar like `S â†’ A$`, FOLLOW(A) = {$}. FIRST(aA) âˆ© FOLLOW(A) = {a} âˆ© {$} = âˆ… âœ“. So **yes, LL(1)** (for this grammar context).

**Q7. Why is `if-else` called the "dangling else problem" and how is it resolved?**

> **A:** In `stmt â†’ if(E) stmt | if(E) stmt else stmt`, after parsing `if(E) stmt`, seeing `else`, two choices: reduce the inner `if(E) stmt` and use the `else` for an outer `if`, or continue parsing `else stmt` for the inner `if`. This creates ambiguity and is not LL(k) for any k. Resolution: use left factoring to `stmt â†’ if(E) stmt rest`, `rest â†’ else stmt | Îµ`, and use the convention of associating `else` with the nearest `if` (i.e., prefer `rest â†’ else stmt` over `rest â†’ Îµ` when `else` is seen).

---

# TOPIC 12 â€” PREDICTIVE PARSING TABLE CONSTRUCTION

## 12.1 The Parsing Table M[A, a]

A **predictive parsing table** is a 2D array `M[A, a]` where:

- Rows: nonterminals A
- Columns: terminals a and $
- `M[A, a]` contains the production to apply when stack has A and input has a

**Construction Algorithm:**

For each production `A â†’ Î±` in the grammar:

1. For each terminal `a âˆˆ FIRST(Î±)` (excluding Îµ): add `A â†’ Î±` to `M[A, a]`
2. If `Îµ âˆˆ FIRST(Î±)`:
    - For each terminal `b âˆˆ FOLLOW(A)`: add `A â†’ Î±` to `M[A, b]`
    - If `$ âˆˆ FOLLOW(A)`: add `A â†’ Î±` to `M[A, $]`
3. All cells not filled by the above rules are **error** (empty)

> **If any cell gets more than one entry â†’ grammar is NOT LL(1)!**

## 12.2 Complete Table: Grammar 4.2

Grammar: `Eâ†’TE'`, `E'â†’+TE'|Îµ`, `Tâ†’FT'`, `T'â†’*FT'|Îµ`, `Fâ†’(E)|id`

FIRST and FOLLOW (from Topic 10):

||FIRST|FOLLOW|
|---|---|---|
|E|{(, id}|{), $}|
|E'|{+, Îµ}|{), $}|
|T|{(, id}|{+, ), $}|
|T'|{*, Îµ}|{+, ), $}|
|F|{(, id}|{*, +, ), $}|

**Building M:**

||id|+|*|(|)|$|
|---|---|---|---|---|---|---|
|**E**|Eâ†’TE'|||Eâ†’TE'|||
|**E'**||E'â†’+TE'|||E'â†’Îµ|E'â†’Îµ|
|**T**|Tâ†’FT'|||Tâ†’FT'|||
|**T'**||T'â†’Îµ|T'â†’*FT'||T'â†’Îµ|T'â†’Îµ|
|**F**|Fâ†’id|||Fâ†’(E)|||

All empty cells = **error**.

## 12.3 Table for `S â†’ +SS | *SS | a`

```
FIRST(+SS) = {+}
FIRST(*SS) = {*}
FIRST(a) = {a}
FOLLOW(S) = {+, *, a, $}   (S appears before S: FIRST(S) = {+, *, a}; also $)
```

||+|*|a|$|
|---|---|---|---|---|
|**S**|Sâ†’+SS|Sâ†’*SS|Sâ†’a||

No conflicts â†’ **LL(1) grammar!**

## 12.4 Ambiguous Grammar â†’ Multiple Entries (Conflict)

For `S â†’ iCtSA | a`, `A â†’ eS | Îµ`, `C â†’ b`:

```
FIRST(iCtSA) = {i}
FIRST(a) = {a}
For A: FIRST(eS) = {e}, FIRST(Îµ) â†’ need FOLLOW(A)
FOLLOW(A) = FOLLOW(S) = {e, $}  (A is at end of S)
```

Cell `M[A, e]` gets both `A â†’ eS` (because e âˆˆ FIRST(eS)) and `A â†’ Îµ` (because e âˆˆ FOLLOW(A)).

**Conflict! Grammar is NOT LL(1)** â€” this is the dangling-else ambiguity.

---

## âœ… Q&A â€” TOPIC 12

**Q1. What does an empty cell in the parsing table mean?**

> **A:** An empty cell `M[A, a]` means it's a **syntax error** for the parser to have nonterminal A on the stack while seeing token a in the input. The parser should invoke its error recovery routine.

**Q2. What does a multiple-entry cell mean?**

> **A:** A multiple-entry cell means the grammar has a **conflict** â€” for the same state (nonterminal A) and lookahead (a), there are two possible productions. This means the grammar is not LL(1). The parser can't deterministically choose one.

**Q3. How do you build entry M[A, a] for production A â†’ Îµ?**

> **A:** For `A â†’ Îµ`, FIRST(Îµ) = {Îµ}. Since Îµ is not a terminal, we don't directly add to FIRST. Instead, for each b âˆˆ FOLLOW(A), add `A â†’ Îµ` to M[A, b]. This is the rule: Îµ-productions go into cells for FOLLOW symbols.

**Q4. Construct the LL(1) table for `S â†’ AaAb | BbBa`, `A â†’ Îµ`, `B â†’ Îµ`.**

> **A:** FIRST(AaAb): Aâ†’Îµ, so FIRST = FIRST(a...) = {a}. FIRST(BbBa): Bâ†’Îµ, FIRST = {b}. FOLLOW(S) = {$}.
> 
> ||a|b|$|
> |---|---|---|---|
> |S|Sâ†’AaAb|Sâ†’BbBa||
> |A|Aâ†’Îµ|Aâ†’Îµ||
> |B|Bâ†’Îµ|Bâ†’Îµ||
> 
> No conflicts â†’ LL(1). âœ“

**Q5. Why must FOLLOW tokens be used for Îµ-productions in the parsing table?**

> **A:** For `A â†’ Îµ`, there are no terminals that "start" the production (since it produces nothing). Instead, we apply `A â†’ Îµ` when the next input token is something that legitimately follows A â€” meaning A should be empty right now. FOLLOW(A) tells us exactly those tokens.

# TOPIC 13 â€” NON-RECURSIVE PREDICTIVE (TABLE-DRIVEN) PARSING

## 13.1 Overview

Instead of actual recursive function calls, a **table-driven LL parser** uses:

1. An **explicit stack** (of grammar symbols)
2. The **parsing table M**
3. An **input buffer**

This avoids function call overhead and is easily implementable in any language.

## 13.2 Initial Configuration

```
Stack (top â†’ bottom):  [  S  |  $  ]
Input buffer:          tokenâ‚ tokenâ‚‚ ... tokenâ‚™ $
```

Start symbol S is on top; $ at the bottom of both stack and input.

## 13.3 Algorithm

```
X = top of stack
a = current input token

while X â‰  $:
    if X is a terminal:
        if X == a:
            pop X from stack
            advance a to next input symbol    # "match"
        else:
            error()   # terminal mismatch
    else:   # X is a nonterminal
        if M[X, a] = "X â†’ Yâ‚Yâ‚‚...Yâ‚–":
            pop X from stack
            push Yâ‚–, Yâ‚–â‚‹â‚, ..., Yâ‚    # push in reverse so Yâ‚ is on top
            output production X â†’ Yâ‚Yâ‚‚...Yâ‚–
        else:
            error()   # empty cell

if a == $: Accept
```

## 13.4 Complete Trace: Parse `id + id * id`

Using Grammar 4.2 parsing table (from Topic 12):

|Stack (topâ†’bot)|Input|Action|
|---|---|---|
|E $|id+id*id$|M[E, id] = Eâ†’TE' â†’ pop E, push E',T|
|T E' $|id+id*id$|M[T, id] = Tâ†’FT' â†’ pop T, push T',F|
|F T' E' $|id+id*id$|M[F, id] = Fâ†’id â†’ pop F, push id|
|id T' E' $|id+id*id$|match id; advance|
|T' E' $|+id*id$|M[T', +] = T'â†’Îµ â†’ pop T', push nothing|
|E' $|+id*id$|M[E', +] = E'â†’+TE' â†’ pop E', push E',T,+|
|+ T E' $|+id*id$|match +; advance|
|T E' $|id*id$|M[T, id] = Tâ†’FT' â†’ push T',F|
|F T' E' $|id*id$|M[F, id] = Fâ†’id â†’ push id|
|id T' E' $|id*id$|match id; advance|
|T' E' $|*id$|M[T', *] = T'â†’_FT' â†’ pop T', push T',F,_|
|* F T' E' $|*id$|match *; advance|
|F T' E' $|id$|M[F, id] = Fâ†’id â†’ push id|
|id T' E' $|id$|match id; advance|
|T' E' $|$|M[T', $] = T'â†’Îµ â†’ pop T'|
|E' $|$|M[E', $] = E'â†’Îµ â†’ pop E'|
|$|$|**ACCEPT!**|

## 13.5 Parsing `+*aaa` with `S â†’ +SS | *SS | a`

Parsing table:

||+|*|a|$|
|---|---|---|---|---|
|S|Sâ†’+SS|Sâ†’*SS|Sâ†’a||

|Stack|Input|Action|
|---|---|---|
|S $|+*aaa$|M[S,+] = Sâ†’+SS: push S,S,+|
|+ S S $|+*aaa$|match +|
|S S $|*aaa$|M[S,*] = Sâ†’_SS: push S,S,_|
|* S S S $|*aaa$|match *|
|S S S $|aaa$|M[S,a] = Sâ†’a: push a|
|a S S $|aaa$|match a|
|S S $|aa$|M[S,a] = Sâ†’a: push a|
|a S $|aa$|match a|
|S $|a$|M[S,a] = Sâ†’a: push a|
|a $|a$|match a|
|$|$|**ACCEPT!**|

---

## âœ… Q&A â€” TOPIC 13

**Q1. Why push production body in REVERSE order onto the stack?**

> **A:** The stack is LIFO (last in, first out). When we expand `A â†’ Yâ‚Yâ‚‚Yâ‚ƒ`, we want Yâ‚ to be processed first (topmost). By pushing Yâ‚ƒ, then Yâ‚‚, then Yâ‚, Yâ‚ ends up on top and will be processed next â€” maintaining left-to-right order.

**Q2. What are the two types of errors in table-driven LL parsing?**

> **A:** (1) **Terminal mismatch:** The terminal on top of the stack doesn't match the current input token. (2) **Empty cell:** The top of the stack is a nonterminal A and M[A, current_token] is empty (error entry).

**Q3. When is the parse successful?**

> **A:** When both the stack and the input reach `$` simultaneously â€” the stack has only $ left (the bottom marker) and the input pointer is at the end-of-input $. At that point, the parser announces acceptance.

**Q4. What does the output sequence of productions represent?**

> **A:** The output is exactly a **leftmost derivation** of the input string. Each production listed was applied to the leftmost nonterminal in the current sentential form, in order.

**Q5. How does this differ from recursive descent parsing?**

> **A:** They implement the same LL(1) logic but differently. Recursive descent uses the **call stack** implicitly â€” function calls push frames, returns pop them. Table-driven parsing uses an **explicit stack** data structure. Table-driven is more efficient in some languages, while recursive descent is easier to write by hand and gives better error messages.

---

# TOPIC 14 â€” ERROR RECOVERY IN PREDICTIVE PARSING

## 14.1 When Errors Occur

In predictive parsing, an error occurs when:

1. The **terminal on top of stack** â‰  **current input symbol**
2. **Nonterminal A** is on top, current input is `a`, and **M[A, a] is empty**

## 14.2 Panic-Mode Recovery for Predictive Parsers

**Synchronizing tokens** for nonterminal A:

- Tokens in **FOLLOW(A)**: when error found with A on stack, skip input until FOLLOW(A) token found, then **pop A** (treat A as empty)
- Tokens in **FIRST(A)**: skip input until FIRST(A) token found, then try to match A again
- Statement-level tokens: `;`, `}`, keywords

**Table modification:** Add "**synch**" entries to M[A, a] for a âˆˆ FOLLOW(A).

**Algorithm:**

```
If M[A, a] = blank (error):
    skip input token a   (discard it)
If M[A, a] = "synch":
    pop A from stack   (treat A as matched/empty)
If terminal T on stack but T â‰  a:
    pop T from stack   (assume T was missing in input)
```

## 14.3 Example: Error Recovery Table

For expression grammar (4.2) with synchronizing tokens added:

||id|+|*|(|)|$|
|---|---|---|---|---|---|---|
|**E**|Eâ†’TE'|synch|synch|Eâ†’TE'|synch|synch|
|**E'**||E'â†’+TE'|||E'â†’Îµ|E'â†’Îµ|
|**T**|Tâ†’FT'|T'â†’Îµ|synch|Tâ†’FT'|synch|synch|
|**T'**||T'â†’Îµ|T'â†’*FT'||T'â†’Îµ|T'â†’Îµ|
|**F**|Fâ†’id|synch|synch|Fâ†’(E)|synch|synch|

**Trace on erroneous input `+ id * + id`:**

|Stack|Input|Action|
|---|---|---|
|E $|+id*+id$|M[E, +] = synch: pop E|
|$|+id*+id$|skip +, error: "unexpected +"|
|...|id*+id$|Resume with remaining input...|
|...|Eventually parses remaining correctly||

## 14.4 Phrase-Level Recovery for Predictive Parsers

Each **empty (error) cell** in the parsing table can be filled with a pointer to an **error routine** that:

- Prints a specific error message
- Deletes or inserts tokens
- Pops or modifies the stack as needed

More sophisticated but requires more design effort.

---

## âœ… Q&A â€” TOPIC 14

**Q1. What is a "synch" entry in the parsing table?**

> **A:** A "synch" entry in M[A, a] means "if nonterminal A is on the stack and current input is a (which is in FOLLOW(A)), this is a synchronizing point â€” pop A from the stack and continue." It's part of panic-mode error recovery: we treat A as if it were matched (or derived Îµ), because we've found a token that legitimately follows A.

**Q2. How do blank cells and synch cells differ?**

> **A:** Both indicate error situations. **Blank cell:** the parser skips the current input token (it's not useful). **Synch cell:** the parser pops the top nonterminal from the stack (it was expecting something in FIRST(A) but got a token in FOLLOW(A) â€” A is effectively done).

**Q3. Why use FOLLOW(A) as synchronizing tokens?**

> **A:** FOLLOW(A) contains tokens that come after A in valid programs. When we're stuck parsing A and see a FOLLOW(A) token, it means A should be "done" by now. Popping A and resuming is safe because the next token naturally continues the parse of whatever contains A.

---

# TOPIC 15 â€” BOTTOM-UP PARSING & SHIFT-REDUCE

## 15.1 The Core Idea

**Bottom-up parsing** starts from the input string (leaves) and builds up to the start symbol (root) by repeatedly finding and **reducing** substrings.

```
Input string: id * id
                 â†“ (reduce id to F)
             F * id
                 â†“ (reduce F to T)
             T * id
                 â†“ (reduce id to F)
             T * F
                 â†“ (reduce T*F to T)
             T
                 â†“ (reduce T to E)
             E         â† START SYMBOL reached â†’ SUCCESS
```

A bottom-up parse is the **reverse of a rightmost derivation:**

```
E â‡’rm E*... ? no, let me show properly:
E â‡’rm T â‡’rm T*F â‡’rm T*id â‡’rm F*id â‡’rm id*id

Reading this right-to-left: id*id â†’ F*id â†’ T*id â†’ T*F â†’ T â†’ E
```

## 15.2 Handles

A **handle** is a substring in a right-sentential form that:

1. Matches the body of a production
2. Its reduction represents one step in the reverse rightmost derivation

**Key property:** The handle always appears at the **top of the stack** in a shift-reduce parser â€” never buried inside.

**Example:** In `idâ‚ * idâ‚‚`:

- First handle: `idâ‚` â†’ reduce to F (using F â†’ id)
- Second handle: `F` (now `F * idâ‚‚`) â†’ reduce to T (using T â†’ F)
- etc.

## 15.3 Shift-Reduce Parsing

A **shift-reduce parser** uses:

- A **stack** of grammar symbols (initially empty, $ at bottom)
- An **input buffer** with the string and $ at the end

**Four possible actions:**

|Action|When|What happens|
|---|---|---|
|**Shift**|Handle not yet complete|Push next input token onto stack; advance input|
|**Reduce**|Handle Î± found at top of stack|Pop|
|**Accept**|Stack has only start symbol + $; input is $|Parsing complete!|
|**Error**|No valid action|Syntax error; call error recovery|

## 15.4 Worked Example: Parse `id * id`

Grammar: `E â†’ E+T | T`, `T â†’ T*F | F`, `F â†’ (E) | id`

|Stack|Input|Action|
|---|---|---|
|$|id*id$|Shift id|
|$id|*id$|Reduce by Fâ†’id|
|$F|*id$|Reduce by Tâ†’F|
|$T|*id$|â€” can we reduce Tâ†’E? No! * follows, might be T*F. Shift *|
|$T*|id$|Shift id|
|$T*id|$|Reduce by Fâ†’id|
|$T*F|$|Reduce by Tâ†’T*F|
|$T|$|Reduce by Eâ†’T|
|$E|$|**Accept!**|

## 15.5 Shift/Reduce Conflicts

A **shift/reduce conflict** occurs when the parser can't decide whether to shift or reduce.

**Example â€” Dangling Else:**

```
Stack: ... if expr then stmt
Input: else ... $
```

Choice: Reduce `if expr then stmt` to `stmt` (no else) OR shift `else` (match it to this `if`).

This is an ambiguity in the grammar. The standard convention: **prefer shift** (associate `else` with nearest `if`).

## 15.6 Reduce/Reduce Conflicts

A **reduce/reduce conflict** occurs when two different reductions are possible at the same point.

**Example:**

```
Stack: ... id
Input: (  ...
```

If both `F â†’ id` and `A â†’ id` are productions, which do we reduce by? This usually indicates grammar ambiguity or poor grammar design.

---

## âœ… Q&A â€” TOPIC 15

**Q1. What does "reduce" mean in shift-reduce parsing?**

> **A:** Reducing means finding a sequence of symbols on top of the stack that matches the right-hand side of some production (this is the "handle"), popping those symbols, and pushing the corresponding left-hand side nonterminal. It's the reverse of applying a production â€” going from the body back to the head.

**Q2. Why must the handle always appear at the top of the stack?**

> **A:** Because we're constructing a rightmost derivation in reverse. In a rightmost derivation, the rightmost nonterminal is always expanded last. When we reverse this, the rightmost handle (the last thing expanded) is reduced first. Since we process input left-to-right and shift onto the stack left-to-right, the rightmost symbols of the current sentential form are always at the top of the stack.

**Q3. How does a shift-reduce parser decide when to shift vs. reduce?**

> **A:** This is exactly what the LR parsing table decides. The parser's current **state** (encoding what's been seen so far) determines the action. The state is determined by the contents of the stack and the lookahead token. LR parsing tables encode this decision precisely and without ambiguity (for LR grammars).

**Q4. What is the difference between a shift/reduce conflict and a reduce/reduce conflict?**

> **A:** **Shift/reduce conflict:** Parser can either shift the next input token or reduce the top of the stack â€” two valid actions for the same state/lookahead. Usually caused by grammar ambiguity (like dangling-else). **Reduce/reduce conflict:** Parser can reduce by two different productions â€” can't decide which nonterminal to create. Usually indicates serious grammar ambiguity or error.

**Q5. In the trace for `id * id`, why don't we reduce `T â†’ E` when we have T on the stack but `*` is the lookahead?**

> **A:** Because if we reduced T to E now, we'd have `$E * id` on stack/input. There's no production that has `E * ...` on the right-hand side, so we'd be stuck. The LR parser knows (from its state) that `*` might follow T in `T * F`, so it shifts `*` instead. This is the power of LR parsing â€” it can see the context.

---

# TOPIC 16 â€” LR(0) ITEMS, CLOSURE & GOTO

## 16.1 Why Do We Need Automata?

To decide "shift or reduce" at each step, an LR parser uses a **Deterministic Finite Automaton (DFA)** built from the grammar. Each state of this DFA represents a **set of LR(0) items** â€” tracking "how far we've gotten" in recognizing various production bodies.

## 16.2 LR(0) Items

An **LR(0) item** is a production with a **dot (â€¢)** placed somewhere in the body.

For production `A â†’ XYZ`, the four possible items are:

```
A â†’ â€¢XYZ    (haven't seen anything yet; about to see X)
A â†’ Xâ€¢YZ    (seen X; about to see Y)
A â†’ XYâ€¢Z    (seen X and Y; about to see Z)
A â†’ XYZâ€¢    (seen all of XYZ; ready to reduce!)
```

For `A â†’ Îµ`: only one item: `A â†’ â€¢` (= `A â†’ â€¢Îµ`)

**Meaning of the dot position:**

- `A â†’ Î±â€¢Î²`: We have seen a string derivable from `Î±` on the stack; we expect a string from `Î²` next in the input
- `A â†’ Î±â€¢` (dot at end): We've seen all of the RHS â€” this is a **final item** â†’ time to reduce!
- `A â†’ â€¢Î±` (dot at start): We haven't started recognizing Î± yet

## 16.3 Augmented Grammar

For grammar G with start symbol S, we create **augmented grammar G'** by adding:

```
S' â†’ S    (new start production)
```

**Purpose:** The parser accepts when it's about to reduce by `S' â†’ S`. This provides a clean "accept" signal without the original start symbol being reachable in other contexts.

**Example:**

```
Original:   E â†’ E+T | T, T â†’ T*F | F, F â†’ (E) | id
Augmented:  E' â†’ E   (plus all original productions)
```

## 16.4 CLOSURE Function

**Purpose:** Given a set of items I, `CLOSURE(I)` expands it by adding all items implied by the current items.

**Algorithm:**

```
CLOSURE(I):
    J = I
    repeat:
        for each item A â†’ Î±â€¢BÎ² in J:
            for each production B â†’ Î³:
                if B â†’ â€¢Î³ not in J:
                    add B â†’ â€¢Î³ to J
    until no more items added
    return J
```

**Intuition:** If we have `A â†’ Î±â€¢BÎ²` (expecting B next), then we might be about to see the beginning of any derivation of B. So we add `B â†’ â€¢Î³` for every B-production.

**Example: `CLOSURE({E' â†’ â€¢E})`**

```
Start: {E' â†’ â€¢E}
E appears after â€¢, E's productions: E â†’ E+T, E â†’ T
Add: E â†’ â€¢E+T, E â†’ â€¢T
T appears after â€¢ in E â†’ â€¢T, T's productions: T â†’ T*F, T â†’ F
Add: T â†’ â€¢T*F, T â†’ â€¢F
F appears after â€¢ in T â†’ â€¢F, F's productions: F â†’ (E), F â†’ id
Add: F â†’ â€¢(E), F â†’ â€¢id
E appears after â€¢ in F â†’ â€¢(E), but E â†’ â€¢E+T and E â†’ â€¢T already added.
No more changes.
```

**Result (= State Iâ‚€):**

```
Iâ‚€:
E' â†’ â€¢E
E  â†’ â€¢E+T
E  â†’ â€¢T
T  â†’ â€¢T*F
T  â†’ â€¢F
F  â†’ â€¢(E)
F  â†’ â€¢id
```

## 16.5 GOTO Function

**Purpose:** `GOTO(I, X)` = the state you move to from state I after seeing grammar symbol X (terminal or nonterminal).

**Algorithm:**

```
GOTO(I, X):
    J = {A â†’ Î±Xâ€¢Î² | A â†’ Î±â€¢XÎ² âˆˆ I}   # advance dot past X for all matching items
    return CLOSURE(J)
```

**Example: GOTO(Iâ‚€, E)**

```
Items in Iâ‚€ with dot before E:
  E' â†’ â€¢E  â†’ E' â†’ Eâ€¢
  E  â†’ â€¢E+T â†’ E â†’ Eâ€¢+T
  
GOTO(Iâ‚€, E) = CLOSURE({E' â†’ Eâ€¢, E â†’ Eâ€¢+T})
            = {E' â†’ Eâ€¢, E â†’ Eâ€¢+T}   (no new closures needed, no B after dot)
```

This becomes state Iâ‚.

## 16.6 Full LR(0) Automaton Construction

**Algorithm:**

```
Build the canonical LR(0) collection C:
    C = {CLOSURE({S' â†’ â€¢S})}   (start state Iâ‚€)
    repeat:
        for each set I in C:
            for each grammar symbol X:
                if GOTO(I, X) â‰  âˆ… and GOTO(I, X) âˆ‰ C:
                    add GOTO(I, X) to C
    until no new sets added
```

The edges between states are labeled by grammar symbols â€” this is the LR(0) DFA.

## 16.7 Kernel vs Non-Kernel Items

**Kernel items:** Items with the dot NOT at the left end, EXCEPT for the augmented start item `S' â†’ â€¢S`. **Non-kernel items:** Items with the dot at the left end (added by closure, not by transitions).

**Why this distinction?** Kernel items uniquely identify a state â€” two states are different iff their kernel items differ. Non-kernel items are fully determined by the kernel (via closure). This is used in LALR to merge states.

---

## âœ… Q&A â€” TOPIC 16

**Q1. What does an LR(0) item represent intuitively?**

> **A:** An LR(0) item `A â†’ Î±â€¢Î²` represents a parsing situation: we have already seen (on the stack) a string derivable from Î±, and we are now expecting a string derivable from Î² in the remaining input. The dot marks our progress through the production body.

**Q2. When is an item called a "final item" and what does it trigger?**

> **A:** An item with the dot at the rightmost position, e.g., `A â†’ XYZâ€¢`, is a final item. It means we've seen all of the RHS â€” the handle has been fully recognized and it's time to **reduce** by production `A â†’ XYZ`.

**Q3. Why do we add `B â†’ â€¢Î³` to CLOSURE when we see `A â†’ Î±â€¢BÎ²`?**

> **A:** Having `A â†’ Î±â€¢BÎ²` means the next thing we expect on the input is something derivable from B. B can begin with any of its productions. So for each `B â†’ Î³`, we need to track that we might be starting to recognize Î³. Adding `B â†’ â€¢Î³` says "we've seen nothing of Î³ yet and are about to start."

**Q4. Why is an augmented grammar needed?**

> **A:** Without augmentation, the original start symbol S might appear in multiple productions, and the parser might try to reduce to S at multiple points. By adding `S' â†’ S`, acceptance happens precisely and uniquely: only when the parser is about to reduce by `S' â†’ S`. It provides a single, clean accept state.

**Q5. What does GOTO(I, X) represent physically?**

> **A:** GOTO(I, X) is the state the parser moves to after reading grammar symbol X from state I. It represents "we were in state I (which tracks what we've seen), and now we've seen X â€” what's our new tracking state?" The GOTO function defines the transitions (edges) of the LR(0) DFA.

**Q6. For production `A â†’ XYZ`, how many LR(0) items does it generate?**

> **A:** 4 items: `A â†’ â€¢XYZ`, `A â†’ Xâ€¢YZ`, `A â†’ XYâ€¢Z`, `A â†’ XYZâ€¢`. In general, a production with k symbols in the body generates k+1 items. For `A â†’ Îµ`, just 1 item: `A â†’ â€¢`.

---

# TOPIC 17 â€” SLR PARSING

## 17.1 What is SLR?

**SLR (Simple LR)** is the simplest variant of LR parsing that uses:

- **LR(0) automaton** (states = sets of LR(0) items)
- **FOLLOW sets** to determine when to reduce (instead of specific lookahead embedded in items)

It's "simple" because the lookahead for reduces is just FOLLOW(A) â€” the same set regardless of which state we're in.

## 17.2 SLR Table Construction Algorithm

Given the LR(0) automaton states Iâ‚€, Iâ‚, ..., Iâ‚™:

**ACTION table:**

1. If `[S' â†’ Sâ€¢]` is in state Iáµ¢: `ACTION[i, $] = accept`
2. If state Iáµ¢ has a transition on terminal `a` to Iâ±¼: `ACTION[i, a] = shift j`
3. If `[A â†’ Î±â€¢]` (final item, A â‰  S') is in state Iáµ¢: for all `a âˆˆ FOLLOW(A)`, `ACTION[i, a] = reduce Aâ†’Î±`

**GOTO table:** 4. If state Iáµ¢ has a transition on nonterminal A to Iâ±¼: `GOTO[i, A] = j`

5. All undefined entries = **error**

**If any conflict (two entries in same cell): grammar is NOT SLR(1).**

## 17.3 SLR Table for Expression Grammar

LR(0) states (abbreviated â€” full construction has 12 states for this grammar):

FOLLOW sets:

||FOLLOW|
|---|---|
|E|{+, ), $}|
|T|{+, *, ), $}|
|F|{+, *, ), $}|

**Partial SLR Parsing Table:**

|State|id|+|*|(|)|$|E|T|F|
|---|---|---|---|---|---|---|---|---|---|
|0|s5|||s4|||1|2|3|
|1||s6||||acc||||
|2||r2|s7||r2|r2||||
|3||r4|r4||r4|r4||||
|4|s5|||s4|||8|2|3|
|5||r6|r6||r6|r6||||
|6|s5|||s4||||9|3|
|7|s5|||s4|||||10|
|8||s6|||s11|||||
|9||r1|s7||r1|r1||||
|10||r3|r3||r3|r3||||
|11||r5|r5||r5|r5||||

_Note: sj = shift to state j; ri = reduce by production i; acc = accept_

Productions: (1)Eâ†’E+T (2)Eâ†’T (3)Tâ†’T*F (4)Tâ†’F (5)Fâ†’(E) (6)Fâ†’id

## 17.4 Why SLR Can Fail

**Example: Grammar `S â†’ L=R | R`, `L â†’ *R | id`, `R â†’ L`**

In state Iâ‚‚ (containing `R â†’ Lâ€¢` and `S â†’ Lâ€¢=R`):

- `R â†’ Lâ€¢` is a final item â†’ reduce by R â†’ L for all a âˆˆ FOLLOW(R)
- FOLLOW(R) includes `=` (from `S â†’ L=R`, R appears before end but L appears before =)
- `S â†’ Lâ€¢=R` is not final, so seeing `=` â†’ shift

**Conflict:** M[Iâ‚‚, =] gets both `shift` and `reduce R â†’ L` â†’ **Shift/Reduce conflict!**

This grammar is **NOT SLR(1)** but IS LR(1) (CLR resolves it with more precise lookahead).

## 17.5 SLR vs LR(0) Difference

||LR(0)|SLR(1)|
|---|---|---|
|Reduce for final item Aâ†’Î±â€¢|All columns (entire row)|Only columns in FOLLOW(A)|
|More conflicts|Yes|Fewer conflicts|
|Power|Weakest|More than LR(0)|

---

## âœ… Q&A â€” TOPIC 17

**Q1. What makes SLR "simple"?**

> **A:** SLR is "simple" because it uses the same LR(0) automaton states as the basic LR(0) parser, but only adds the use of FOLLOW sets to restrict where reduce actions are placed. This is simpler than building the full LR(1) automaton with lookahead embedded in every item.

**Q2. What is the key difference between the ACTION table entries for SLR vs LR(0)?**

> **A:** In LR(0), when state i has a final item `A â†’ Î±â€¢`, you put `reduce Aâ†’Î±` in ALL columns (every terminal and $). In SLR(1), you only put `reduce Aâ†’Î±` in columns for terminals that are in FOLLOW(A). This restricts reduce actions, eliminating some conflicts that LR(0) would have.

**Q3. Explain a shift/reduce conflict in SLR with an example.**

> **A:** In grammar `S â†’ L=R | R`, `L â†’ *R | id`, `R â†’ L`, state Iâ‚‚ contains both `R â†’ Lâ€¢` (shift/reduce candidate) and `S â†’ Lâ€¢=R` (requires shifting `=`). The FOLLOW set of R includes `=`. So the table has both `shift` (from `S â†’ Lâ€¢=R`) and `reduce Râ†’L` (because `=` âˆˆ FOLLOW(R)) in cell [Iâ‚‚, =]. The parser can't decide â€” conflict!

**Q4. What does `r3` in cell ACTION[2, +] mean?**

> **A:** `r3` means "reduce using production number 3." The parser pops the appropriate number of symbols from the stack (equal to the length of production 3's RHS), then pushes the LHS nonterminal, then uses GOTO to determine the next state.

**Q5. Is every LL(1) grammar also SLR(1)?**

> **A:** Yes. Every LL(1) grammar is LR(1), and every LR(1) grammar includes SLR(1) as a subset. More precisely: LL(1) âŠ† SLR(1) âŠ† LALR(1) âŠ† LR(1).

---

# TOPIC 18 â€” LR PARSING ALGORITHM

## 18.1 LR Parser Structure

An LR parser uses:

1. **Stack** containing alternating states and grammar symbols: `sâ‚€ Xâ‚ sâ‚ Xâ‚‚ sâ‚‚ ... Xâ‚˜ sâ‚˜`
2. **Input buffer:** next token `a`
3. **ACTION table:** ACTION[s, a] = shift t | reduce Aâ†’Î² | accept | error
4. **GOTO table:** GOTO[s, A] = t (after reducing by A, go to state t)

> **Key:** The state on top of the stack (`sâ‚˜`) summarizes all the information from the stack below. The stack records the parsing history.

## 18.2 LR Parsing Algorithm

```python
stack = [sâ‚€]   # initial state
a = next_input_token()

while True:
    s = top_state(stack)
    
    if ACTION[s, a] == "shift t":
        push a onto stack
        push t onto stack
        a = next_input_token()
    
    elif ACTION[s, a] == "reduce A â†’ Î²":
        pop 2 * |Î²| symbols from stack   # pop the body and its states
        t = top_state(stack)
        push A onto stack
        push GOTO[t, A] onto stack
        # DON'T advance input; a stays the same
    
    elif ACTION[s, a] == "accept":
        break   # parsing complete!
    
    else:
        error_recovery()
```

**Why pop `2 * |Î²|`?** Because the stack has alternating symbols and states â€” each symbol `Xáµ¢` has a state `sáµ¢` above it.

## 18.3 Worked Trace: Parse `id + id$`

Using the SLR table from Topic 17:

|Stack|Input|Action|
|---|---|---|
|$0|id+id$|ACTION[0,id] = s5: push id, push 5|
|$0 id 5|+id$|ACTION[5,+] = r6 (Fâ†’id): pop 2, GOTO[0,F]=3: push F,3|
|$0 F 3|+id$|ACTION[3,+] = r4 (Tâ†’F): pop 2, GOTO[0,T]=2: push T,2|
|$0 T 2|+id$|ACTION[2,+] = r2 (Eâ†’T): pop 2, GOTO[0,E]=1: push E,1|
|$0 E 1|+id$|ACTION[1,+] = s6: push +, push 6|
|$0 E 1 + 6|id$|ACTION[6,id] = s5: push id, push 5|
|$0 E 1 + 6 id 5|$|ACTION[5,$] = r6 (Fâ†’id): pop 2, GOTO[6,F]=3: push F,3|
|$0 E 1 + 6 F 3|$|ACTION[3,$] = r4 (Tâ†’F): pop 2, GOTO[6,T]=9: push T,9|
|$0 E 1 + 6 T 9|$|ACTION[9,$] = r1 (Eâ†’E+T): pop 6, GOTO[0,E]=1: push E,1|
|$0 E 1|$|ACTION[1,$] = accept â†’ **ACCEPT!**|

---

## âœ… Q&A â€” TOPIC 18

**Q1. Why does the LR parser stack hold states rather than just grammar symbols?**

> **A:** The states encode the parsing context â€” they summarize the entire history of what's been seen. By just looking at the top state and the current input, the parser knows what to do. Storing just symbols wouldn't give enough information to determine the next action without re-scanning the whole stack.

**Q2. After a reduce action, why don't we advance the input pointer?**

> **A:** Reducing doesn't consume any input â€” it just restructures the stack. We replace the RHS symbols on the stack with the LHS nonterminal. The same input token that triggered the reduce is still waiting to be processed (now in the new context created by the reduction).

**Q3. What information does GOTO[t, A] provide?**

> **A:** After reducing by `A â†’ Î²` (which pops Î²'s symbols off the stack), the stack's new top state is `t`. GOTO[t, A] tells us: "given that we just recognized A while in state t, what state should we transition to?" It's the DFA transition on nonterminal A.

**Q4. How many symbols are popped during a reduce by `T â†’ T * F`?**

> **A:** Production `T â†’ T * F` has 3 symbols in the body. Since the stack has alternating symbol-state pairs, we pop `2 * 3 = 6` items (T, its state, *, its state, F, its state).

**Q5. Trace `ACTION[5, +] = r6 (Fâ†’id)` in the example above. Walk through the reduce step.**

> **A:** Production 6 is `F â†’ id`. The body has 1 symbol. Pop 2 items (id and its state 5) from stack. Top state is now 0. Look up GOTO[0, F] = 3. Push F onto stack, then push state 3. Stack becomes `$0 F 3`. Resume with same input `+id$`.

---

# TOPIC 19 â€” LR(0) PARSING

## 19.1 What is LR(0) Parsing?

**LR(0)** is the simplest LR method. It uses:

- LR(0) automaton (same as SLR)
- **No lookahead at all** for reduce decisions

**Key rule:** When a final item `A â†’ Î±â€¢` is in state Iáµ¢, put `reduce Aâ†’Î±` in **all columns** of row i (every terminal and $).

## 19.2 LR(0) vs SLR Comparison

|Aspect|LR(0)|SLR(1)|
|---|---|---|
|Reduce in columns|All columns (entire row)|Only FOLLOW(A) columns|
|Lookahead used|None|FOLLOW set|
|Power|Weakest|More powerful|
|Conflicts|More likely|Fewer|

## 19.3 When LR(0) Fails

**Example: `E â†’ T + E | T`, `T â†’ id`**

State Iâ‚‚ contains `T â†’ idâ€¢` (final). In LR(0), we put reduce in ALL columns. State Iâ‚ might also have a shift on `+`. â†’ Shift/reduce conflict â†’ **Not LR(0).**

SLR would fix this because FOLLOW(T) = {+, $} would restrict the reduce.

## 19.4 The Advantage of SLR Over LR(0)

SLR(1) adds just one token of lookahead and uses FOLLOW to restrict reduces. This single improvement greatly expands the class of parseable grammars.

**Rule:** If the grammar is LR(0), it is definitely SLR(1). If it is SLR(1), it may or may not be LR(0).

---

## âœ… Q&A â€” TOPIC 19

**Q1. Why is LR(0) considered too weak for practical use?**

> **A:** Most real grammar constructs involve operators or keywords that require some context to determine when to reduce. LR(0)'s "reduce everywhere" rule creates conflicts for almost any non-trivial grammar. Even simple grammars like `E â†’ E + T | T`, `T â†’ id` fail LR(0) due to shift/reduce conflicts.

**Q2. Is the grammar `S â†’ a` (a single production) LR(0)?**

> **A:** Yes! State Iâ‚€ = CLOSURE({S' â†’ â€¢S}) = {S'â†’â€¢S, Sâ†’â€¢a}. Transition on `a` gives Iâ‚ = {Sâ†’aâ€¢}. Iâ‚ is a final item only, so: ACTION[1, a] = r(Sâ†’a) for all terminals. ACTION[0, $] = accept. No conflicts â†’ **LR(0)**.

---

# TOPIC 20 â€” CANONICAL LR / CLR(1) PARSING

## 20.1 Motivation

SLR's weakness: it uses the **global** FOLLOW(A) set for reduce decisions. But the correct lookahead for a reduce depends on the **specific state** â€” not all tokens in FOLLOW(A) are valid in every state.

**CLR (Canonical LR)** / **LR(1)** solves this by embedding lookahead information directly into the items.

## 20.2 LR(1) Items

An **LR(1) item** is an LR(0) item plus a **lookahead symbol**:

```
[A â†’ Î±â€¢Î², a]
```

- `A â†’ Î±Î²` is a production
- The dot marks progress
- `a` is the lookahead terminal (or $)

**Meaning of lookahead:**

- For non-final items `[A â†’ Î±â€¢Î², a]` where Î² â‰  Îµ: lookahead is informational (tells us under what context we're parsing this)
- For final items `[A â†’ Î±â€¢, a]`: reduce by `A â†’ Î±` **only if the current input token is `a`**

This is the precision SLR lacks! SLR reduces whenever the token is in FOLLOW(A); CLR reduces only when the token is the specific `a` recorded in the item.

## 20.3 LR(1) CLOSURE Function

```
CLOSURE(I):
    J = I
    repeat:
        for each item [A â†’ Î±â€¢BÎ², a] in J:
            for each production B â†’ Î³:
                for each terminal b âˆˆ FIRST(Î²a):
                    if [B â†’ â€¢Î³, b] âˆ‰ J:
                        add [B â†’ â€¢Î³, b] to J
    until no change
    return J
```

**Key difference from LR(0) closure:** We compute the lookahead `b = FIRST(Î²a)` for the new items. This propagates lookahead information through the closure.

**Why FIRST(Î²a)?** When we're expanding B (having `A â†’ Î±â€¢BÎ²` with lookahead `a`), after B is recognized, the next input could be the first of `Î²`. If Î² can derive Îµ, then the lookahead for B is `a`.

## 20.4 Example: Grammar `S' â†’ S`, `S â†’ CC`, `C â†’ cC | d`

**Computing Iâ‚€ = CLOSURE({[S' â†’ â€¢S, $]}):**

```
Start: {[S' â†’ â€¢S, $]}

S appears after â€¢, S â†’ CC:
  FIRST($ after S matches) = FIRST(nothing) ... wait, for [S'â†’â€¢S, $]:
  Î² = Îµ, a = $, FIRST(Îµ$) = FIRST($) = {$}
  Add: [S â†’ â€¢CC, $]

C appears after â€¢, C â†’ cC | d:
  In [S â†’ â€¢CC, $]: after C comes C, then $. So FIRST(C$) = FIRST(C) = {c, d}
  Add: [C â†’ â€¢cC, c], [C â†’ â€¢cC, d], [C â†’ â€¢d, c], [C â†’ â€¢d, d]

c appears after â€¢ in [C â†’ â€¢cC, c/d]: terminal, no closure
d appears after â€¢ in [C â†’ â€¢d, c/d]: terminal, no closure
```

**Iâ‚€:**

```
[S' â†’ â€¢S, $]
[S  â†’ â€¢CC, $]
[C  â†’ â€¢cC, c]
[C  â†’ â€¢cC, d]
[C  â†’ â€¢d, c]
[C  â†’ â€¢d, d]
```

## 20.5 CLR Parsing Table Construction

**Simplified algorithm:**

1. If `[S' â†’ Sâ€¢, $]` in state Iáµ¢: `ACTION[i, $] = accept`
2. If Iáµ¢ has transition on terminal `a` to Iâ±¼: `ACTION[i, a] = shift j`
3. If `[A â†’ Î±â€¢, a]` in state Iáµ¢ (A â‰  S'): `ACTION[i, a] = reduce Aâ†’Î±`
4. If Iáµ¢ has transition on nonterminal A to Iâ±¼: `GOTO[i, A] = j`
5. Conflicts â†’ grammar is NOT CLR(1)

**Notice the precision:** Reduce is placed only for the specific lookahead `a` recorded in the item â€” not for all of FOLLOW(A)!

---

## âœ… Q&A â€” TOPIC 20

**Q1. What is the key advantage of LR(1)/CLR over SLR?**

> **A:** CLR embeds specific lookahead tokens into each item, making reduce decisions more precise. SLR uses FOLLOW(A) globally â€” which may include tokens that, while valid followers of A in general, are not valid in a specific parsing state. CLR's state-specific lookahead eliminates these false conflicts, allowing it to parse more grammars without conflicts.

**Q2. The grammar `S â†’ L=R | R`, `L â†’ *R | id`, `R â†’ L` fails SLR. Does CLR handle it?**

> **A:** Yes! In CLR, the item that causes trouble in SLR â€” `R â†’ Lâ€¢` â€” will only have specific lookaheads embedded. In the state where `S â†’ Lâ€¢=R` exists, the lookahead for `R â†’ Lâ€¢` won't include `=` (because `=` comes from a different parsing context). The CLR can distinguish the two cases that SLR conflates.

**Q3. What is FIRST(Î²a) in the CLR closure computation and why use it?**

> **A:** Given item `[A â†’ Î±â€¢BÎ², a]`, when we add B's productions, the new items need lookaheads. The next token after B (in the context of A's production) could be the start of Î², or if Î² â†’ Îµ, it's `a`. FIRST(Î²a) captures exactly these possibilities â€” all terminals that could immediately follow B in this context.

**Q4. Why does CLR have more states than SLR for the same grammar?**

> **A:** CLR splits states by lookahead â€” items that have the same core (same production + dot position) but different lookaheads become different items and may create different states. SLR collapses these (one state regardless of lookahead). So CLR has equal or more states than SLR, sometimes many more.

**Q5. What's the relationship: CLR vs SLR in terms of grammar power?**

> **A:** CLR(1) is strictly more powerful than SLR(1). Every SLR(1) grammar is CLR(1), but not vice versa. CLR(1) handles all grammars that LALR(1) handles, plus some more (those where merging LALR states causes conflicts). The hierarchy: SLR(1) âŠ‚ LALR(1) âŠ‚ CLR(1).

---

# TOPIC 21 â€” LALR PARSING

## 21.1 The Motivation

**CLR problem:** Too many states. For C, CLR might have thousands of states; SLR/LALR have hundreds.

**LALR insight:** Many CLR states have the **same core items** (same production + dot position) but different lookaheads. We can **merge** these states by taking the **union of lookaheads**.

This gives us:

- LALR tables: **same number of states as SLR**
- But more powerful than SLR (because lookaheads are state-specific, not global FOLLOW sets)

## 21.2 Definition

**LALR (Lookahead-LR)** is constructed by:

1. Build the CLR(1) collection of item sets
2. **Merge** all sets that have the same **core** (ignore lookaheads) by taking union of lookaheads
3. Build the table from the merged sets

**Core of an item** = the LR(0) part (production + dot position), ignoring lookahead.

**Example:** CLR states Iâ‚ƒ = {[C â†’ dâ€¢, c], [C â†’ dâ€¢, $]} and Iâ‚† = {[C â†’ dâ€¢, c], [C â†’ dâ€¢, d]}

Both have core `C â†’ dâ€¢`. Merge: Iâ‚ƒâ‚† = {[C â†’ dâ€¢, c/d/$]}

## 21.3 LALR Table Construction

After merging:

1. Same as CLR table construction, but using merged states
2. If merging creates a reduce/reduce conflict â†’ grammar is NOT LALR(1)
3. (Merging can never create shift/reduce conflicts â€” only reduce/reduce)

## 21.4 Worked Example: Grammar `S â†’ CC`, `C â†’ cC | d`

**CLR has 10 states; LALR merges to 7 states:**

Pairs to merge (same cores):

- Iâ‚ƒ {Câ†’dâ€¢, c} + Iâ‚† {Câ†’dâ€¢, c} + part of Iâ‚ˆ â†’ merge: Iâ‚ƒâ‚† with lookaheads {c, d, $}  
    _(actual merging depends on full automaton; simplified here)_

**LALR table is more compact but recognizes same grammar as CLR for LALR(1) grammars.**

## 21.5 When LALR Fails Where CLR Succeeds

**Example:**

```
S â†’ Aa | bAc | Bc | bBa
A â†’ d
B â†’ d
```

In CLR, the item `A â†’ dâ€¢` in one state has lookahead `{a, c}` and `B â†’ dâ€¢` in another state has `{c, a}`. These states have the same core but different lookaheads â€” merging them creates: `{[A â†’ dâ€¢, a/c], [B â†’ dâ€¢, a/c]}` in the same state. Now seeing `a` or `c`, we don't know if we're reducing by `A â†’ d` or `B â†’ d` â†’ **reduce/reduce conflict!**

So: **LALR fails, CLR succeeds.**

## 21.6 Comparing SLR, LALR, CLR

|Property|SLR(1)|LALR(1)|CLR(1)|
|---|---|---|---|
|**Items**|LR(0) items|LR(1) items (merged)|LR(1) items|
|**# States**|n|n|n to nÃ—(number of lookaheads) more|
|**Reduce lookahead**|FOLLOW(A)|Union of LR(1) lookaheads|Exact LR(1) lookahead|
|**Power**|Least|Middle|Most powerful|
|**Practical use**|Rarely used alone|âœ… Most widely used|Rarely â€” too large|
|**Parser generators**||YACC, Bison|GNU Bison (with option)|

**Number of states:** `n(LR(0)) = n(SLR) = n(LALR) â‰¤ n(CLR)`

---

## âœ… Q&A â€” TOPIC 21

**Q1. What does it mean to "merge states" in LALR construction?**

> **A:** Two or more CLR states with the same core items (same LR(0) base, ignoring lookaheads) are combined into one state by taking the union of their lookahead sets. The transitions (GOTO) are updated accordingly â€” multiple former states now map to/from one combined state.

**Q2. Can merging LALR states introduce shift/reduce conflicts?**

> **A:** No. Merging can only introduce **reduce/reduce conflicts**, not shift/reduce conflicts. The reason: shift actions are determined by the core items (not lookaheads), and the cores are the same in merged states. The conflict only arises when the same lookahead triggers two different reduces â€” a reduce/reduce conflict.

**Q3. Why is LALR(1) the most practically used LR variant?**

> **A:** LALR(1) offers the best balance: it's nearly as powerful as CLR(1) (handles almost all practical programming language constructs) while having the same compact table size as SLR(1). Parser generators like YACC and Bison default to LALR(1), and most programming language grammars are LALR(1).

**Q4. What is the grammar power hierarchy of LR parsers?**

> **A:** From weakest to strongest: **LR(0) âŠ‚ SLR(1) âŠ‚ LALR(1) âŠ‚ CLR(1)**. Every grammar parseable by a weaker method is also parseable by a stronger one, but not vice versa.

**Q5. For C language, how many states do LALR and CLR parsers have approximately?**

> **A:** For a language like C, SLR and LALR parsers typically have **several hundred states**, while CLR (canonical LR(1)) can have **several thousand states**. This explosive state growth is why CLR is rarely used for full-scale programming languages despite being the most powerful.

**Q6. Why does the grammar `S â†’ Aa | bAc | Bc | bBa`, `A â†’ d`, `B â†’ d` fail LALR but work with CLR?**

> **A:** In CLR, the two states for `A â†’ dâ€¢` (with lookahead `{a}`) and `B â†’ dâ€¢` (with lookahead `{c}`) are separate. When we see `d` followed by `a`, we know to reduce `A â†’ d`; when `d` followed by `c`, reduce `B â†’ d`. LALR merges these into one state (same core `dâ€¢`), and that state contains both `A â†’ dâ€¢` and `B â†’ dâ€¢` with overlapping lookaheads `{a, c}`. A reduce/reduce conflict results.

**Q7. What error reporting property differs between CLR and LALR/SLR?**

> **A:** CLR will detect errors **immediately** without performing any incorrect reductions, because its lookahead is exact. LALR and SLR may perform a few reductions before detecting the error, because their lookaheads are less precise (some reductions may be technically valid in the automaton even though the overall input is invalid).

---

# TOPIC 22 â€” VIABLE PREFIXES & VALID ITEMS

## 22.1 Viable Prefixes

**Definition:** A **viable prefix** is any prefix of a right-sentential form that does **not extend past the right end of the rightmost handle**.

In simpler terms: the contents of the LR parser stack at any point during parsing form a viable prefix.

**Why "viable"?** It's always possible to append more terminals to a viable prefix to obtain a complete right-sentential form. The parse can always continue.

**Why not all prefixes?** Some prefixes of right-sentential forms can never appear on the LR stack because the parser would have reduced before shifting past the handle.

**Example:** For grammar `E â†’ E+T | T`, `T â†’ T*F | F`, `F â†’ (E) | id`:

Right-sentential form: `(E) * id`

Viable prefixes: `Îµ, (, (E, (E)` but NOT `(E)*` â€” by the time we have `(E)` on stack, we reduce it to F before shifting `*`.

## 22.2 Valid Items

**Definition:** An item `A â†’ Î²â‚â€¢Î²â‚‚` is **valid** for a viable prefix `Î±Î²â‚` if there is a rightmost derivation:

```
S' â‡’rm* Î±Aw â‡’rm Î±Î²â‚Î²â‚‚w
```

(where the dot position shows how much of A's production we've seen)

**Key property:** By knowing the valid items for the current viable prefix (= current stack contents), the LR parser can determine the correct action.

## 22.3 Viable Prefixes in LR(0) Items

Every viable prefix corresponds to LR(0) items in the automaton. Specifically:

- A viable prefix `Î±Î²â‚` corresponds to item `A â†’ Î²â‚â€¢Î²â‚‚` being valid for it
- The LR(0) automaton state reached after reading `Î±Î²â‚` contains all valid items for that viable prefix

**Connection to LR parsing:** The LR(0) automaton **recognizes viable prefixes**. Each state represents the set of valid items for all viable prefixes that lead to that state.

## 22.4 Example Viable Prefixes

For `E â†’ E + T`, with sentential form `E + T * F`:

|Item|Viable Prefix|
|---|---|
|`E â†’ â€¢E + T`|Îµ (empty stack)|
|`E â†’ E â€¢ + T`|`E`|
|`E â†’ E + â€¢ T`|`E +`|
|`E â†’ E + T â€¢`|`E + T` (ready to reduce!)|

The string `E + T` is a viable prefix (the handle is exactly `T`, and `E + T` doesn't go past the handle).

---

## âœ… Q&A â€” TOPIC 22

**Q1. What are viable prefixes and why are they important?**

> **A:** Viable prefixes are the possible contents of an LR parser's stack at any valid step of parsing. They're "viable" because they can always be extended to a complete right-sentential form. They're important because the LR(0) automaton is built to **recognize** viable prefixes â€” each state identifies where we are in parsing, enabling correct shift/reduce decisions.

**Q2. Why can't `id *` be a viable prefix in the expression grammar?**

> **A:** Because by the time we have `id` on the stack, the handle `id` is recognized, and we immediately reduce to `F â†’ id`, then possibly to `T â†’ F`. The parser would never shift `*` while `id` is on the stack; it always reduces `id` to F first. So `id *` is a prefix of some right-sentential form, but it can never appear on the parser's stack.

**Q3. How does the LR(0) automaton "recognize" viable prefixes?**

> **A:** The LR(0) automaton is a DFA where states are sets of LR(0) items. When we feed the stack contents (from bottom to top) as input to this DFA, we end up in a state that contains exactly the valid items for that viable prefix. This state tells the parser what action to take next.

---

# TOPIC 23 â€” COMPARISON OF ALL PARSERS

## 23.1 Complete Hierarchy

```
                    All Context-Free Languages
                              |
                   Deterministic Context-Free Languages
                    /                              \
            LR(1) = CLR(1)                   LL(k) for large k
                 |
             LALR(1)
                 |
              SLR(1)
                 |
              LR(0)
                 |
              LL(1)         â† LL(1) âŠ‚ SLR(1) âŠ‚ LALR(1) âŠ‚ LR(1)
```

## 23.2 Summary Table

|Property|LR(0)|SLR(1)|LALR(1)|CLR(1)|
|---|---|---|---|---|
|**DFA used**|LR(0) automaton|LR(0) automaton|LR(1) automaton (merged)|LR(1) automaton|
|**Items**|LR(0)|LR(0)|LR(1) merged|LR(1)|
|**Reduce lookahead**|None (all columns)|FOLLOW(A)|Union of LR(1) lookaheads|Exact LR(1) lookahead|
|**# States**|n|n|n|â‰¥ n (often much more)|
|**Power**|Weakest||Most practical|Strongest|
|**Error detection**|Least precise|||Most precise (no wrong reductions)|
|**Conflicts**|Most||Fewer|Fewest|

**State count rule:** `n(LR(0)) = n(SLR) = n(LALR) â‰¤ n(CLR)`

## 23.3 Hierarchy Relationships

- Every **LR(0)** grammar is **SLR(1)**
- Every **SLR(1)** grammar is **LALR(1)**
- Every **LALR(1)** grammar is **CLR(1)**
- Every **LL(1)** grammar is **SLR(1)** (and therefore LALR and CLR)
- Converses are NOT always true

---

## âœ… Q&A â€” TOPIC 23

**Q1. Given a grammar, how would you determine its parser class?**

> **A:** Try in order: (1) Build LR(0) automaton. If no conflicts â†’ LR(0). (2) Apply SLR rules. If no conflicts â†’ SLR(1). (3) Build LR(1) automaton and apply LALR merging. If no conflicts â†’ LALR(1). (4) Without merging (full LR(1) table). If no conflicts â†’ CLR(1). If conflicts persist â†’ not in LR(1).

**Q2. Can an ambiguous grammar be in any LR class?**

> **A:** No. Every LR(k) grammar for any k is unambiguous. If a grammar is ambiguous, it will always have shift/reduce or reduce/reduce conflicts in any LR parser, for any k. However, the converse is not true â€” an unambiguous grammar might still not be LR(k) for practical k.

**Q3. What are the practical parsers used in real compiler tools?**

> **A:** YACC and GNU Bison generate **LALR(1)** parsers by default. GNU Bison also supports CLR(1) (via `%define lr.type canonical-lr`). Most modern programming languages (C, Java, Ruby, etc.) are designed to have LALR(1) grammars.

**Q4. Classify which of these is true: (a) Every SLR grammar is LALR, (b) Every LALR grammar is SLR, (c) Every LL(1) grammar is LR(1), (d) Every LR(1) grammar is LL(1)**

> **A:** (a) **True** â€” SLR âŠ‚ LALR âŠ‚ CLR. (b) **False** â€” there are LALR grammars that are not SLR. (c) **True** â€” LL(1) âŠ‚ SLR âŠ‚ LALR âŠ‚ CLR = LR(1). (d) **False** â€” LR(1) is much more powerful than LL(1); many LR(1) grammars are not LL(1).

---

# TOPIC 24 â€” PARSER GENERATORS: YACC/BISON

## 24.1 Why Not Write LR Parsers by Hand?

Building an LR parser manually requires:

- Constructing LR(0)/LR(1) item sets
- Computing CLOSURE and GOTO
- Building ACTION/GOTO tables for hundreds/thousands of states
- Resolving conflicts

This is tedious, error-prone, and impractical for real languages. **Parser generators automate this entirely.**

## 24.2 YACC/Bison Overview

**GNU Bison** (replacement for YACC â€” Yet Another Compiler-Compiler) is the standard parser generator:

- Input: a **grammar specification** in modified BNF notation
- Output: a **C/C++/Java parser** (LALR(1) by default)
- Also supports: GLR, CLR, IELR(1) modes

**Key operations Bison performs:**

1. Reads the grammar
2. Builds LALR(1) parsing tables
3. Reports any conflicts (shift/reduce, reduce/reduce)
4. Generates a working parser as C source

## 24.3 Bison File Structure

```
%{
/* C declarations: #includes, global variables */
#include <stdio.h>
%}

/* Declarations section */
%token DIGIT NUM ID        /* declare terminal tokens */
%left '+' '-'              /* left-associative, lower precedence */
%left '*' '/'              /* left-associative, higher precedence */
%right UMINUS              /* right-assoc unary minus */

%%
/* Rules section: grammar + semantic actions */
expr : expr '+' expr    { $$ = $1 + $3; }
     | expr '*' expr    { $$ = $1 * $3; }
     | '(' expr ')'     { $$ = $2; }
     | NUM              { $$ = $1; }
     ;
%%
/* User C code section */
int main() { return yyparse(); }
```

## 24.4 Bison Keywords

|Keyword|Meaning|
|---|---|
|`%token NAME`|Declare terminal token NAME|
|`%left sym`|Declare symbols as left-associative|
|`%right sym`|Declare symbols as right-associative|
|`%nonassoc sym`|Declare symbols as non-associative|
|`%start sym`|Declare the start symbol|
|`%type <field> sym`|Declare semantic type for nonterminal|

## 24.5 Semantic Actions

In Bison rules, `$$` refers to the value of the LHS nonterminal; `$1`, `$2`, ... refer to the values of the RHS symbols (left to right):

```
expr : expr '+' term    { $$ = $1 + $3; }
```

Here `$1` = value of left `expr`, `$2` = value of `+` (usually ignored), `$3` = value of `term`. `$$` = value of the resulting `expr`.

## 24.6 Interaction with Flex/Lex

```
Lex/Flex                          YACC/Bison
  lex.l                           parser.y
    |                                 |
    â†“                                 â†“
  lex.yy.c â†â”€â”€ includes â†â”€â”€ y.tab.h
    |                                 |
    â†“                                 â†“
  yylex()   â†â”€â”€â”€â”€ calls â”€â”€â”€â”€  yyparse()
  (tokenizer)                   (parser)
```

**Compilation steps:**

```bash
$ lex lexer.l           # generates lex.yy.c
$ yacc -d parser.y      # generates y.tab.c, y.tab.h
$ gcc y.tab.c lex.yy.c -ll -ly  # link and compile
$ ./a.out < input       # run
```

**Windows (with Bison/Flex):**

```bash
$ bison -dy parser.y
$ flex lexer.l
$ gcc y.tab.c lex.yy.c
```

## 24.7 Token Values

- Single-character tokens (like `+`, `-`, `*`) get the **ASCII value** of that character as their token number
- Named tokens (like `DIGIT`, `ID`) get values starting from **258** (reserved values 0-257)
- The lexer returns token numbers; the parser table uses them

## 24.8 The yylval Variable

Each token can carry a **semantic value** stored in `yylval`. Type is controlled by:

```c
#define YYSTYPE double  /* all values are doubles */
```

In the lexer:

```c
[0-9]+(.[0-9]+)?  { yylval = atof(yytext); return NUMBER; }
```

---

## âœ… Q&A â€” TOPIC 24

**Q1. What does YACC stand for and what does it do?**

> **A:** YACC = **Yet Another Compiler-Compiler**. It takes a grammar specification (productions + semantic actions) and generates a parser in C/C++. It automates the construction of LALR(1) parsing tables and the LR parsing algorithm, so programmers write grammar rules instead of implementing a parser from scratch.

**Q2. What is the role of `%token` in a YACC/Bison specification?**

> **A:** `%token NAME` declares `NAME` as a terminal symbol (token). Bison assigns an integer token code to it (typically â‰¥ 258) and puts the definition in `y.tab.h`. The lexer includes this header to return the same integer codes that the parser expects.

**Q3. Why must the lexer include `y.tab.h`?**

> **A:** The lexer (yylex) must return token codes (integers) that match what the parser expects. These codes are assigned by Bison and written to `y.tab.h`. Without including this header, the lexer would have to hardcode integers, leading to mismatches.

**Q4. What does `$$` mean in a Bison action, and what do `$1`, `$2`, `$3` mean?**

> **A:** `$$` is the **semantic value** of the LHS nonterminal (the production's result). `$1`, `$2`, `$3` are the semantic values of the 1st, 2nd, 3rd symbol on the RHS, respectively. For `E â†’ E + T { $$ = $1 + $3; }`: `$1` = value of E, `$2` = value of `+` (irrelevant here), `$3` = value of T, and `$$` = their sum, which becomes the value of the resulting E.

**Q5. How does Bison handle operator precedence and associativity?**

> **A:** Using `%left`, `%right`, `%nonassoc` declarations. Lines listed later have **higher precedence**. Tokens on the same line have the same precedence. This lets Bison automatically resolve shift/reduce conflicts that arise from the ambiguous arithmetic grammar `E â†’ E+E | E*E | ...`. Instead of rewriting the grammar, you just declare precedence.

**Q6. Why do token values start at 258 in YACC/Bison?**

> **A:** Values 0-255 are reserved for single-character tokens (equal to their ASCII codes). Value 256 and 257 are typically reserved for special purposes (like end-of-file = 0, error token). Named tokens start at 258 to avoid conflicts with character tokens.

**Q7. What is the `-d` flag in `yacc -d parser.y`?**

> **A:** The `-d` flag tells YACC to generate the **`y.tab.h` header file** containing `#define` statements that associate token names with their integer codes. Without `-d`, only `y.tab.c` is generated, and the lexer can't include the token definitions.

**Q8. Bison generates an LALR(1) parser by default. What if the grammar has conflicts?**

> **A:** Bison reports the conflicts (number of shift/reduce and reduce/reduce conflicts) and uses default resolution rules: prefer shift over reduce (resolves most dangling-else cases), and for reduce/reduce, prefer the production listed first in the grammar. The `-v` flag generates `y.output` showing exactly which states have conflicts and why.

---

# TOPIC 25 â€” GRAMMAR HIERARCHY SUMMARY

## 25.1 Complete Picture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           ALL CONTEXT-FREE LANGUAGES                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚        DETERMINISTIC CONTEXT-FREE              â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚   â”‚
â”‚  â”‚  â”‚           LR(1) / CLR(1)                â”‚  â”‚   â”‚
â”‚  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚   â”‚
â”‚  â”‚  â”‚  â”‚         LALR(1)                   â”‚  â”‚  â”‚   â”‚
â”‚  â”‚  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚  â”‚   â”‚
â”‚  â”‚  â”‚  â”‚  â”‚         SLR(1)              â”‚  â”‚  â”‚  â”‚   â”‚
â”‚  â”‚  â”‚  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚  â”‚  â”‚   â”‚
â”‚  â”‚  â”‚  â”‚  â”‚  â”‚       LR(0)           â”‚  â”‚  â”‚  â”‚  â”‚   â”‚
â”‚  â”‚  â”‚  â”‚  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚  â”‚  â”‚  â”‚   â”‚
â”‚  â”‚  â”‚  â”‚  â”‚  â”‚  â”‚     LL(1)       â”‚  â”‚  â”‚  â”‚  â”‚  â”‚   â”‚
â”‚  â”‚  â”‚  â”‚  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚  â”‚  â”‚  â”‚   â”‚
â”‚  â”‚  â”‚  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚  â”‚  â”‚   â”‚
â”‚  â”‚  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚  â”‚   â”‚
â”‚  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## 25.2 No Ambiguous Grammar is in Any LR or LL Class

The moment a grammar is ambiguous, it causes conflicts in every parser type. All LR and LL grammars are unambiguous.

## 25.3 Practical Takeaways

|Use Case|Recommended Parser|
|---|---|
|Hand-written parsers|LL(1) / Recursive Descent|
|Language with easy structure (if/while/for dominates)|LL(1)|
|Expression-heavy languages|LALR(1)|
|Most real programming languages|LALR(1) via Bison/YACC|
|Research/maximum power|CLR(1)|
|Python/modern languages|PEG parsers|

---

## âœ… Q&A â€” FINAL COMPREHENSIVE REVIEW

**Q1. List the transformations needed to make a grammar suitable for LL(1) parsing.**

> **A:** (1) **Eliminate left recursion** â€” convert `A â†’ AÎ± | Î²` to `A â†’ Î²A'`, `A' â†’ Î±A' | Îµ`. (2) **Apply left factoring** â€” factor out common prefixes. (3) **Check LL(1) conditions** â€” verify no FIRST/FIRST or FIRST/FOLLOW conflicts. If conflicts remain, the grammar may inherently not be LL(1).

**Q2. Suppose you see a state in an SLR automaton with both `A â†’ Î±â€¢` and `A â†’ Î±â€¢aÎ²`. Is there a conflict?**

> **A:** Only if the same token appears in both the shift column (for some `a`) and the reduce column (from FOLLOW(A)). If `a âˆˆ FOLLOW(A)`, there's a shift/reduce conflict. If `a âˆ‰ FOLLOW(A)`, no conflict.

**Q3. Explain the complete compilation pipeline from source to assembly, focusing on the role of the parser.**

> **A:** Source code â†’ [Lexer: tokenizes into token stream] â†’ [Parser: checks grammar, builds AST] â†’ [Semantic Analyzer: type checking, symbol resolution, builds symbol table] â†’ [Intermediate Code Generator: produces IR like 3-address code] â†’ [Optimizer: improves IR] â†’ [Code Generator: produces assembly/machine code]. The parser's role is structural validation and tree construction â€” without a correct parse tree, no further compilation can proceed.

**Q4. What is the "viable prefix property" and why is it desirable?**

> **A:** Both LL and LR parsers have the viable prefix property: they detect errors at the earliest possible point â€” no valid input symbol is shifted past the point where an error is inevitable. This means error messages pinpoint the exact location of the problem, giving programmers precise feedback.

**Q5. Why is `*` in `F â†’ *` (unary dereference) not ambiguous in C's grammar?**

> **A:** In C's declaration grammar, `*` is carefully distinguished by context â€” whether it appears before a type name (declaring a pointer) or before an expression (dereferencing). The C grammar uses separate nonterminals for declaration specifiers and expression operators, avoiding ambiguity.

**Q6. Compare the error detection precision of CLR vs LALR parsers.**

> **A:** CLR has **exact LR(1) lookaheads** â†’ it never reduces by the wrong production â†’ detects errors immediately with no incorrect reductions. LALR has **merged lookaheads** â†’ may have some states where a reduce is performed that CLR wouldn't do â†’ may make a few reductions before declaring error. SLR is even less precise (uses global FOLLOW). **CLR provides the earliest possible error detection.**

**Q7. What is the significance of the production `S' â†’ S` in augmented grammars?**

> **A:** It provides a **unique, unambiguous accept state** for the parser. Acceptance occurs only when the parser is about to reduce by `S' â†’ S` and the input is $. Without augmentation, the start symbol S might appear in other productions' bodies, making it unclear when to accept. The augmented start symbol S' appears in only one production, ensuring clean acceptance.

**Q8. You have a grammar that is LALR but not SLR. What does that tell you?**

> **A:** The grammar has a state where SLR's use of the global FOLLOW set creates a conflict, but LALR's more precise state-specific lookaheads (from LR(1) items) resolve it. The grammar likely has a context where the same nonterminal A appears in two different positions requiring different tokens to follow it â€” SLR can't distinguish these positions, but LALR can.

---

> ðŸŽ“ **End of Chapter 4 â€” Syntax Analysis Master Notebook**
> 
> This notebook covered all topics from the syllabus with full depth: CFGs, ambiguity, left recursion elimination, left factoring, all parser types (RDP, LL(1), LR(0), SLR, LALR, CLR), FIRST/FOLLOW sets, parsing table construction, error recovery, viable prefixes, and YACC/Bison.