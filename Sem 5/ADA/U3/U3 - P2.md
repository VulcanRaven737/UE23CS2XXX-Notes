# A Foundational Guide to Causal Inference: Assumptions, Estimation, and Graphical Models

## 1.0 The Core Challenge: Moving from Association to Causation

The fundamental goal of causal inference is to move beyond simple correlation and understand the true effect of a treatment, policy, or intervention on a given outcome. While standard statistical analysis can tell us that two variables are associated, it cannot, on its own, tell us if one _causes_ the other. Causal inference provides a specialized framework for answering counterfactual "what if?" questions, allowing us to estimate the impact of a change and make more informed decisions. At the heart of this discipline lies a conceptual puzzle that shapes every method we use.

This puzzle is known as the **Fundamental Problem of Causal Inference**. It states that for any single individual, we can only ever observe one potential outcome—either the outcome after receiving a treatment or the outcome after not receiving it, but never both simultaneously. This limitation prevents us from directly measuring the causal effect on an individual level.

To formalize this, we use the language of **Potential Outcomes**. For any individual `i`, we define two potential outcomes:

- `Yi(1)`: The outcome that _would have been observed_ for individual `i` if they had received the treatment.
- `Yi(0)`: The outcome that _would have been observed_ for individual `i` if they had not received the treatment.

With this notation, we can define the ideal causal measure, the **Individual Treatment Effect (ITE)**, as the difference between these two potential states:

`ITE = Yi(1) - Yi(0)`

Because we can never observe both `Yi(1)` and `Yi(0)` for the same individual at the same time, the ITE is fundamentally unobservable. This forces a strategic shift in our goal. Instead of trying to measure the impossible, we aim to estimate the **Average Treatment Effect (ATE)** across an entire population. The ATE is the expectation, or average, of the individual treatment effects:

`ATE = E[Yi(1) - Yi(0)]`

The ATE provides a powerful summary of the average impact of the treatment across the population of interest. The entire rest of this guide is a roadmap for how to validly estimate this single, powerful quantity from messy, real-world data by bridging the gap between the unobservable world of potential outcomes and the observable world of statistics.

## 2.0 The Bedrock: Essential Assumptions for Causal Estimation

To bridge the gap between the unobservable causal world and the observable statistical world, we cannot rely on data alone. Instead, we must build our analysis on a set of critical, and often untestable, assumptions. These assumptions are not mere technicalities; they form the logical foundation upon which all causal claims from non-experimental data are built. Understanding them is essential for both conducting and critically evaluating causal studies.

### 2.1 The Ideal Scenario: Ignorability and Exchangeability

In the ideal world of a perfectly randomized controlled trial (RCT), the most important assumption is automatically satisfied. This assumption is known as **Ignorability**, and it is formally stated as:

`(Y(1), Y(0)) ⊥⊥ T`

In simple terms, this means that the assignment of the treatment (`T`) is statistically independent of the potential outcomes (`Y(1)` and `Y(0)`). Whether an individual is placed in the treatment or control group has nothing to do with what their outcome would have been in either scenario.

The critical implication of ignorability is that it provides a mathematical bridge from the unobservable causal ATE to an observable statistical quantity known as the **Associational Difference**. When ignorability holds, we can equate the two:

`ATE = E[Y(1)] - E[Y(0)] = E[Y|T=1] - E[Y|T=0]`

This equation is the key: it allows us to use the average outcome of the observed treatment group (`E[Y|T=1]`) as a valid proxy for the average potential outcome under treatment for the whole population, and likewise for the control group.

A closely related concept is **Exchangeability**. This is the key idea that the treatment and control groups are comparable in every way that matters for the outcome, except for the treatment itself. If the groups were "exchangeable," we could swap their treatment assignments, and the resulting average outcomes would be the same. Ignorability and exchangeability are essentially the same concept viewed from different perspectives; both are achieved through successful randomization.

**Illustrative Example: The Tutoring Program**

Consider a school evaluating an after-school tutoring program.

- **Valid Study (Ignorability/Exchangeability Holds):** Students are _randomly assigned_ to either the tutoring group or the no-tutoring group. Because assignment is random, both groups will have a similar mix of strong and weak students. The groups are exchangeable. If we observe that the tutored students' average score is **85** and the non-tutored students' average score is **75**, we can confidently attribute the **10-point difference** to the causal effect of the tutoring.
- **Invalid Study (Ignorability/Exchangeability Violated):** Students are allowed to _self-select_ into the program. In this case, students who are already struggling are more likely to sign up for tutoring. The treatment and control groups are no longer comparable; the treatment assignment is dependent on the students' potential outcomes. The observed 10-point difference is now a mix of the true tutoring effect and selection bias. Since struggling students are more likely to seek tutoring, the tutored group started with a lower baseline potential score. This means the observed difference of 10 points is likely an _underestimate_ of the true causal effect.

### 2.2 Core Technical Assumptions: Consistency and No Interference

Beyond the ideal of exchangeability, two technical assumptions are required to connect our theoretical model to the real-world data we observe.

1. **Consistency:** This assumption links the potential outcomes we theorize about to the outcomes we actually see in our data. Formally, if an individual `i` receives treatment `t` (i.e., `Ti=t`), then their observed outcome `Yi` is their potential outcome under that treatment, `Yi(t)`. This sounds obvious, but it requires that the treatment is well-defined and consistently applied.
2. **No Interference:** This assumption states that an individual's outcome is only a function of their own treatment status and is not affected by the treatment status of others. This can be violated in situations with social or network effects. For example, in a study where the treatment is "get a dog" and the outcome is happiness, your happiness might be influenced not only by whether you get a dog but also by whether your friends get dogs, as this could lead to more social gatherings.

Together, these two assumptions are known as the **Stable Unit-Treatment Value Assumption (SUTVA)**. SUTVA is a critical, often implicit, assumption that underlies the majority of causal analyses.

### 2.3 Adapting for Observational Data: Conditional Exchangeability and Positivity

In observational studies, where we analyze data we collected but did not experimentally generate, the assumption of simple exchangeability is almost always unrealistic. There is no randomization to guarantee that the treatment and control groups are comparable. This is where the concept of **Conditional Exchangeability**, also known as **Unconfoundedness**, becomes the most important assumption in observational causal inference.

The formal statement for Conditional Exchangeability is:

`(Y(1), Y(0)) ⊥⊥ T | X`

This formula means that within specific subgroups, or strata, defined by a set of measured covariates `X`, the treatment assignment is independent of the potential outcomes. In essence, we are assuming that while the overall treatment and control groups may not be comparable, they become comparable _after we control for all the relevant confounding variables in X_.

To make this adjustment work, we need a second crucial assumption: **Positivity**, also known as **Overlap**. The formal statement is:

`0 < P(T=1 | X=x) < 1`

Positivity requires that for any given subgroup defined by the covariates `x`, there is a non-zero probability of being in either the treatment group or the control group. Put simply, within every subgroup, there must be at least some individuals who received the treatment and some who did not. If this assumption is violated for a particular subgroup, we have no data on the counterfactual, making it impossible to estimate the causal effect for that group.

**Consequence of Violated Positivity: The Hybrid Teaching Method**

Imagine a study on a hybrid teaching method. We collect data on covariates like student location (urban/rural) and internet access.

- **Scenario:** For the subgroup of rural students who have no internet or devices, the school _only_ offers traditional in-person teaching. None of them receive the hybrid teaching treatment.
- **Violation:** In this case, `P(Hybrid Teaching | X=rural, no internet) = 0`. Positivity is violated.
- **Consequence:** It is impossible to answer the causal question: "What is the effect of hybrid teaching on rural students without internet?" We simply have no data to compare the hybrid and traditional outcomes for this specific group.

There is often a practical **trade-off between Unconfoundedness and Positivity**. To achieve strong unconfoundedness, we are tempted to control for a large number of covariates. However, as we add more variables, we create more and more specific subgroups. This increases the risk that some of these subgroups will be very small or will contain only treated (or only untreated) individuals, thus violating the positivity assumption.

With this full set of assumptions in place, we can finally move from theory to the practical methods used to estimate causal effects from real-world data.

## 3.0 Estimating Causal Effects in Practice

With a clear understanding of the necessary assumptions, we can turn to the practical task of estimating causal effects. This involves defining the specific metrics that quantify the impact of an intervention and learning the computational tools required to calculate them, especially when working with observational data where direct comparisons are misleading.

### 3.1 Quantifying Impact: ATE and CATE

There are two primary metrics used to measure causal effects, each offering a different level of granularity.

The **Average Treatment Effect (ATE)**, as previously defined, measures the overall impact of an intervention on the entire population.

`ATE = E[Y(1)] - E[Y(0)]`

It answers the question: "On average, what is the effect of this treatment for everyone?"

While ATE is useful for high-level decisions, it can hide important variations in how different people respond to a treatment. To gain a more nuanced understanding, we use the **Conditional Average Treatment Effect (CATE)**.

`CATE(x) = E[Y(1) - Y(0) | X=x]`

CATE estimates the Average Treatment Effect for a specific subgroup of the population defined by a set of characteristics `X=x`. It answers the question: "What is the average effect of this treatment for a specific group of people with these characteristics?"

**Case Study: Digital Marketing Campaign**

A technology company wants to know if a personalized email campaign increases customer spending.

- **Treatment (T):** Receiving a personalized email (`1` = yes, `0` = no)
- **Outcome (Y):** Monthly purchase amount ($)
- **Covariates (X):** Customer age, account type (Premium/Basic)

First, the company calculates the **ATE**, which turns out to be **$25**. This means that, on average, sending a personalized email increases a customer's monthly purchase amount by $25. If the cost to send an email is low, this suggests the campaign is profitable overall. However, this single number doesn't tell the whole story.

Next, the company calculates the **CATE** for different customer segments.

|   |   |
|---|---|
|Account Type|CATE|
|Premium|$45|
|Basic|$15|

|   |   |
|---|---|
|Age Group|CATE|
|<30|$35|
|30–50|$25|
|>50|$10|

These results are far more insightful. They show that the email campaign is three times more effective for Premium members than for Basic members.

This enables **Advanced Targeting**. Suppose the company develops a new, "high-touch" campaign that is more effective but also more expensive, costing **$20 per customer**. Using the ATE of $25 would suggest sending it to everyone is profitable. However, CATE provides a more sophisticated decision framework:

|   |   |   |   |
|---|---|---|---|
|Segment|CATE|Cost|Decision|
|Premium|$45|$20|✅ **Send**|
|Basic|$15|$20|❌ **Don't Send**|

The analysis shows it is only profitable to send the high-cost campaign to Premium members, as their expected return (45) exceeds the cost (20). For Basic members, the cost would outweigh the benefit. By leveraging CATE, the company can move beyond one-size-fits-all strategies to make profit-maximizing, personalized decisions.

### 3.2 The Adjustment Formula for Observational Data

When we lack the luxury of a randomized experiment, we cannot simply compare the average outcomes of the treated and untreated groups. This is where we must apply the central theme of this guide: bridging the causal and statistical worlds. The **Adjustment Formula** is the primary method for doing so. It is the computational implementation of the Conditional Exchangeability assumption from Section 2.3, allowing us to calculate the causal effect of a treatment `T` on an outcome `Y` while controlling for a set of confounding variables `X`.

The formal Adjustment Formula is:

`E[Y|do(T=t)] = Σx E[Y|T=t, X=x]P(X=x)`

The `do(T=t)` notation represents an intervention where we _set_ the value of the treatment, distinguishing it from simply observing `T=t`. The goal of the formula is to estimate the counterfactual `E[Y(1)]` by asking, "What would the average outcome have been if everyone received the treatment, but maintained their original distribution of confounding characteristics `X`?" It achieves this by:

1. Breaking the population down into smaller, more comparable subgroups (strata) based on the values of the confounding variables `X`.
2. Within each subgroup `x`, calculating the average outcome for those who received the treatment (`E[Y|T=t, X=x]`).
3. Computing a weighted average of these subgroup-specific effects, where the weights are based on the prevalence of each subgroup in the overall population (`P(X=x)`).

There are two primary methods for implementing this adjustment:

1. **Stratification:** This method directly follows the intuition of the formula. The process involves four steps: (1) divide the data into strata based on the values of the confounder(s) `X`; (2) estimate the treatment effect within each stratum; (3) weight each stratum's estimate by the proportion of the population in that stratum (`P(X=x)`); and (4) sum the weighted estimates to get the overall adjusted effect.
2. **Regression:** A more common and flexible approach is to use a statistical model. We fit a regression model that predicts the outcome `Y` using the treatment `T` and the set of confounders `X` as predictors. The model automatically provides a single, adjusted estimate for the effect of `T` on `Y` after accounting for the influence of `X`.

For the Adjustment Formula to yield a valid causal estimate, several key assumptions must hold, including **Causal Sufficiency** (i.e., we have measured and included all common causes of `T` and `Y`) and **No Model Misspecification** (if using regression, the model accurately reflects the true relationships between the variables).

The Adjustment Formula tells us _how_ to control for variables, but it doesn't tell us _which_ variables we need to control for. For that, we turn to a powerful visual tool: Directed Acyclic Graphs.

## 4.0 A Visual Language for Causality: Directed Acyclic Graphs (DAGs)

While the Adjustment Formula provides the mathematical recipe for controlling for confounding variables, **Directed Acyclic Graphs (DAGs)** provide the map. DAGs are an essential tool for visually encoding our assumptions about the causal relationships between variables. They allow us to reason systematically about confounding and determine precisely which variables we need to adjust for to estimate a causal effect.

A DAG consists of two core components:

- **Nodes:** Represent the variables in our system (e.g., treatment, outcome, confounders).
- **Directed Edges (Arrows):** Represent a direct causal relationship. An arrow from `A` to `B` means `A` is a direct cause of `B`.
- **Acyclic:** The graph must not contain any cycles; you cannot start at a node, follow the arrows, and return to your starting point.

### 4.1 The Building Blocks of Causal Structures

All complex DAGs are built from three elementary path structures. Understanding how information (or statistical association) flows through these structures—and how that flow is affected when we "condition" on a variable—is the key to using DAGs effectively.

1. **Chains (Mediators)**
    - **Structure:** `A → B → C`
    - **Description:** The effect of `A` on `C` is mediated through `B`. `A` and `C` are associated.
    - **Rule:** Conditioning on the middle node `B` **blocks** the flow of association along this path.
    - **Example:** `Education → Income → Health`. The positive effect of education on health may be mediated by income. If we control for income, we block this specific causal pathway.
2. **Forks (Confounders)**
    - **Structure:** `A ← B → C`
    - **Description:** `B` is a common cause of both `A` and `C`, creating a non-causal (spurious) association between them.
    - **Rule:** Conditioning on the common cause `B` **blocks** the spurious association.
    - **Example:** `Rain ← Season → Coat Sales`. Rain does not cause coat sales, but they are associated because both are influenced by the season. Controlling for the season removes this confounding association.
3. **Colliders**
    - **Structure:** `A → B ← C`
    - **Description:** `B` is a common _effect_ of both `A` and `C`. `A` and `C` are two independent causes of `B`. This structure is sometimes called an "immorality" because the two "parent" nodes (`A` and `C`) are not "married" by an edge.
    - **Rule:** The path between `A` and `C` is naturally **blocked** by the collider `B`. Counter-intuitively, conditioning on the collider `B` (or one of its descendants) **opens** the path, creating a spurious association between `A` and `C`.
    - **Example:** `Talent → Fame ← Luck`. Among the general population, talent and luck are independent. However, if we study only famous people (i.e., condition on fame), we create an association. Among the famous, those with less talent must have had more luck, and vice-versa.

### 4.2 d-Separation and The Backdoor Criterion

These three building blocks give us a set of rules for determining whether variables are statistically connected in a graph. This concept is formalized as **d-separation** (directional separation), the graphical criterion for determining if two variables are conditionally independent. Two nodes `A` and `B` are said to be d-separated by a set of nodes `X` if `X` blocks every path between `A` and `B`.

A path is blocked by a set of nodes `X` if one of the following is true:

1. The path contains a **chain** or a **fork** where the middle node is in the set `X`.
2. The path contains a **collider**, and neither the collider nor any of its descendants are in the set `X`.

The key application of d-separation in causal inference is the **Backdoor Criterion**. This criterion provides a clear, graphical method for identifying a "sufficient adjustment set"—a set of variables that, when controlled for, is sufficient to remove confounding bias and identify the causal effect of a treatment `T` on an outcome `Y`. It is the final bridge we need, connecting our visual assumptions to our statistical method.

A set of variables `X` satisfies the Backdoor Criterion relative to `T` and `Y` if it meets two conditions:

1. `**X**` **blocks every backdoor path between** `**T**` **and** `**Y**`**.** A backdoor path is a path connecting `T` and `Y` that starts with an arrow pointing into `T`. These paths represent non-causal or spurious associations created by common causes (confounders).
2. `**X**` **contains no descendants of** `**T**`**.** A descendant of `T` is any variable that is causally affected by `T` (e.g., a mediator).

This second rule is critical. We must not control for descendants of the treatment because doing so can introduce bias in two primary ways:

1. **Blocking the Causal Path:** Controlling for a mediator (a type of descendant) blocks the very causal pathway we want to measure. In the chain `Treatment → Medication Adherence → Health Outcome`, controlling for `Adherence` would likely make the treatment appear ineffective, as we have blocked its primary mechanism of action.
2. **Inducing Collider Bias:** Controlling for a descendant can create a collider. If `Treatment` and an unmeasured confounder `U` both cause a descendant `D`, controlling for `D` will open a spurious path between `Treatment` and `U`, biasing the estimated effect.

The set of variables `X` that satisfies the Backdoor Criterion is precisely the set of covariates you should use in the Adjustment Formula. The DAG and the Backdoor Criterion give us a principled, non-statistical method for selecting the variables for our statistical adjustment.

## 5.0 Application and Real-World Challenges

This final section solidifies the core concepts by walking through a complete case study that demonstrates the power of adjusting for confounding. It then concludes by summarizing the primary challenges that practitioners face when applying these powerful but demanding methods in the real world.

### 5.1 Case Study: Adjusting for Confounding in the COVID-27 Scenario

Imagine a scenario where we need to determine which of two treatments, A or B, is more effective for patient recovery from a new disease, COVID-27. The outcome we measure is the death rate.

A preliminary analysis of the total patient population yields the following results:

|   |   |
|---|---|
|Treatment|Total Death Rate|
|**A**|**16%** (240/1500)|
|**B**|19% (105/550)|

Based on this naive comparison, Treatment A appears to be superior, with a lower overall death rate. However, a domain expert suggests that the patient's initial "Condition" (Mild or Severe) at the time of treatment could be a confounding variable. This is because doctors may be more likely to give the newer Treatment B to more severe cases, and a patient's initial condition also directly affects their chance of recovery.

This relationship can be represented by a simple DAG where `Condition` creates a backdoor path:

`Condition → Treatment` `Condition → Outcome`

This structure is a **fork**, and `Condition` is a confounder that must be adjusted for. To do this, we apply the Adjustment Formula via stratification. We re-analyze the data, breaking it down by the patient's initial condition.

|   |   |   |   |
|---|---|---|---|
||**Condition**|||
|**Treatment**|**Mild**|**Severe**|**Total**|
|**A**|15% (210/1400)|30% (30/100)|16% (240/1500)|
|**B**|**10%** (5/50)|**20%** (100/500)|19% (105/550)|

Within each stratum, the conclusion is reversed! For both Mild and Severe patients, Treatment B has a lower death rate. This reversal of an association when a confounding variable is considered is a famous statistical phenomenon known as **Simpson's Paradox**. It powerfully illustrates why adjusting for confounders is not just a technicality, but essential for arriving at the correct causal conclusion.

To find the true causal effect, we calculate the adjusted death rate for each treatment, weighting the stratum-specific rates by the overall proportion of patients in each condition. Out of 2050 total patients, 1450 had a mild condition and 600 had a severe condition.

- **Adjusted Death Rate for A:** `(1450/2050) * 15% + (600/2050) * 30% = 19.4%`
- **Adjusted Death Rate for B:** `(1450/2050) * 10% + (600/2050) * 20% = 12.9%`

After correctly adjusting for the confounder identified via the DAG, the conclusion is completely reversed. The adjusted analysis reveals that **Treatment B is causally superior to Treatment A**, with an estimated death rate of 12.9% compared to 19.4%.

### 5.2 Common Hurdles in Causal Inference

Applying these methods is not without its challenges. Practitioners must navigate several significant hurdles to produce reliable causal estimates.

- **Identifiability:** The effect cannot be calculated if non-causal paths, such as those created by confounders, cannot be fully blocked from the available data.
- **Lack of a Known Causal Graph:** The true causal structure of a system is rarely known with certainty. Constructing a DAG relies on domain expertise, which can be subjective, incomplete, or incorrect. An inaccurate graph can lead to adjusting for the wrong variables, introducing bias.
- **Insufficient Data:** Causal estimates, especially those derived from stratifying the data into many subgroups, require large sample sizes to be stable and reliable. Small datasets can lead to noisy estimates and a lack of statistical power.
- **Unverifiable Assumptions:** The most critical assumption, **Causal Sufficiency** (as discussed in Section 3.2), is untestable. We must rely on expert judgment to argue that we have controlled for all important common causes, but we can never prove it with data.

### Conclusion

The journey from observing a simple association to making a credible causal claim is a structured and rigorous one. It requires a fundamental shift in thinking: moving from a "data-first" mindset that asks "what does the data say?" to an "assumption-first" mindset that asks "what must I assume about the world to interpret this data causally?" By starting with the **Fundamental Problem of Causal Inference**, we recognize the need for a framework built on clear, though untestable, assumptions like **Conditional Exchangeability**. With tools like **Directed Acyclic Graphs** to encode our assumptions and the **Adjustment Formula** to execute the necessary corrections, we can build a logical bridge from statistical observation to causal understanding. While significant challenges remain, this principled, assumption-based framework provides the most reliable path we have for answering the "what if" questions that drive science and decision-making.